{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e07e91",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from arch import arch_model\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen, VECM\n",
    "from statsmodels.tsa.stattools import coint\n",
    "from pykalman import KalmanFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9742937",
   "metadata": {},
   "source": [
    "# V1X는 S&P500 의 V1X, V2X는 EURO의 V1X 지표. 2020.11.20~ 2024.11.19 최근 5년 일일데이터 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee9949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2020.11.20~ 2024.11.19 최근 5년 일일데이터 사용\n",
    "snp=pd.read_csv('/Users/danielkhur/Documents/카이스트 MFE/MFE 2024-2학기/후반기/금융시계열분석(9-16)/3주차과제/CBOE Volatility Index Historical Data.csv')\n",
    "euro=pd.read_csv('/Users/danielkhur/Documents/카이스트 MFE/MFE 2024-2학기/후반기/금융시계열분석(9-16)/3주차과제/STOXX 50 Volatility VSTOXX EUR Historical Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be24462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "snp=snp[['Date','Price']]\n",
    "snp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606c6572",
   "metadata": {},
   "outputs": [],
   "source": [
    "euro=euro[['Date','Price']]\n",
    "euro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b41d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "snp.columns=['date','snpprice']\n",
    "euro.columns=['date','europrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dde538",
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_euro=pd.merge(snp, euro, on='date', how='inner')\n",
    "snp_euro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9a9a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_euro = snp_euro[::-1].reset_index(drop=True)\n",
    "snp_euro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d73cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1x=snp_euro['snpprice'].values[::-1]\n",
    "v2x=snp_euro['europrice'].values[::-1]\n",
    "dates=snp_euro['date'].values[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ef40aa",
   "metadata": {},
   "source": [
    "# 1. V1X와 V2X의 시계열을 추세, 계절성 및 잡음으로 분해하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fc3aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition_snp = sm.tsa.seasonal_decompose(v1x, model='additive', period = 12) \n",
    "y = pd.Series(v1x, index=dates)\n",
    "observed = pd.Series(decomposition_snp.observed, index=y.index)\n",
    "trend = pd.Series(decomposition_snp.trend, index=y.index)\n",
    "seasonal = pd.Series(decomposition_snp.seasonal, index=y.index)\n",
    "resid = pd.Series(decomposition_snp.resid, index=y.index)\n",
    "\n",
    "# Plot the decomposition with a title\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(10, 10))\n",
    "fig.suptitle('Seasonal Decomposition of Time Series (S&P V1X)', fontsize=16)\n",
    "\n",
    "observed.plot(ax=ax1, title='Observed', legend=False)\n",
    "trend.plot(ax=ax2, title='Trend', legend=False)\n",
    "seasonal.plot(ax=ax3, title='Seasonal', legend=False)\n",
    "resid.plot(ax=ax4, title='Residual', legend=False)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to make room for the title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25313f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition_euro = sm.tsa.seasonal_decompose(v2x, model='additive', period = 12) \n",
    "y = pd.Series(v2x, index=dates)\n",
    "observed = pd.Series(decomposition_euro.observed, index=y.index)\n",
    "trend = pd.Series(decomposition_euro.trend, index=y.index)\n",
    "seasonal = pd.Series(decomposition_euro.seasonal, index=y.index)\n",
    "resid = pd.Series(decomposition_euro.resid, index=y.index)\n",
    "\n",
    "# Plot the decomposition with a title\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(10, 10))\n",
    "fig.suptitle('Seasonal Decomposition of Time Series (EURO V2X)', fontsize=16)\n",
    "\n",
    "observed.plot(ax=ax1, title='Observed', legend=False)\n",
    "trend.plot(ax=ax2, title='Trend', legend=False)\n",
    "seasonal.plot(ax=ax3, title='Seasonal', legend=False)\n",
    "resid.plot(ax=ax4, title='Residual', legend=False)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to make room for the title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f28f690",
   "metadata": {},
   "source": [
    "# 2. V1X와 V2X 각각에 대해 단위근 테스트를 실행하고 정상성을 살펴보라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f420f62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.Series(v1x, index=dates)\n",
    "\n",
    "# Augmented Dickey-Fuller test\n",
    "result = adfuller(y)\n",
    "\n",
    "# Extracting the results\n",
    "print('ADF Statistic for V1X:', result[0])\n",
    "print('V1X p-value:', result[1])\n",
    "print('V1X Critical Values:', result[4])\n",
    "\n",
    "# Interpretation\n",
    "if result[1] <= 0.05:\n",
    "    print(\"The V1X time series is stationary (reject the null hypothesis of unit root).\")\n",
    "else:\n",
    "    print(\"The V1X time series is non-stationary (fail to reject the null hypothesis of unit root).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e6f15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.Series(v2x, index=dates)\n",
    "\n",
    "# Augmented Dickey-Fuller test\n",
    "result = adfuller(y)\n",
    "\n",
    "# Extracting the results\n",
    "print('ADF Statistic for V2X:', result[0])\n",
    "print('V2X p-value:', result[1])\n",
    "print('V2X Critical Values:', result[4])\n",
    "\n",
    "# Interpretation\n",
    "if result[1] <= 0.05:\n",
    "    print(\"The V2X time series is stationary (reject the null hypothesis of unit root).\")\n",
    "else:\n",
    "    print(\"The V2X time series is non-stationary (fail to reject the null hypothesis of unit root).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b27402",
   "metadata": {},
   "source": [
    "# V1X, V2X 다 정상성이라는 결과가 나왔다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f9c539",
   "metadata": {},
   "source": [
    "# 3. V1X와 V2X 각각에 대해 ARIMA GARCH 모델을 적용하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19094c0d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# V1X에 대해 ARIMA, GARCH(1,1)을 각각 적용\n",
    "v1x_series = pd.Series(v1x, index=dates)\n",
    "# Fit ARIMA model (Example: ARIMA(1, 1, 1))\n",
    "arima_model = ARIMA(v1x_series, order=(1, 1, 1))\n",
    "arima_result = arima_model.fit()\n",
    "\n",
    "# Summary of the ARIMA model\n",
    "print(arima_result.summary())\n",
    "\n",
    "# Plot residuals\n",
    "residuals_v1x = arima_result.resid\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(residuals_v1x, label='ARIMA Residuals')\n",
    "plt.title('ARIMA Residuals for V1X')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5db3800",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Fit GARCH(1, 1) model to ARIMA residuals\n",
    "garch_model = arch_model(residuals_v1x, vol='Garch', p=1, q=1)\n",
    "garch_result = garch_model.fit()\n",
    "\n",
    "# Summary of the GARCH model\n",
    "print(garch_result.summary())\n",
    "\n",
    "# Plot the volatility\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(garch_result.conditional_volatility, label='Conditional Volatility (GARCH)')\n",
    "plt.title('GARCH Conditional Volatility of V1X')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b694e1a2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# V2X에 대해 ARIMA, GARCH(1,1)을 각각 적용\n",
    "v2x_series = pd.Series(v2x, index=dates)\n",
    "# Fit ARIMA model (Example: ARIMA(1, 1, 1))\n",
    "arima_model = ARIMA(v2x_series, order=(1, 1, 1))\n",
    "arima_result = arima_model.fit()\n",
    "\n",
    "# Summary of the ARIMA model\n",
    "print(arima_result.summary())\n",
    "\n",
    "# Plot residuals\n",
    "residuals_v2x = arima_result.resid\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(residuals_v2x, label='ARIMA Residuals')\n",
    "plt.title('ARIMA Residuals for V2X')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55232cf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Fit GARCH(1, 1) model to ARIMA residuals\n",
    "garch_model = arch_model(residuals_v2x, vol='Garch', p=1, q=1)\n",
    "garch_result = garch_model.fit()\n",
    "\n",
    "# Summary of the GARCH model\n",
    "print(garch_result.summary())\n",
    "\n",
    "# Plot the volatility\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(garch_result.conditional_volatility, label='Conditional Volatility (GARCH)')\n",
    "plt.title('GARCH Conditional Volatility of V2X')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8117ba9a",
   "metadata": {},
   "source": [
    "# 4. V1X와 V2X에 대해 Multivariate GARCH 모델를 적용해보라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcefd39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = pd.DataFrame({'V1X': snp_euro['snpprice'], 'V2X': snp_euro['europrice']}).dropna()\n",
    "\n",
    "# Step 2: Fit Univariate GARCH(1,1) for each series\n",
    "v1x_garch = arch_model(returns['V1X'], vol='Garch', p=1, q=1).fit(disp=\"off\")\n",
    "v2x_garch = arch_model(returns['V2X'], vol='Garch', p=1, q=1).fit(disp=\"off\")\n",
    "\n",
    "# Extract standardized residuals\n",
    "v1x_std_resid = v1x_garch.resid / v1x_garch.conditional_volatility\n",
    "v2x_std_resid = v2x_garch.resid / v2x_garch.conditional_volatility\n",
    "\n",
    "# Combine standardized residuals\n",
    "std_residuals = np.column_stack((v1x_std_resid, v2x_std_resid))\n",
    "\n",
    "# Step 3: Initialize DCC-GARCH parameters\n",
    "T, k = std_residuals.shape\n",
    "Q_bar = np.cov(std_residuals.T)  # Unconditional covariance matrix\n",
    "Q = Q_bar.copy()\n",
    "alpha, beta = 0.05, 0.9  # DCC parameters (to be tuned)\n",
    "R_matrices = np.zeros((T, k, k))  # Store dynamic correlation matrices\n",
    "\n",
    "# Step 4: Estimate Dynamic Conditional Correlation (DCC)\n",
    "for t in range(T):\n",
    "    # Update Q_t (dynamic covariance matrix)\n",
    "    outer_product = np.outer(std_residuals[t], std_residuals[t])\n",
    "    Q = (1 - alpha - beta) * Q_bar + alpha * outer_product + beta * Q\n",
    "\n",
    "    # Convert Q to correlation matrix R\n",
    "    diag_Q = np.sqrt(np.diag(Q))\n",
    "    R = Q / np.outer(diag_Q, diag_Q)  # Normalize to get correlation matrix\n",
    "    R_matrices[t] = R\n",
    "\n",
    "# Step 5: Extract and visualize dynamic correlations\n",
    "dynamic_corr = [R[0, 1] for R in R_matrices]\n",
    "\n",
    "# Plot dynamic correlations\n",
    "plt.plot(dynamic_corr)\n",
    "plt.title(\"Dynamic Conditional Correlation (DCC) Between V1X and V2X using DCC GARCH(1,1)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ace4d1",
   "metadata": {},
   "source": [
    "# 5. V1X와 V2X를 이용해 VAR 모델을 만들고, Granger causality 테스트를 실행하고, Impulse response 그래프와 분산 분해(Varinace Decomposion)을 계산하고 그래프로 보여라.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00be46c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAR 모델 생성\n",
    "model = VAR(returns)\n",
    "\n",
    "# Select the optimal lag order using AIC\n",
    "lag_order = model.select_order(maxlags=10)\n",
    "print(\"Optimal lag order based on AIC:\", lag_order.aic)\n",
    "\n",
    "# Fit the model\n",
    "var_model = model.fit(lag_order.aic)\n",
    "print(var_model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9697c629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# granger-casuality 테스트 진행\n",
    "print(\"Granger Causality Test:\")\n",
    "grangercausalitytests(returns, maxlag=lag_order.aic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55029af3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# impulse response 계산\n",
    "irf = var_model.irf(10)  # Impulse response for 10 steps ahead\n",
    "\n",
    "# Plot impulse response functions\n",
    "irf.plot(orth=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b414dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance decomposition 계산 후 그래프로 표현\n",
    "fevd = var_model.fevd(10)  # 10-step ahead variance decomposition\n",
    "\n",
    "# Plot variance decomposition\n",
    "fevd.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdce676",
   "metadata": {},
   "source": [
    "# 6. V1X와 V2X를 이용해 공적분관계를 확인하고, VECM 모델을 구축하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25de3f07",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# 2. Load and Prepare Data\n",
    "# Assuming V1X and V2X are your time series arrays\n",
    "# Combine them into a DataFrame\n",
    "data = returns\n",
    "\n",
    "# Check stationarity of individual series using differencing (if needed)\n",
    "data_diff = data.diff().dropna()\n",
    "\n",
    "# Visualize\n",
    "data.plot(title=\"Original Series\")\n",
    "plt.show()\n",
    "\n",
    "data_diff.plot(title=\"Differenced Series\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d2a749",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# 3. Check for Cointegration Using Johansen Test\n",
    "# Johansen Test for Cointegration\n",
    "result = coint_johansen(data, det_order=0, k_ar_diff=1)  # det_order=0 assumes no deterministic trend\n",
    "trace_stat = result.lr1  # Trace statistics\n",
    "crit_values = result.cvt  # Critical values\n",
    "\n",
    "# Print results\n",
    "print(\"Johansen Cointegration Test\")\n",
    "print(f\"Trace Statistics: {trace_stat}\")\n",
    "print(f\"Critical Values:\\n{crit_values}\")\n",
    "\n",
    "# Check if Trace Statistics > Critical Values for cointegration\n",
    "if trace_stat[0] > crit_values[0, 1]:  # Compare with 5% critical value\n",
    "    print(\"Cointegration exists at the 5% significance level.\")\n",
    "else:\n",
    "    print(\"No cointegration at the 5% significance level.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf06c15",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# 4. Construct VECM Model\n",
    "# Once cointegration is confirmed, construct a VECM model.\n",
    "# Fit VECM model\n",
    "vecm = VECM(data, k_ar_diff=1, coint_rank=1)  # coint_rank=1 for one cointegrating relationship\n",
    "vecm_fit = vecm.fit()\n",
    "\n",
    "# Display VECM summary\n",
    "print(vecm_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2588a35e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# 5. Impulse Response and Diagnostics\n",
    "# After fitting the VECM model, you can analyze impulse responses or conduct model diagnostics.\n",
    "# Impulse Response Analysis\n",
    "irf = vecm_fit.irf(10)  # Impulse response for 10 periods\n",
    "irf.plot()\n",
    "plt.show()\n",
    "\n",
    "# Diagnostics (Residuals)\n",
    "vecm_resid = vecm_fit.resid\n",
    "plt.plot(vecm_resid)\n",
    "plt.title(\"VECM Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650c5b6a",
   "metadata": {},
   "source": [
    "# 7. 페어트레이딩 (트레이딩 경계: 평균 +/- some 표준편차)\n",
    "# (1) 단순 스프레드를 이용한 페어 트레이딩, (2) 공적분관계를 이용한 페어트레이딩, \n",
    "# (3) 칼만 필터를 이용한 페어트레이딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf91e8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=returns\n",
    "entry_threshold = 1  # Entry threshold in terms of standard deviations\n",
    "exit_threshold = 0   # Exit when spread returns to mean\n",
    "\n",
    "\n",
    "# Function to calculate trading signals\n",
    "def generate_signals(spread, mean, std_dev, entry_threshold, exit_threshold):\n",
    "    signals = pd.DataFrame(index=spread.index, columns=['Spread', 'Position'])\n",
    "    signals['Spread'] = spread\n",
    "\n",
    "    # Entry and exit signals\n",
    "    signals['Position'] = np.where(spread > mean + entry_threshold * std_dev, -1,  # Short\n",
    "                                   np.where(spread < mean - entry_threshold * std_dev, 1,  # Long\n",
    "                                            0))  # Exit\n",
    "    return signals\n",
    "\n",
    "\n",
    "# Pair Trading Using Simple Spread\n",
    "def pair_trading_simple(data):\n",
    "    # Calculate spread as the difference between the two series\n",
    "    spread = data['V1X'] - data['V2X']\n",
    "    mean = spread.mean()\n",
    "    std_dev = spread.std()\n",
    "\n",
    "    # Generate trading signals\n",
    "    signals = generate_signals(spread, mean, std_dev, entry_threshold, exit_threshold)\n",
    "    return signals\n",
    "\n",
    "\n",
    "# Pair Trading Using Cointegration\n",
    "def pair_trading_coint(data):\n",
    "    # Cointegration test\n",
    "    _, p_value, _ = coint(data['V1X'], data['V2X'])\n",
    "    if p_value > 0.05:\n",
    "        raise ValueError(\"No cointegration relationship found\")\n",
    "\n",
    "    # Calculate spread using regression coefficients\n",
    "    hedge_ratio = np.polyfit(data['V2X'], data['V1X'], 1)[0]\n",
    "    spread = data['V1X'] - hedge_ratio * data['V2X']\n",
    "    mean = spread.mean()\n",
    "    std_dev = spread.std()\n",
    "\n",
    "    # Generate trading signals\n",
    "    signals = generate_signals(spread, mean, std_dev, entry_threshold, exit_threshold)\n",
    "    return signals\n",
    "\n",
    "\n",
    "# Pair Trading Using Kalman Filter\n",
    "def pair_trading_kalman(data):\n",
    "    # Kalman Filter for dynamic hedge ratio\n",
    "    kf = KalmanFilter(transition_matrices=[1],\n",
    "                      observation_matrices=[1],\n",
    "                      initial_state_mean=0,\n",
    "                      initial_state_covariance=1,\n",
    "                      observation_covariance=1,\n",
    "                      transition_covariance=0.01)\n",
    "\n",
    "    # Apply Kalman Filter\n",
    "    spread = []\n",
    "    hedge_ratios = []\n",
    "    state_means, _ = kf.filter(data['V1X'].values - data['V2X'].values)\n",
    "    for i in range(len(data)):\n",
    "        hedge_ratio = state_means[i][0]\n",
    "        hedge_ratios.append(hedge_ratio)\n",
    "        spread.append(data['V1X'].iloc[i] - hedge_ratio * data['V2X'].iloc[i])\n",
    "\n",
    "    spread = pd.Series(spread, index=data.index)\n",
    "    mean = spread.mean()\n",
    "    std_dev = spread.std()\n",
    "\n",
    "    # Generate trading signals\n",
    "    signals = generate_signals(spread, mean, std_dev, entry_threshold, exit_threshold)\n",
    "    return signals, hedge_ratios\n",
    "\n",
    "\n",
    "# Execute strategies\n",
    "simple_signals = pair_trading_simple(data)\n",
    "coint_signals = pair_trading_coint(data)\n",
    "kalman_signals, kalman_hedge_ratios = pair_trading_kalman(data)\n",
    "'''\n",
    "# Plot the spread and trading boundaries for the first strategy (simple spread)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(simple_signals['Spread'], label='Spread', color='blue')\n",
    "plt.axhline(simple_signals['Spread'].mean(), color='green', linestyle='--', label='Mean')\n",
    "plt.axhline(simple_signals['Spread'].mean() + entry_threshold * simple_signals['Spread'].std(),\n",
    "            color='red', linestyle='--', label='Upper Bound')\n",
    "plt.axhline(simple_signals['Spread'].mean() - entry_threshold * simple_signals['Spread'].std(),\n",
    "            color='red', linestyle='--', label='Lower Bound')\n",
    "plt.title(\"Simple Spread Pair Trading\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18363f1e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Plot all three spreads as subplots\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 18))  # 3 rows, 1 column\n",
    "\n",
    "# Simple Spread Plot\n",
    "axes[0].plot(simple_signals['Spread'], label='Simple Spread', color='blue')\n",
    "axes[0].axhline(simple_signals['Spread'].mean(), color='green', linestyle='--', label='Mean')\n",
    "axes[0].axhline(simple_signals['Spread'].mean() + entry_threshold * simple_signals['Spread'].std(),\n",
    "                color='red', linestyle='--', label='Upper Bound')\n",
    "axes[0].axhline(simple_signals['Spread'].mean() - entry_threshold * simple_signals['Spread'].std(),\n",
    "                color='red', linestyle='--', label='Lower Bound')\n",
    "axes[0].set_title(\"Simple Spread Pair Trading\")\n",
    "axes[0].legend()\n",
    "axes[0].grid()\n",
    "\n",
    "# Cointegration Spread Plot\n",
    "coint_spread = data['V1X'] - np.polyfit(data['V2X'], data['V1X'], 1)[0] * data['V2X']\n",
    "axes[1].plot(coint_spread, label='Cointegration Spread', color='orange')\n",
    "axes[1].axhline(coint_spread.mean(), color='green', linestyle='--', label='Mean')\n",
    "axes[1].axhline(coint_spread.mean() + entry_threshold * coint_spread.std(),\n",
    "                color='red', linestyle='--', label='Upper Bound')\n",
    "axes[1].axhline(coint_spread.mean() - entry_threshold * coint_spread.std(),\n",
    "                color='red', linestyle='--', label='Lower Bound')\n",
    "axes[1].set_title(\"Cointegration Spread Pair Trading\")\n",
    "axes[1].legend()\n",
    "axes[1].grid()\n",
    "\n",
    "# Kalman Filter Spread Plot\n",
    "kalman_spread = pd.Series(kalman_signals['Spread'], index=data.index)\n",
    "axes[2].plot(kalman_spread, label='Kalman Filter Spread', color='green')\n",
    "axes[2].axhline(kalman_spread.mean(), color='green', linestyle='--', label='Mean')\n",
    "axes[2].axhline(kalman_spread.mean() + entry_threshold * kalman_spread.std(),\n",
    "                color='red', linestyle='--', label='Upper Bound')\n",
    "axes[2].axhline(kalman_spread.mean() - entry_threshold * kalman_spread.std(),\n",
    "                color='red', linestyle='--', label='Lower Bound')\n",
    "axes[2].set_title(\"Kalman Filter Spread Pair Trading\")\n",
    "axes[2].legend()\n",
    "axes[2].grid()\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.10 ('kaist311')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
