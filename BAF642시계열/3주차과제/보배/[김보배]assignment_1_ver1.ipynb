{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10a8f61a",
   "metadata": {},
   "source": [
    "# 0. 데이터 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e613740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import pmdarima as pm\n",
    "from arch import arch_model\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from mgarch import mgarch\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from statsmodels.tsa.vector_ar.vecm import VECM\n",
    "from pykalman import KalmanFilter\n",
    "import hurst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c0892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIX = pd.read_csv('VIX.csv',index_col=0,parse_dates=True)[['종가']].astype(float).sort_index(ascending=True)\n",
    "V2X = pd.read_csv('V2X.csv',index_col=0,parse_dates=True)[['Price']].astype(float).sort_index(ascending=True)\n",
    "VIX_df = pd.merge(VIX,V2X,left_index=True,right_index=True,how='inner')\n",
    "VIX_df.columns = ['V1X','V2X']\n",
    "VIX_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86c8ec9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eaa248",
   "metadata": {},
   "source": [
    "## 1. V1X와 V2X의 시계열을 추세, 계절성 및 잡음으로 분해하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e70a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시계열 분해\n",
    "result_V1X = seasonal_decompose(VIX_df['V1X'], model='additive', period=12)\n",
    "result_V2X = seasonal_decompose(VIX_df['V2X'], model='additive', period=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38415ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decomposition(result, title):\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(12, 8), sharex=True)\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    axes[0].plot(result.observed, label='Observed')\n",
    "    axes[0].set_ylabel('Observed')\n",
    "\n",
    "    axes[1].plot(result.trend, label='Trend')\n",
    "    axes[1].set_ylabel('Trend')\n",
    "\n",
    "    axes[2].plot(result.seasonal, label='Seasonal')\n",
    "    axes[2].set_ylabel('Seasonal')\n",
    "\n",
    "    axes[3].plot(result.resid, label='Residual')\n",
    "    axes[3].set_ylabel('Residual')\n",
    "    axes[3].set_xlabel('Time')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa1a46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIX 시계열 분해 시각화\n",
    "plot_decomposition(result_V1X, 'VIX Decomposition')\n",
    "plot_decomposition(result_V2X, 'V2X Decomposition')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31a7b0a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508434ec",
   "metadata": {},
   "source": [
    "## 2. VIX와 V2X 각각에 대해 단위근 테스트를 실행하고 정상성을 살펴보라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80a47dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented Dickey-Fuller Test\n",
    "def adf_test(timeseries):\n",
    "    result = adfuller(timeseries)\n",
    "    print('ADF Statistic: %f' % result[0])\n",
    "    print('p-value: %f' % result[1])\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print('\\t%s: %.3f' % (key, value))\n",
    "\n",
    "adf_test(VIX_df['V1X'])\n",
    "adf_test(VIX_df['V2X'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e010bc2",
   "metadata": {},
   "source": [
    "### Augmented Dickey-Fuller (ADF) 단위근 테스트 결과\n",
    "\n",
    "가설\n",
    "- 귀무가설 ( H_0 ): 시계열 데이터는 단위근을 가지고 있으며 비정상성을 띤다.\n",
    "- 대립가설 ( H_1 ): 시계열 데이터는 단위근이 없으며 정상성을 띤다\n",
    "\n",
    "---\n",
    "\n",
    " 결과\n",
    "\n",
    " VIX \n",
    "- ADF 통계량: -5.504838\n",
    "- p-value: 0.000002\n",
    "- 임계값:\n",
    "  - 1%: -3.433\n",
    "  - 5%: -2.863\n",
    "  - 10%: -2.567\n",
    "\n",
    "결론: \n",
    "- ADF 통계량(-5.504838)이 모든 임계값보다 작고 p-value(0.000002)가 0.05보다 훨씬 작다.\n",
    "- 따라서,  H_0 를 기각한다.  VIX  시계열은 정상성을 가진다.\n",
    "\n",
    "\n",
    " V2X \n",
    "- ADF 통계량: -6.280245\n",
    "- p-value: 0.000000\n",
    "- 임계값:\n",
    "  - 1%: -3.433\n",
    "  - 5%: -2.863\n",
    "  - 10%: -2.567\n",
    "\n",
    "결론: \n",
    "- ADF 통계량(-6.280245)이 모든 임계값보다 작고 p-value(0.000000)가 0.05보다 훨씬 작다.\n",
    "- 따라서,  H_0 를 기각한다.  V2X  시계열은 정상성을 가진다.\n",
    "\n",
    "---\n",
    "\n",
    "최종 결론\n",
    " VIX 와  V2X  모두 Augmented Dickey-Fuller 테스트 결과 정상성을 만족한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edb25fa",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. VIX와 V2X 각각에 대해 ARIMA GARCH 모델을 적용하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b3c2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA 차수 선택을 알아서 해주는 auto_arima 함수\n",
    "model_V1X = pm.auto_arima(VIX_df['V1X'], seasonal=False, stepwise=True, \n",
    "                        suppress_warnings=True, error_action=\"ignore\", trace=True)\n",
    "model_V2X = pm.auto_arima(VIX_df['V2X'], seasonal=False, stepwise=True, \n",
    "                        suppress_warnings=True, error_action=\"ignore\", trace=True)\n",
    "\n",
    "model_V1X.fit(VIX_df['V1X'])\n",
    "model_V2X.fit(VIX_df['V2X'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f440020",
   "metadata": {},
   "source": [
    "auto_arima를 사용해  VIX와  V2X 시계열 각각에 대해 최적의 p, d, q값을 자동으로 선택하고 ARIMA 모델을 적합시켰다.\n",
    "\n",
    "---\n",
    "\n",
    "결과창:\n",
    "ARIMA(3,1,2)(0,0,0)[0]             : AIC=11653.045, Time=0.69 sec\n",
    "\n",
    "최종적으로 VIX 의 최적 모델은 AIC가 가장 작은 ARIMA(3,1,2)로 선택했다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d96f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_V1X.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72834afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_V2X.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eb9204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA-GARCH\n",
    "\n",
    "# 잔차 추출\n",
    "residuals_V1X = model_V1X.resid()\n",
    "residuals_V2X = model_V2X.resid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77548dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arch_model을 활용해 GARCH 모델 최적화 수행\n",
    "# AIC 기준으로 최적의 p, q 찾기 iteration\n",
    "def find_best_garch(residuals):\n",
    "    parameters = product(range(1, 4), range(1, 4))\n",
    "    best_aic = float('inf')\n",
    "    best_param = None\n",
    "\n",
    "    for param in parameters:\n",
    "        try:\n",
    "            model = arch_model(residuals, vol='Garch', p=param[0], q=param[1])\n",
    "            result = model.fit(disp='off')\n",
    "            if result.aic < best_aic:\n",
    "                best_aic = result.aic\n",
    "                best_param = param\n",
    "        except:\n",
    "            continue\n",
    "    return best_param, best_aic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a091ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param_V1X, best_aic_V1X = find_best_garch(residuals_V1X)\n",
    "print(f\"Best GARCH(p, q) for V1X: {best_param_V1X}, AIC: {best_aic_V1X}\")\n",
    "\n",
    "best_param_V2X, best_aic_V2X = find_best_garch(residuals_V2X)\n",
    "print(f\"Best GARCH(p, q) for V2X: {best_param_V2X}, AIC: {best_aic_V2X}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455d72ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적의 GARCH 모델 학습\n",
    "garch_V1X = arch_model(residuals_V1X, vol='GARCH', p=best_param_V1X[0], q=best_param_V1X[1])\n",
    "garch_V2X = arch_model(residuals_V2X, vol='GARCH', p=best_param_V2X[0], q=best_param_V2X[1])\n",
    "\n",
    "result_garch_V1X = garch_V1X.fit()\n",
    "result_garch_V2X = garch_V2X.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aad30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 출력\n",
    "print(result_garch_V1X.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40361a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_garch_V2X.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcbdb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA-GARCH 모델의 잔차 분석\n",
    "# ACF, PACF 그래프를 통해 잔차가 백색잡음인지 확인\n",
    "## V1X\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Residuals ACF\n",
    "plot_acf(result_garch_V1X.resid, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('ACF of Residuals (V1X)')\n",
    "\n",
    "# Residuals PACF\n",
    "plot_pacf(result_garch_V1X.resid, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('PACF of Residuals (V1X)')\n",
    "\n",
    "# Squared Residuals ACF\n",
    "plot_acf(result_garch_V1X.resid**2, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('ACF of Squared Residuals (V1X)')\n",
    "\n",
    "# Squared Residuals PACF\n",
    "plot_pacf(result_garch_V1X.resid**2, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('PACF of Squared Residuals (V1X)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7745390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA-GARCH 모델의 잔차 분석\n",
    "# ACF, PACF 그래프를 통해 잔차가 백색잡음인지 확인\n",
    "## V2X\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Residuals ACF\n",
    "plot_acf(result_garch_V2X.resid, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('ACF of Residuals (V2X)')\n",
    "\n",
    "# Residuals PACF\n",
    "plot_pacf(result_garch_V2X.resid, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('PACF of Residuals (V2X)')\n",
    "\n",
    "# Squared Residuals ACF\n",
    "plot_acf(result_garch_V2X.resid**2, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('ACF of Squared Residuals (V2X)')\n",
    "\n",
    "# Squared Residuals PACF\n",
    "plot_pacf(result_garch_V2X.resid**2, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('PACF of Squared Residuals (V2X)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c436c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ljung-Box 검정\n",
    "## 변동성이 백색잡음인지 확인\n",
    "lb_test_V1X = acorr_ljungbox(result_garch_V1X.resid**2, return_df=True)\n",
    "print('-'*50)\n",
    "print('VIX')\n",
    "print(lb_test_V1X)\n",
    "lb_test_V2X = acorr_ljungbox(result_garch_V2X.resid**2, return_df=True)\n",
    "print('-'*50)\n",
    "print('V2X')\n",
    "print(lb_test_V2X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df326ab4",
   "metadata": {},
   "source": [
    "### ARIMA-GARCH 모델 잔차 분석 결과 해석\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. 잔차의 ACF (왼쪽 위)\n",
    "- 잔차의 ACF에서 모든 시차가 신뢰구간 내에 있다.\n",
    "- 즉 잔차가 자기상관이 없는 white noise에 가깝다.\n",
    "- 결론: ARIMA 모델이 평균값을 잘 설명했다.\n",
    "\n",
    "\n",
    "#### 2. 잔차의 PACF (오른쪽 위)\n",
    "- 잔차의 PACF도 모든 시차에서 신뢰구간 내에 있다.\n",
    "- 즉 잔차에 추가적인 AR 구조가 없음을 의미한다.\n",
    "- 결론: ARIMA 모델이 적절히 평균값의 자기상관을 제거했다.\n",
    "\n",
    "\n",
    "#### 3. 잔차 제곱의 ACF (왼쪽 아래)\n",
    "- 잔차 제곱의 ACF에서 일부 시차에서 신뢰구간을 약간 벗어난다.\n",
    "- 즉 변동성 클러스터링이 완전히 제거되지 않았을 가능성이 있다.\n",
    "- 결론: GARCH(2,2)(V1X), GARCH(3,2)(V2X) 모델이 변동성을 대부분 잘 설명했지만 추가적인 변동성을 더 설명할 여지가 있다.\n",
    "\n",
    "\n",
    "#### 4. 잔차 제곱의 PACF (오른쪽 아래)\n",
    "- 잔차 제곱의 PACF도 초기 몇 개의 시차에서 신뢰구간을 약간 벗어난다.\n",
    "- 즉 잔차 제곱의 자기상관 중 일부가 GARCH(2,2)(V1X), GARCH(3,2)(V2X) 모델로 충분히 제거되지 않았음을 나타낸다.\n",
    "- 결론: 모델 개선 여지가 존재한다.\n",
    "\n",
    "\n",
    "#### 5. Ljung-Box 검정\n",
    "1.\t가설 :\n",
    "\n",
    "\t•\t H_0 : 잔차의 제곱값에 자기상관이 없다 (백색잡음).\n",
    "\t\n",
    "\t•\t H_1 : 잔차의 제곱값에 자기상관이 있다 (백색잡음 아님).\n",
    "2.\t결과 해석:\n",
    "\n",
    "\t•\tp-value:\n",
    "\n",
    "\t•\t p < 0.05 : 귀무가설( H_0 )을 기각 → 자기상관이 있음 → 모델이 변동성을 충분히 설명하지 못한다.\n",
    "\n",
    "\t•\t p >= 0.05 : 귀무가설( H_0 )을 채택 → 자기상관이 없음 → 모델이 변동성을 잘 설명한다.\n",
    "3.\t결과 값:\n",
    "\n",
    "\t•\t VIX 와  V2X  모두 모든 lag에서 p-value가 매우 작다 ( p < 0.05 ).\n",
    "\t\n",
    "\t•\t이는 잔차의 제곱값에 자기상관이 존재함을 나타낸다.\n",
    "---\n",
    "\n",
    "### 결론\n",
    "1. ARIMA 모델 적합성:\n",
    "   - 잔차의 ACF와 PACF 모두 신뢰구간 내에 있어 ARIMA 모델이 평균값을 잘 설명한다.\n",
    "   - 추가적인 평균 자기상관은 존재하지 않는다.\n",
    "\n",
    "2. GARCH 모델 적합성:\n",
    "   - 잔차 제곱의 ACF와 PACF에서 초기 시차에서 약간의 자기상관이 남아 있다.\n",
    "   - Ljung-Box 검정결과에서도 자기상관이 존재한다.\n",
    "   - 변동성 클러스터링을 완전히 제거하지 못했을 가능성이 있으며 GARCH 모델의 구조를 개선할 필요가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a11146",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. VIX와 V2X에 대해 Multivariate GARCH 모델를 적용해보라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf15ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로그 수익률 계산\n",
    "VIX_df2 = VIX_df.copy()\n",
    "VIX_df2['V1X_return'] = np.log(VIX_df2['V1X'] / VIX_df2['V1X'].shift(1))\n",
    "VIX_df2['V2X_return'] = np.log(VIX_df2['V2X'] / VIX_df2['V2X'].shift(1))\n",
    "\n",
    "# 로그 수익률 데이터 추출\n",
    "VIX_df2 = VIX_df2[['V1X_return', 'V2X_return']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9906652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcc_model = mgarch(dist='norm')\n",
    "result = dcc_model.fit(VIX_df2.values)\n",
    "print(\"DCC-GARCH Model Results:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1005825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동적 상관관계 계산 및 시각화\n",
    "R_t = []\n",
    "Q_bar = np.cov(dcc_model.rt.T)  # 무조건부 공분산 행렬\n",
    "Q_t = Q_bar  # 초기 Q_t 설정\n",
    "\n",
    "for t in range(dcc_model.T):\n",
    "    diag_D = np.diag(dcc_model.D_t[t])  # D_t의 대각 행렬\n",
    "    D_inv = np.linalg.inv(diag_D)  # D_t의 역행렬\n",
    "    et = D_inv @ dcc_model.rt[t].T  # 표준화된 잔차\n",
    "    # Q_t 업데이트 (조건부 공분산 행렬)\n",
    "    Q_t = (1 - dcc_model.a - dcc_model.b) * Q_bar + dcc_model.a * (et @ et.T) + dcc_model.b * Q_t\n",
    "    # Q_t를 상관행렬 R_t로 변환\n",
    "    diag_Q = np.sqrt(np.diag(Q_t))\n",
    "    R = Q_t / np.outer(diag_Q, diag_Q)  # 정규화하여 상관행렬 생성\n",
    "    R_t.append(R)\n",
    "\n",
    "# 동적 상관계수 추출 및 시각화\n",
    "dynamic_corr = [R[0, 1] for R in R_t]  # 첫 번째와 두 번째 변수 간 상관계수\n",
    "plt.plot(dynamic_corr)\n",
    "plt.title(\"Dynamic Conditional Correlation (DCC-GARCH)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7911f3",
   "metadata": {},
   "source": [
    "DCC-GARCH 모델을 사용하여 VIX와 V2X의 동적 조건부 상관관계를 분석한 결과 두 시장의 변동성 간의 관계가 시간에 따라 복잡하게 변화한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f13e009",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. V1X와 V2X를 이용해 VAR 모델을 만들고, Granger causality 테스트를 실행하고,Impulse response 그래프와 분산 분해(Varinace Decomposion)을 계산하고 그래프로 보여라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f37da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAR 모델 적합\n",
    "model_var = VAR(VIX_df[['V1X', 'V2X']]) # V1X, V2X 변수가 정상성을 만족했기 때문에 바로 VAR 모델 적합\n",
    "results_var = model_var.fit(maxlags=15, ic='aic') # 최적의 lag 차수는 AIC 기준으로 선택\n",
    "\n",
    "# 결과 출력\n",
    "print(results_var.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcb1385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Granger causality 테스트\n",
    "# V1X_return이 V2X_return에 미치는 영향\n",
    "granger_v1_to_v2 = results_var.test_causality('V1X', 'V2X', kind='f')\n",
    "print(granger_v1_to_v2.summary())\n",
    "\n",
    "# V2X_return이 V1X_return에 미치는 영향\n",
    "granger_v2_to_v1 = results_var.test_causality('V2X', 'V1X', kind='f')\n",
    "print(granger_v2_to_v1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3a5cf4",
   "metadata": {},
   "source": [
    "### Granger Causality 테스트 결과\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. V2X → V1X\n",
    "- 귀무가설 (H_0): \"V2X는 V1X를 Granger-cause 하지 않는다.\"\n",
    "- Test statistic: 2.616\n",
    "- Critical value: 1.668\n",
    "- p-value: 0.001\n",
    "- 결론: \n",
    "  - p-value가 5% 유의수준보다 작으므로 귀무가설을 기각한다.\n",
    "  - V2X는 V1X를 Granger-cause한다.\n",
    "\n",
    "\n",
    "\n",
    "#### 2. V1X → V2X\n",
    "- 귀무가설 (H_0): \"V1X는 V2X를 Granger-cause 하지 않는다.\"\n",
    "- Test statistic: 24.91\n",
    "- Critical value: 1.668\n",
    "- p-value: 0.000\n",
    "- 결론: \n",
    "  - p-value가 5% 유의수준보다 작으므로 귀무가설을 기각한다.\n",
    "  - V1X는 V2X를 Granger-cause한다.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. 결론\n",
    "- 양방향 Granger causality 관계가 존재한다:\n",
    "    - V2X가 V1X에 Granger causality를 미치며, 반대로 V1X도 V2X에 Granger causality를 미친다.\n",
    "    - 즉 V2X(유럽 변동성 지수)가 과거 정보를 통해 V1X(미국 변동성 지수)의 변동성을 예측하는 데 기여할 수 있음.\n",
    "    - 반대로 V1X(미국 변동성 지수)가 과거 정보를 통해 V2X(유럽 변동성 지수)의 변동성을 예측하는 데 기여할 수 있음.\n",
    "- Test statistic에 따라 V1X → V2X의 영향 강도가 더 강한 것으로 보인다.\n",
    "- 공적분 검정을 통해 두 지수 간의 장기적인 균형 관계를 추가적으로 검토해야하며, VAR 대신 VECM을 고려할 필요가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f697fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impulse response 분석\n",
    "irf = results_var.irf(10)   # 10개의 기간에 대한 impulse response\n",
    "irf.plot(orth=False)    # orthogonalized 하지 않은 충격반응을 보여주는 그래프\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5786a9",
   "metadata": {},
   "source": [
    "### Impulse Response Function 해석\n",
    "\n",
    "#### 1. V1X → V1X (좌상단)\n",
    "- 설명:\n",
    "  - V1X가 자기 자신에 미치는 충격.\n",
    "  - 초기에는 큰 영향을 미치지만 시간이 지남에 따라 점진적으로 감소하며 0으로 수렴한다.\n",
    "- 해석:\n",
    "  - V1X의 충격은 단기적으로 강하게 작용하지만 장기적으로 사라지는 특성을 보인다.\n",
    "\n",
    "\n",
    "#### 2. V2X → V1X (우상단)\n",
    "- 설명:\n",
    "  - V2X가 V1X에 미치는 충격.\n",
    "  - 초기에는 영향이 작지만 시간이 지나면서 증가하다가 점진적으로 감소한다.\n",
    "- 해석:\n",
    "  - 유럽 변동성 지수(V2X)의 변화가 미국 변동성 지수(V1X)에 시차를 두고 유의미한 영향을 미친다.\n",
    "\n",
    "\n",
    "#### 3. V1X → V2X (좌하단)\n",
    "- 설명:\n",
    "  - V1X가 V2X에 미치는 충격.\n",
    "  - 초기에는 큰 영향을 미치며 시간이 지남에 따라 점진적으로 감소한다.\n",
    "- 해석:\n",
    "  - 미국 변동성 지수(V1X)는 유럽 변동성 지수(V2X)에 단기적으로 강한 영향을 미치며 시간이 지나면서 그 영향이 감소한다.\n",
    "\n",
    "\n",
    "#### 4. V2X → V2X (우하단)\n",
    "- 설명:\n",
    "  - V2X가 자기 자신에 미치는 충격.\n",
    "  - 초기에는 큰 영향을 미치지만 시간이 지남에 따라 점진적으로 감소하며 0으로 수렴한다.\n",
    "- 해석:\n",
    "  - V2X의 충격은 자기 자신에게 단기적으로 강하게 작용하지만 장기적으로는 사라지는 특성을 보인다.\n",
    "\n",
    "---\n",
    "\n",
    "### 결론\n",
    "1. 자기 자신에 대한 충격:\n",
    "   - V1X와 V2X 모두 자기 자신에 대한 충격에 대해 초기 반응은 크지만 시간이 지나면서 감소한다.\n",
    "   - 즉 금융 데이터의 일반적인 특성(충격이 단기적으로 강하고, 장기적으로 소멸)을 반영한다.\n",
    "\n",
    "2. 상호 변수 간의 충격:\n",
    "   - V1X가 V2X에 미치는 영향(V1X → V2X)이 V2X가 V1X에 미치는 영향(V2X → V1X)보다 초기 반응이 강하다.\n",
    "   - 즉 미국 변동성 지수(V1X)가 유럽 변동성 지수(V2X)에 더 빠르고 강하게 영향을 미치는 구조를 시사한다.\n",
    "   - V2X 충격이 V1X에 미치는 영향은 일정한 시차를 두고 발생한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dc62fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance Decomposition 분석\n",
    "vd = results_var.fevd(10)   # 10개의 기간에 대한 Variance Decomposition\n",
    "vd.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3806b521",
   "metadata": {},
   "source": [
    "### Variance Decomposition분석\n",
    "---\n",
    "\n",
    "#### 1. V1X의 Variance Decomposition (상단)\n",
    "- 설명:\n",
    "  - V1X의 예측 오차 분산이 대부분 V1X 자체에 의해 설명된다.\n",
    "  - 다른 변수인 V2X는 V1X의 예측 오차 분산에 거의 기여하지 않는다.\n",
    "- 해석:\n",
    "  - V1X는 자기 자신에 의한 설명력이 매우 높으며 V2X의 충격은 V1X의 예측 오차 분산에 유의미한 영향을 미치지 않는다.\n",
    "  - 즉 V1X가 독립적인 성격을 가질 가능성을 시사한다.\n",
    "\n",
    "\n",
    "#### 2. V2X의 Variance Decomposition (하단)\n",
    "- 설명:\n",
    "  - V2X의 예측 오차 분산은 초기에는 V1X와 V2X 모두에 의해 설명된다.\n",
    "  - 시간이 지남에 따라 V1X의 기여도는 점차 감소하고 V2X 자체의 기여도가 증가한다.\n",
    "- 해석:\n",
    "  - 초기에는 V1X가 V2X의 예측 오차 분산에 영향을 미치지만 시간이 지남에 따라 V2X가 점점 더 큰 설명력을 가지게 된다.\n",
    "  - 즉 단기적으로는 V1X의 영향이 중요하지만 장기적으로 V2X의 자체적인 영향이라는 것을 시사한다.\n",
    "\n",
    "---\n",
    "\n",
    "#### 결론\n",
    "1. V1X는 독립적인 변수:\n",
    "   - V1X의 예측 오차 분산은 거의 전적으로 자기 자신에 의해 설명된다.\n",
    "   - 이는 V1X가 외부 충격보다 내부 요인에 의해 좌우됨을 시사한다.\n",
    "\n",
    "2. V2X는 V1X의 영향을 받음:\n",
    "   - V2X의 예측 오차 분산은 초기에는 V1X에 의해 상당 부분 설명되며 이는 V1X가 V2X에 단기적으로 영향을 미친다.\n",
    "   - 그러나 장기적으로는 V2X 자체가 지배적인 설명력을 가진다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d24a25f",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. V1X와 V2X를 이용해 공적분관계를 확인하고, VECM 모델을 구축하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a746e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Johansen 공적분 검정\n",
    "result_coint = coint_johansen(VIX_df[['V1X', 'V2X']], det_order=0, k_ar_diff=1)\n",
    "\n",
    "# Trace Statistic과 임계값 출력\n",
    "print(\"Trace Statistic:\", result_coint.lr1)\n",
    "print(\"Critical Values (Trace):\", result_coint.cvt)\n",
    "\n",
    "# Maximum Eigenvalue Statistic과 임계값 출력\n",
    "print(\"Maximum Eigenvalue Statistic:\", result_coint.lr2)\n",
    "print(\"Critical Values (Max-Eigen):\", result_coint.cvm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d67746",
   "metadata": {},
   "source": [
    "### 요한센 공적분 검정 결과 해석\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Trace Statistic 해석\n",
    "1. 검정 통계량 (Trace Statistic): [110.98581484, 36.36566577]\n",
    "   - 각 값은 공적분 벡터의 수 (r)에 따라 계산된 통계량을 의미한다.\n",
    "   - 첫 번째 값: r = 0 (공적분 벡터가 없다는 귀무가설에 대한 검정).\n",
    "   - 두 번째 값: r = 1 (최대 1개의 공적분 벡터가 있다는 귀무가설에 대한 검정).\n",
    "\n",
    "2. 임계값 (Critical Values):\n",
    "   - r = 0: [13.4294, 15.4943, 19.9349] (10%, 5%, 1% 유의수준).\n",
    "   - r = 1: [2.7055, 3.8415, 6.6349] (10%, 5%, 1% 유의수준).\n",
    "\n",
    "#### 결과 해석:\n",
    "1. 첫 번째 공적분 벡터 (r = 0):\n",
    "   - 검정 통계량 (110.99) > 임계값 (19.93 at 1% 유의수준).\n",
    "   - 귀무가설(r = 0, 공적분 벡터가 없음)을 기각 → 최소 1개 이상의 공적분 관계가 존재한다.\n",
    "\n",
    "2. 두 번째 공적분 벡터 (r = 1):\n",
    "   - 검정 통계량 (36.37) > 임계값 (6.63 at 1% 유의수준).\n",
    "   - 귀무가설(r \\leq 1, 최대 1개의 공적분 벡터가 존재)을 기각 → 최소 2개 이상의 공적분 관계가 존재 가능하다.\n",
    "\n",
    "\n",
    "#### 2. Maximum Eigenvalue Statistic 해석\n",
    "1. 검정 통계량 (Max-Eigen Statistic): [74.62014907, 36.36566577]\n",
    "   - 각 값은 공적분 벡터의 수 (r)에 따라 계산된 통계량을 의미한다.\n",
    "   - 첫 번째 값: r = 0 (공적분 벡터가 없다는 귀무가설에 대한 검정).\n",
    "   - 두 번째 값: r = 1 (최대 1개의 공적분 벡터가 있다는 귀무가설에 대한 검정).\n",
    "\n",
    "2. 임계값 (Critical Values):\n",
    "   - r = 0: [12.2971, 14.2639, 18.52] (10%, 5%, 1% 유의수준).\n",
    "   - r = 1: [2.7055, 3.8415, 6.6349] (10%, 5%, 1% 유의수준).\n",
    "\n",
    "---\n",
    "\n",
    "#### 결과 해석:\n",
    "1. 첫 번째 공적분 벡터 (r = 0):\n",
    "   - 검정 통계량 (74.62) > 임계값 (18.52 at 1% 유의수준).\n",
    "   - 귀무가설(r = 0, 공적분 벡터가 없음)을 기각 → 최소 1개 이상의 공적분 관계가 존재한다.\n",
    "\n",
    "2. 두 번째 공적분 벡터 (r = 1):\n",
    "   - 검정 통계량 (36.37) > 임계값 (6.63 at 1% 유의수준).\n",
    "   - 귀무가설(r = 1, 최대 1개의 공적분 벡터가 존재)을 기각 → 최소 2개 이상의 공적분 관계가 존재 가능하다.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. 결론\n",
    "- Trace Statistic과 Maximum Eigenvalue Statistic 모두 r = 1에 대해 귀무가설을 기각하므로, 최소 2개의 공적분 관계가 존재한다고 결론내릴 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9bf55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VECM 모델 적합\n",
    "model_vecm = VECM(VIX_df[['V1X', 'V2X']], k_ar_diff=1, coint_rank=2)\n",
    "results_vecm = model_vecm.fit()\n",
    "\n",
    "# 결과 출력\n",
    "print(results_vecm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cc5dd8",
   "metadata": {},
   "source": [
    "### VECM 결과 해석\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. 각 방정식의 계수\n",
    "\n",
    "#### 1.1 V1X 방정식\n",
    "\n",
    "$$\n",
    "  \\Delta V1X_t = \\alpha_1 \\cdot ec1 + \\alpha_2 \\cdot ec2 + \\beta_1 \\cdot \\Delta V1X_{t-1} + \\beta_2 \\cdot \\Delta V2X_{t-1} + \\epsilon_t\n",
    "$$\n",
    "\n",
    "- L1.V1X:\n",
    "  - 계수: -0.1497, p-value < 0.01 → 유의미.\n",
    "  - 이전 시점 V1X의 변화가 현재 V1X에 음의 영향을 미침.\n",
    "- L1.V2X:\n",
    "  - 계수: -0.0093, p-value = 0.686 → 유의미하지 않음.\n",
    "  - V2X의 변화는 현재 V1X에 거의 영향을 미치지 않음.\n",
    "\n",
    "#### 1.2 V2X 방정식\n",
    "$$\n",
    "\\Delta V2X_t = \\alpha_3 \\cdot ec1 + \\alpha_4 \\cdot ec2 + \\beta_3 \\cdot \\Delta V1X_{t-1} + \\beta_4 \\cdot \\Delta V2X_{t-1} + \\epsilon_t\n",
    "$$\n",
    "\n",
    "- L1.V1X:\n",
    "  - 계수: 0.2897, p-value < 0.01 → 유의미.\n",
    "  - 이전 시점 V1X의 변화가 현재 V2X에 양의 영향을 미친다.\n",
    "- L1.V2X:\n",
    "  - 계수: -0.2227, p-value < 0.01 → 유의미.\n",
    "  - 이전 시점 V2X의 변화가 현재 V2X에 음의 영향을 미친다.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 오차 수정 항 (Error Correction Term, ec)\n",
    "\n",
    "#### 2.1 V1X의 오차 수정 계수 (alpha_1, alpha_2)\n",
    "- ec1: -0.0334, p-value < 0.01 → 유의미.\n",
    "  - V1X는 장기 균형에서 벗어날 경우 균형으로 복귀하려는 음의 조정 메커니즘이 작동한다.\n",
    "- ec2: 0.0269, p-value < 0.01 → 유의미.\n",
    "  - V1X는 장기 균형에서 벗어날 경우 추가적인 양의 조정 메커니즘이 작동한다.\n",
    "\n",
    "#### 2.2 V2X의 오차 수정 계수 (alpha_3, alpha_4)\n",
    "- ec1: 0.0331, p-value < 0.01 → 유의미.\n",
    "  - V2X는 장기 균형에서 벗어날 경우 균형으로 복귀하려는 양의 조정 메커니즘이 작동한다.\n",
    "- ec2: -0.0318, p-value < 0.01 → 유의미.\n",
    "  - V2X는 장기 균형에서 벗어날 경우 음의 조정 메커니즘이 작동한다.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. 공적분 벡터 (Cointegration Relations)\n",
    "\n",
    "#### 3.1 첫 번째 공적분 벡터 (ec1)\n",
    "- beta.1: 1.0000\n",
    "- beta.2: $$ 1.199 \\times 10^{-17} $$ (0에 가까움)\n",
    "- 해석:\n",
    "  - ec1은 V1X의 장기적인 균형 관계.\n",
    "\n",
    "#### 3.2 두 번째 공적분 벡터 (ec2)\n",
    "- beta.1: $$ 4.085 \\times 10^{-18}$$ (0에 가까움)\n",
    "- beta.2: 1.0000\n",
    "- 해석:\n",
    "  - ec2는 V2X의 장기적인 균형 관계.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. 결론\n",
    "1. 장기 관계:\n",
    "   - V1X와 V2X는 두 개의 공적분 관계를 통해 장기적인 균형을 유지한다.\n",
    "   - 균형에서 벗어날 경우 오차 수정 항(ec)을 통해 복귀 메커니즘이 작동한다.\n",
    "\n",
    "2. 단기 관계:\n",
    "   - V1X는 V2X의 변화에 민감하지 않지만, V2X는 V1X의 변화에 민감하게 반응한다.\n",
    "   - 즉 미국 변동성 지수(V1X)가 유럽 변동성 지수(V2X)에 강한 단기적 영향을 미친다는 것을 의미한다.(L1.V1X = 0.2897)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2b5b0e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 페어트레이딩 (트레이딩 경계: 평균 +/- some 표준편차)\n",
    "\n",
    "(1) 단순 스프레드를 이용한 페어 트레이딩\n",
    "\n",
    "(2) 공적분관계를 이용한 페어트레이딩\n",
    "\n",
    "(3) 칼만 필터를 이용한 페어트레이딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6effc888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트레이딩은 선물 데이터를 사용\n",
    "VIX_f = pd.read_csv('VIX_futures.csv',index_col=0,parse_dates=True)[['종가']].astype(float).sort_index(ascending=True)\n",
    "V2X_f = pd.read_csv('V2X_futures.csv',index_col=0,parse_dates=True)[['Price']].astype(float).sort_index(ascending=True)\n",
    "EUR_USD = pd.read_csv('EUR_USD.csv',index_col=0,parse_dates=True)[['Price']].astype(float).sort_index(ascending=True)\n",
    "\n",
    "VIX_f_df = pd.merge(VIX_f,V2X_f,left_index=True,right_index=True,how='inner')\n",
    "VIX_f_df = pd.merge(VIX_f_df,EUR_USD,left_index=True,right_index=True,how='inner')\n",
    "VIX_f_df.columns = ['V1X_F','V2X_F','EUR_USD']\n",
    "\n",
    "VIX_f_df['V2X_F'] = (VIX_f_df['V2X_F'] * VIX_f_df['EUR_USD']).round(2) # 환율 적용\n",
    "VIX_f_df = VIX_f_df.drop(columns='EUR_USD')\n",
    "\n",
    "Total_VIX_df = pd.merge(VIX_df,VIX_f_df,left_index=True,right_index=True,how='inner')\n",
    "Total_VIX_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35c5993",
   "metadata": {},
   "source": [
    "### 7.1 단순 스프레드를 이용한 페어 트레이딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8910326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) 단순 스프레드 기반 페어 트레이딩 전략\n",
    "def simple_spread_strategy(df, lookback, k):\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    df['spread'] = df['V1X'] - df['V2X']\n",
    "    df['mu'] = df['spread'].rolling(window=lookback).mean()\n",
    "    df['sigma'] = df['spread'].rolling(window=lookback).std()\n",
    "\n",
    "    # thresholds\n",
    "    df['low'] = df['mu'] - k * df['sigma']\n",
    "    df['high'] = df['mu'] + k * df['sigma']\n",
    "\n",
    "    # positions\n",
    "    df['position'] = 0.0  # Initialize position\n",
    "    df.loc[df['spread'] < df['low'], 'position'] = (df['low'] - df['spread']) / df['sigma'] # z-score화\n",
    "    df.loc[df['spread'] > df['high'], 'position'] = -(df['spread'] - df['high']) / df['sigma']  # z-score화\n",
    "    df.loc[(df['spread'] > df['mu']) & (df['spread'].shift(1) < df['mu']), 'position'] = 0.0  # 스프레드가 평균을 상향 돌파할 때 청산\n",
    "    df.loc[(df['spread'] < df['mu']) & (df['spread'].shift(1) > df['mu']), 'position'] = 0.0  # 스프레드가 평균을 하향 돌파할 때 청산\n",
    "    \n",
    "    return df\n",
    "\n",
    "def smooth_positions(df, min_change=0.05, max_change=0.2, max_position=0.5):\n",
    "\n",
    "    df = df.copy()\n",
    "    df['smoothed_position'] = df['position']\n",
    "    for i in range(1, len(df)):\n",
    "        prev_position = df.loc[df.index[i-1], 'smoothed_position']\n",
    "        raw_position = df.loc[df.index[i], 'position']\n",
    "        \n",
    "        # Smoothing\n",
    "        # 변화량이 너무 작으면 이전 포지션 유지\n",
    "        # 변화량이 너무 크면 max_change로 제한\n",
    "        if abs(raw_position - prev_position) < min_change:\n",
    "            df.loc[df.index[i], 'smoothed_position'] = prev_position\n",
    "        else:\n",
    "            change = np.clip(raw_position - prev_position, -max_change, max_change)\n",
    "            new_position = prev_position + change\n",
    "            df.loc[df.index[i], 'smoothed_position'] = np.clip(new_position, -max_position, max_position)\n",
    "    df['position'] = df['smoothed_position']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd8e06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_simple = simple_spread_strategy(Total_VIX_df, lookback=40, k=1)\n",
    "signals_smoothed = smooth_positions(signals_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1bcd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spread(result, title):\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.plot(result.index, result['spread'], label='Spread', color='blue')\n",
    "    ax.plot(result.index, result['low'], linestyle='--', label='Lower', color='red')\n",
    "    ax.plot(result.index, result['high'], linestyle='--', label='Upper', color='green')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Spread')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f90828",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spread(signals_simple, 'Simple Spread Strategy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d0419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spread(signals_smoothed, 'Smoothed Spread Strategy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cca296c",
   "metadata": {},
   "source": [
    "### 7.2 Cointegration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e31b9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Cointegration 기반 페어 트레이딩 전략\n",
    "def coint_based_strategy(df, lookback, k):\n",
    "\n",
    "    from statsmodels.tsa.stattools import coint\n",
    "\n",
    "    df = df.copy()\n",
    "    # Initialize columns\n",
    "    df['spread'] = np.nan\n",
    "    df['mu'] = np.nan\n",
    "    df['high'] = np.nan\n",
    "    df['low'] = np.nan\n",
    "    \n",
    "    for i in range(lookback, len(df)):\n",
    "        window = df.iloc[i - lookback:i]\n",
    "        \n",
    "        score, p_value, _ = coint(window['V1X'], window['V2X'])\n",
    "        if p_value < 0.05: # Cointegration이 성립해야만 전략 수행\n",
    "            hedge_ratio = np.polyfit(window['V2X'], window['V1X'], 1)[0] # V1X = hedge_ratio * V2X + beta_0\n",
    "            spread = window['V1X'] - hedge_ratio * window['V2X']\n",
    "\n",
    "            mean = spread.mean()\n",
    "            sigma = spread.std()\n",
    "            high = mean + k * sigma\n",
    "            low = mean - k * sigma\n",
    "\n",
    "\n",
    "            df.loc[df.index[i], 'spread'] = spread.iloc[-1] # rolling한 후 현재 시점의 스프레드 값\n",
    "            df.loc[df.index[i], 'sigma'] = sigma\n",
    "            df.loc[df.index[i], 'mu'] = mean\n",
    "            df.loc[df.index[i], 'high'] = high\n",
    "            df.loc[df.index[i], 'low'] = low      \n",
    "        else:\n",
    "            # Cointegration이 성립하지 않으면 이전 값을 그대로 사용\n",
    "            df.loc[df.index[i], 'spread'] = df.loc[df.index[i - 1], 'spread']\n",
    "            df.loc[df.index[i], 'sigma'] = df.loc[df.index[i - 1], 'sigma']\n",
    "            df.loc[df.index[i], 'mu'] = df.loc[df.index[i - 1], 'mu']\n",
    "            df.loc[df.index[i], 'high'] = df.loc[df.index[i - 1], 'high']\n",
    "            df.loc[df.index[i], 'low'] = df.loc[df.index[i - 1], 'low']\n",
    "\n",
    "    # positions\n",
    "    df['position'] = 0.0  # Initialize position\n",
    "    df.loc[df['spread'] < df['low'], 'position'] = (df['low'] - df['spread']) / df['sigma'] # z-score화\n",
    "    df.loc[df['spread'] > df['high'], 'position'] = -(df['spread'] - df['high']) / df['sigma']  # z-score화\n",
    "    df.loc[(df['spread'] > df['mu']) & (df['spread'].shift(1) < df['mu']), 'position'] = 0.0  # 스프레드가 평균을 상향 돌파할 때 청산\n",
    "    df.loc[(df['spread'] < df['mu']) & (df['spread'].shift(1) > df['mu']), 'position'] = 0.0  # 스프레드가 평균을 하향 돌파할 때 청산\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeb2b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_coint = coint_based_strategy(Total_VIX_df, lookback=40, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d442793",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spread(signals_coint, 'Cointegration-based Spread Strategy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b45240",
   "metadata": {},
   "source": [
    "### 7.3 Kalman filter을 이용한 pair trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5587e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) Kalman Filter 기반 페어 트레이딩 전략\n",
    "def kalman_based_strategy(df, lookback, k):\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # 관측값 y_t가 상태 변수 beta_0, beta_1와 어떤 관계가 있는지를 정의하는 관측 행렬\n",
    "    obs_mat = sm.add_constant(df['V2X'].values, prepend=False)[:, np.newaxis] # y = beta_0 + beta_1 * x + epsilon ## 칼만필터 입력형식에 맞추기 위해 2차원 배열로 확장\n",
    "    delta = 1e-5\n",
    "    trans_cov = delta / (1 - delta) * np.eye(2) # 상태 전이 잡음의 분산은 delta / (1 - delta)\n",
    "\n",
    "    kf = KalmanFilter(n_dim_obs=1, n_dim_state=2, # 관측(y)은 1-dimensional, (beta_0, beta_2) 는 2-dimensional\n",
    "                    initial_state_mean=np.zeros(2), # 초기 상태 평균은 [0, 0]\n",
    "                    initial_state_covariance=np.ones((2, 2)),  # 초기 상태 공분산은 단위 행렬\n",
    "                    transition_matrices=np.eye(2), # 상태 전이 행렬은 단위 행렬로 설정\n",
    "                    observation_matrices=obs_mat, # 관측(y) 행렬은 관측값과 상태 변수의 관계를 나타내는 행렬\n",
    "                    observation_covariance=1.0, # 관측(y) 노이즈의 분산  # 1.0\n",
    "                    transition_covariance=trans_cov)    # 상태 전이 노이즈의 분산\n",
    "\n",
    "    state_means, state_covs = kf.filter(df['V1X'].values)   # y : V1X, x : V2X\n",
    "    df['state_beta_0']= pd.DataFrame(state_means[:, 0], index=df['V1X'].index, columns=['beta_0'])\n",
    "    df['state_beta_1']= pd.DataFrame(state_means[:, 1], index=df['V1X'].index, columns=['beta_1'])\n",
    "\n",
    "    df['spread'] = df['V1X'] - (df['state_beta_0'] + df['state_beta_1']* df['V2X'])\n",
    "        \n",
    "    df['mu'] = df['spread'].rolling(window=lookback).mean()\n",
    "    df['sigma'] = df['spread'].rolling(window=lookback).std()\n",
    "\n",
    "    # thresholds\n",
    "    df['low'] = df['mu'] - k * df['sigma']\n",
    "    df['high'] = df['mu'] + k * df['sigma']\n",
    "\n",
    "    # positions\n",
    "    df['position'] = 0.0  # Initialize position\n",
    "    df.loc[df['spread'] < df['low'], 'position'] = (df['low'] - df['spread']) / df['sigma'] # Z-score화\n",
    "    df.loc[df['spread'] > df['high'], 'position'] = -(df['spread'] - df['high']) / df['sigma']  # Z-score화\n",
    "    df.loc[(df['spread'] > df['mu']) & (df['spread'].shift(1) < df['mu']), 'position'] = 0.0  # 스프레드가 평균을 상향 돌파할 때 청산\n",
    "    df.loc[(df['spread'] < df['mu']) & (df['spread'].shift(1) > df['mu']), 'position'] = 0.0  # 스프레드가 평균을 하향 돌파할 때 청산\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0286af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_kalman = kalman_based_strategy(Total_VIX_df, lookback=40, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2875b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spread(signals_kalman, 'Kalman Filter-based Spread Strategy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398a800d",
   "metadata": {},
   "source": [
    "## 8. 최소 총 이익(MTP) 경계를 구하고 이를 이용한 백테스트 결과를 제시하라. (논문과 블로그참조)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da8fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(df,transaction_cost_v1x=0.0002,transaction_cost_v2x=0.0003):\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    df['ret_V1X'] = df['V1X_F'].pct_change()\n",
    "    df['ret_V2X'] = df['V2X_F'].pct_change()\n",
    "    \n",
    "    # TODO : transaction cost 계산\n",
    "    ## V1X : 0.02%, V2X : 0.03%\n",
    "    ## position_diff : 전일 대비 position의 변화량\n",
    "    ## transaction_cost, daily_returns, cumulative_returns 계산\n",
    "    df['position_diff'] = df['position'].diff().abs()\n",
    "    df['transaction_cost'] = (df['position_diff'] * (transaction_cost_v1x + transaction_cost_v2x) / 2).fillna(0)\n",
    "\n",
    "    df['daily_returns'] = (\n",
    "                        0.5 * (df['position'].shift(1) * df['ret_V1X'] - df['position'].shift(1) * df['ret_V2X']) \n",
    "                        - df['transaction_cost']\n",
    "    )\n",
    "    df['cumulative_returns'] = (1 + df['daily_returns']).cumprod()-1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2115f66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mtp(df, U, phi, sigma_a, b):\n",
    "\n",
    "    # AR(1) 과정의 평균 첫 번째 통과 시간을 계산하는 함수\n",
    "    def mean_first_passage_time(phi, sigma_a, u, b, n=100):\n",
    "        h = (b - u) / n # 구간 길이\n",
    "        u_i = np.linspace(u, b, n+1) # 구간\n",
    "        \n",
    "        # 커널 행렬 K 계산\n",
    "        K = np.zeros((n+1, n+1))\n",
    "        for i in range(n+1):\n",
    "            for j in range(n+1):\n",
    "                if i != j:\n",
    "                    K[i, j] = (h / (2 * np.sqrt(2 * np.pi) * sigma_a)) * np.exp(-((u_i[i] - phi * u_i[j])**2) / (2 * sigma_a**2))\n",
    "\n",
    "        # 선형 방정식 (I - K)E = 1 풀기\n",
    "        I = np.eye(n+1)\n",
    "        E = np.linalg.solve(I - K, np.ones(n+1))\n",
    "        return E\n",
    "    \n",
    "    # 평균 첫 번째 통과 시간 계산\n",
    "    E = mean_first_passage_time(phi, sigma_a, U, b)\n",
    "\n",
    "    # TD_U 계산\n",
    "    TD_U = E[0]\n",
    "\n",
    "    # I_U 계산\n",
    "    I_U = mean_first_passage_time(phi, sigma_a, -U, b)[0]\n",
    "\n",
    "    # MTP 계산\n",
    "    mtp = (TD_U + I_U - 1) * U\n",
    "    return mtp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3426417e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def backtest_strategy(df,method,transaction_cost_v1x=0.0002,transaction_cost_v2x=0.0003):\n",
    "\n",
    "    df = df.copy()\n",
    "    df = df.dropna()\n",
    "    ################ 1. 전략 수익률 계산 ################\n",
    "    df = calculate_returns(df,transaction_cost_v1x,transaction_cost_v2x)\n",
    "\n",
    "    ################ 2. MTP 계산 ################\n",
    "    # ADF 검정을 통해 phi, sigma_a 추정\n",
    "    adf_result = adfuller(df['spread'].dropna())\n",
    "    phi = adf_result[0]\n",
    "    sigma_a = df['spread'].std()\n",
    "\n",
    "    # 최적의 U 계산\n",
    "    b = df['spread'].mean()\n",
    "    U_range = np.arange(0, 5 * sigma_a, 0.01)   # U의 후보군\n",
    "    mtp_values = [calculate_mtp(df, U, phi, sigma_a, b) for U in U_range] # MTP 계산 Simulation\n",
    "    optimal_U = U_range[np.argmax(mtp_values)]  # MTP가 최대가 되는 U 선택\n",
    "    mtp = np.max(mtp_values)  # 최대 MTP\n",
    "\n",
    "    # # MTP를 고려한 누적 수익률 계산\n",
    "    # df['cumulative_returns_after_mtp'] = df['cumulative_returns'] - mtp\n",
    "\n",
    "    ################ 3. 성과 분석 ################\n",
    "    # 1) CAGR\n",
    "    num_years = len(df['daily_returns']) / 252\n",
    "    cagr = (1 + df['cumulative_returns'][-1]) ** (1/num_years) - 1\n",
    "    # 2) Volatility\n",
    "    annualized_volatility = df['daily_returns'].std() * np.sqrt(252)\n",
    "    # 3) IR Ratio\n",
    "    ir = (df['daily_returns'].mean() * 252) / annualized_volatility\n",
    "    # 4) MDD\n",
    "    peak = df['cumulative_returns'].cummax()\n",
    "    drawdown = (df['cumulative_returns'] - peak) / peak\n",
    "    drawdown.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    mdd = drawdown.min()\n",
    "    # 5) Hurst Exponent\n",
    "    hurst_exponent, c, data = hurst.compute_Hc(df['cumulative_returns'].fillna(0))\n",
    "    \n",
    "    # 6) Half-life\n",
    "    def half_life(spread):\n",
    "        spread_lag = spread.shift(1)\n",
    "        spread_lag.iloc[0] = spread_lag.iloc[1]\n",
    "\n",
    "        spread_ret = spread - spread_lag\n",
    "        spread_ret.iloc[0] = spread_ret.iloc[1]\n",
    "\n",
    "        spread_lag2 = sm.add_constant(spread_lag)\n",
    "        model = sm.OLS(spread_ret,spread_lag2)\n",
    "        res = model.fit()\n",
    "        halflife = round(-np.log(2) / res.params[1],2)\n",
    "        if halflife <= 0:\n",
    "            halflife = 1        \n",
    "        return halflife\n",
    "    half_life_ = half_life(df['spread'])\n",
    "\n",
    "    \n",
    "    result = {\n",
    "        'MTP': mtp,\n",
    "        'CAGR': cagr,\n",
    "        'Volatility': annualized_volatility,\n",
    "        'IR': ir,\n",
    "        'MDD': mdd,\n",
    "        'Hurst Exponent': hurst_exponent,\n",
    "        'Half-life': half_life_\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(result, index=[method])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe02fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_simple = backtest_strategy(signals_simple, 'simple', transaction_cost_v1x=0.0002,transaction_cost_v2x=0.0003)\n",
    "metrics_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af304738",
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_simple_ret = calculate_returns(signals_simple.copy())\n",
    "(signals_simple_ret['cumulative_returns']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3068e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_smoothed = backtest_strategy(signals_smoothed,'smooth',transaction_cost_v1x=0.0002,transaction_cost_v2x=0.0003)\n",
    "metrics_smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c55d44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_smoothed_ret = calculate_returns(signals_smoothed.copy())\n",
    "(signals_smoothed_ret['cumulative_returns']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cf6ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_coint = backtest_strategy(signals_coint,'coint',transaction_cost_v1x=0.0002,transaction_cost_v2x=0.0003)\n",
    "metrics_coint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cd85d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_coint_ret = calculate_returns(signals_coint.copy())\n",
    "(signals_coint_ret['cumulative_returns']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbcd96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_kalman = backtest_strategy(signals_kalman, 'karman', transaction_cost_v1x=0.0002,transaction_cost_v2x=0.0003)\n",
    "metrics_kalman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbcdcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_kalman_ret = calculate_returns(signals_kalman.copy())\n",
    "(signals_kalman_ret['cumulative_returns']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5d0242",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.concat([metrics_simple, metrics_smoothed, metrics_coint, metrics_kalman],axis=0)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4ff588",
   "metadata": {},
   "source": [
    "### (참고) 각 lookback과 k 조합에 대해 전략 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd65b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 lookback과 k 조합에 대해 전략 계산\n",
    "\n",
    "# Parameters\n",
    "lookback_values = [10, 20, 40]\n",
    "k_values = [0.5, 0.75, 1]\n",
    "\n",
    "simple_results = {}\n",
    "performance_metrics = {}\n",
    "for lookback in lookback_values:\n",
    "    for k in k_values:\n",
    "        key = f\"lookback={lookback}, k={k}\"\n",
    "\n",
    "        spread_df = simple_spread_strategy(Total_VIX_df, lookback, k)\n",
    "        spread_df = smooth_positions(spread_df)\n",
    "        \n",
    "        simple_results[key] = spread_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c312af",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lookbacks = len(lookback_values)\n",
    "num_ks = len(k_values)\n",
    "fig, axes = plt.subplots(num_lookbacks, num_ks, figsize=(15, 5 * num_lookbacks), sharex=True, sharey=True)\n",
    "\n",
    "for i, lookback in enumerate(lookback_values):\n",
    "    for j, k in enumerate(k_values):\n",
    "        key = f\"lookback={lookback}, k={k}\"\n",
    "        result = simple_results[key]\n",
    "\n",
    "        ax = axes[i, j]\n",
    "\n",
    "        ax.plot(result.index, result['spread'], label='Spread', color='blue')\n",
    "        ax.plot(result.index, result['low'], linestyle='--', label='Lower', color='red')\n",
    "        ax.plot(result.index, result['high'], linestyle='--', label='Upper', color='green')\n",
    "\n",
    "        ax.set_title(f\"Lookback={lookback}, k={k}\")\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Spread')\n",
    "        ax.grid()\n",
    "        ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c9c892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
