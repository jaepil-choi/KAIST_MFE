{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 거래소에서 option data 가져와보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from typing import List, Tuple \n",
    "from tqdm import tqdm\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CWD_PATH = Path.cwd()\n",
    "DATA_PATH = CWD_PATH / 'data'\n",
    "BACKUP_PATH = CWD_PATH / 'backup'\n",
    "OUTPUT_PATH = CWD_PATH / 'output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('c:/Users/chlje/VSCodeProjects/KAIST_MFE/BAF645파생거래전략/final_hw')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CWD_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## custom libs\n",
    "\n",
    "from krx_config import API_URL, HEADERS, PAYLOAD_TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API scraping code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt 1\n",
    "\n",
    "I need to write code to scrape data from KRX(한국거래소) website. The website uses POST to get daily snapshot of all the options traded on that day. \n",
    "\n",
    "Below is the information for its endpint (getJsonData.cmd) and what are in the headers and what the payload is and what the response looks like. \n",
    "\n",
    "Write Python code that takes start date and end date as an input and iteratively get the option price data from the api. \n",
    "\n",
    "You should make the requests to look like human by setting web browser user-agent and other stuff that can be detected by site admin. \n",
    "\n",
    "Although, you don't have to use dynamic scraping like selenium because as you can see, we're communicating with an API in restful way, with POST method. \n",
    "\n",
    "When you write the code, make the code modular and keep the function document as simple as possible. Never make verbose docstring that can bloat the code with unnecessary details. \n",
    "\n",
    "Below are the information that you need to write the correct API call scraper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_date_range(start_date: str, end_date: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generates a list of dates in YYYYMMDD format between start_date and end_date inclusive.\n",
    "\n",
    "    Args:\n",
    "        start_date (str): Start date in 'YYYY-MM-DD' format.\n",
    "        end_date (str): End date in 'YYYY-MM-DD' format.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of dates as strings in 'YYYYMMDD' format.\n",
    "    \"\"\"\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    delta = end - start\n",
    "    return [(start + timedelta(days=i)).strftime(\"%Y%m%d\") for i in range(delta.days + 1)]\n",
    "\n",
    "def fetch_option_data(session: requests.Session, trade_date: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches option data for a specific trade date.\n",
    "\n",
    "    Args:\n",
    "        session (requests.Session): The requests session with headers set.\n",
    "        trade_date (str): Trade date in 'YYYYMMDD' format.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing option data for the trade date.\n",
    "    \"\"\"\n",
    "    payload = PAYLOAD_TEMPLATE.copy()\n",
    "    payload[\"trdDd\"] = trade_date\n",
    "\n",
    "    response = session.post(API_URL, data=payload)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    data = response.json()\n",
    "\n",
    "    if \"output\" not in data:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.DataFrame(data[\"output\"])\n",
    "\n",
    "def scrape_krx_option_data(start_date: str, end_date: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrapes KRX option data between start_date and end_date.\n",
    "\n",
    "    Args:\n",
    "        start_date (str): Start date in 'YYYY-MM-DD' format.\n",
    "        end_date (str): End date in 'YYYY-MM-DD' format.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame containing option data for all dates.\n",
    "    \"\"\"\n",
    "    dates = generate_date_range(start_date, end_date)\n",
    "    all_data = []\n",
    "\n",
    "    with requests.Session() as session:\n",
    "        session.headers.update(HEADERS)\n",
    "        for date in tqdm(dates, desc=\"Fetching data\"):\n",
    "            try:\n",
    "                daily_data = fetch_option_data(session, date)\n",
    "                if not daily_data.empty:\n",
    "                    daily_data['Trade_Date'] = datetime.strptime(date, \"%Y%m%d\").date()\n",
    "                    all_data.append(daily_data)\n",
    "                time.sleep(1)  # Delay to mimic human behavior\n",
    "            except requests.HTTPError as http_err:\n",
    "                print(f\"HTTP error for date {date}: {http_err}\")\n",
    "            except Exception as err:\n",
    "                print(f\"Error for date {date}: {err}\")\n",
    "\n",
    "    if all_data:\n",
    "        return pd.concat(all_data, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def save_to_excel(df: pd.DataFrame, filename: str):\n",
    "    \"\"\"\n",
    "    Saves the DataFrame to an Excel file.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to save.\n",
    "        filename (str): Filename for the Excel file.\n",
    "    \"\"\"\n",
    "    df.to_excel(filename, index=False)\n",
    "    print(f\"Data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_date = '20241204'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with requests.Session() as session:\n",
    "    session.headers.update(HEADERS)\n",
    "    payload = PAYLOAD_TEMPLATE.copy()\n",
    "    payload[\"trdDd\"] = sample_date\n",
    "\n",
    "    response = session.post(API_URL, data=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = '2021-01-01'\n",
    "END_DATE = '2024-12-05'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### New Prompt\n",
    "\n",
    "Implement retry strategy in this code. \n",
    "Retry strategy should:\n",
    "- when there's bad requests like 400, step back and wait and retry. \n",
    "\n",
    "You should suggest me additional measure to avoid getting blocked. Is there a free VPN service that I can use to hide my identity? \n",
    "\n",
    "Also, to avoid losing all the data when I get blocked, write the code to:\n",
    "\n",
    "- start from end date to start date\n",
    "- for each date scraped, save it in the data by appending the data (not overwriting). You should use h5py to save it in h5 format to append the data. \n",
    "- periodically (per 100 scrape) copy that h5 file to somewhere else for me to safely use it without racing condition. ( I will copy the copy of the h5 and use it at somewhere else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import os\n",
    "import shutil\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# Import configurations\n",
    "from krx_config import API_URL, HEADERS, PAYLOAD_TEMPLATE\n",
    "\n",
    "# Constants for backup\n",
    "BACKUP_INTERVAL = 100  # Backup after every 100 successful scrapes\n",
    "H5_FILE = DATA_PATH / \"krx_option_data.h5\"\n",
    "\n",
    "def generate_date_range(start_date: str, end_date: str) -> List[str]: \n",
    "    \"\"\"\n",
    "    Generates a list of dates in YYYYMMDD format between start_date and end_date inclusive.\n",
    "    Starts from end_date to start_date.\n",
    "    \"\"\"\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    delta = end - start\n",
    "    return [(end - timedelta(days=i)).strftime(\"%Y%m%d\") for i in range(delta.days + 1)]\n",
    "\n",
    "def setup_session() -> requests.Session:\n",
    "    \"\"\"\n",
    "    Sets up a requests session with headers and retry strategy.\n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "    session.headers.update(HEADERS)\n",
    "    \n",
    "    retry_strategy = Retry(\n",
    "        total=5,\n",
    "        status_forcelist=[400, 429, 500, 502, 503, 504],\n",
    "        method_whitelist=[\"POST\"],\n",
    "        backoff_factor=1\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    \n",
    "    return session\n",
    "\n",
    "def fetch_option_data(session: requests.Session, trade_date: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches option data for a specific trade date.\n",
    "    \"\"\"\n",
    "    payload = PAYLOAD_TEMPLATE.copy()\n",
    "    payload[\"trdDd\"] = trade_date\n",
    "\n",
    "    response = session.post(API_URL, data=payload)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    data = response.json()\n",
    "\n",
    "    if \"output\" not in data:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.DataFrame(data[\"output\"])\n",
    "\n",
    "def save_data_h5(df: pd.DataFrame, filename: str):\n",
    "    \"\"\"\n",
    "    Appends DataFrame to an HDF5 file.\n",
    "    \"\"\"\n",
    "    with pd.HDFStore(filename, mode='a') as store:\n",
    "        store.append('option_data', df, format='table', data_columns=True)\n",
    "\n",
    "def backup_h5_file(source: str, backup_path: Path):\n",
    "    \"\"\"\n",
    "    Copies the HDF5 file to the backup directory with a timestamp.\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    backup_path = BACKUP_PATH / f\"krx_option_data_backup_{timestamp}.h5\"\n",
    "    shutil.copy(source, backup_path)\n",
    "    print(f\"Backup created at {backup_path}\")\n",
    "\n",
    "def scrape_krx_option_data(start_date: str, end_date: str) -> None:\n",
    "    \"\"\"\n",
    "    Scrapes KRX option data between start_date and end_date and saves to HDF5.\n",
    "    Implements retry and backup strategies.\n",
    "    \"\"\"\n",
    "    dates = generate_date_range(start_date, end_date)\n",
    "    success_count = 0\n",
    "\n",
    "    session = setup_session()\n",
    "    \n",
    "    for date in tqdm(dates, desc=\"Fetching data\"):\n",
    "        try:\n",
    "            daily_data = fetch_option_data(session, date)\n",
    "            if not daily_data.empty:\n",
    "                daily_data['Trade_Date'] = datetime.strptime(date, \"%Y%m%d\").date()\n",
    "                save_data_h5(daily_data, H5_FILE)\n",
    "                success_count += 1\n",
    "                \n",
    "                if success_count % BACKUP_INTERVAL == 0:\n",
    "                    backup_h5_file(H5_FILE, BACKUP_PATH)\n",
    "            \n",
    "            time.sleep(1)  # Delay to mimic human behavior\n",
    "        except requests.HTTPError as http_err:\n",
    "            print(f\"HTTP error for date {date}: {http_err}\")\n",
    "            time.sleep(5)  # Wait before retrying\n",
    "        except Exception as err:\n",
    "            print(f\"Error for date {date}: {err}\")\n",
    "            time.sleep(5)  # Wait before continuing\n",
    "    \n",
    "    # Final backup after completion\n",
    "    backup_h5_file(H5_FILE, BACKUP_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    # Example usage\n",
    "    START_DATE = \"2024-12-01\"\n",
    "    END_DATE = \"2024-12-06\"\n",
    "    \n",
    "    print(f\"Scraping KRX option data from {START_DATE} to {END_DATE}...\")\n",
    "    scrape_krx_option_data(START_DATE, END_DATE)\n",
    "    print(\"Scraping completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
