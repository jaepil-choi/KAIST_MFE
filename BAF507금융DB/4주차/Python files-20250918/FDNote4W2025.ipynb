{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eabcc52-d84c-4d43-b147-e3bab93d67f9",
   "metadata": {},
   "source": [
    "# FDNote4W.ipynb\n",
    "\n",
    "Prepared by Inmoo Lee for the Financial Databases class at KAIST\n",
    "\n",
    "inmool@kaist.ac.kr\n",
    "\n",
    "For return calculations using SQL\n",
    "\n",
    "Input files used\n",
    "\n",
    "    - 2020SP500Constituents_2025_Short.xlsx\n",
    "    - note4w.sas7bdat\n",
    "    - note4Ori.xlsx\n",
    "    - note4dat.xlsx\n",
    "    - return_data.ft\n",
    "Out files\n",
    "    - fbhrs.ft\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40895526-ec73-4588-8f66-c403fed60daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os #import a package called os\n",
    "\n",
    "print(os.getcwd())  #get the current working directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c158f7",
   "metadata": {},
   "source": [
    "# SQL\n",
    "\n",
    "Let's define a function, pysqldf, to use pandasql's sqldf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df829365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the working directory\n",
    "path='D:\\\\####'#Change this to your directory to work with\n",
    "os.chdir(path) # change the working directory\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce498ee1-d1f4-457e-8952-e94424d00531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "430f0a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Define a function to use pandasql\n",
    "#pandasql is a package that allows you to use SQL queries on pandas DataFrames\n",
    "from pandasql import sqldf\n",
    "def pysqldf(q):\n",
    " return sqldf(q, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfe20d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from an Excel file\n",
    "Table1=pd.read_excel('./note4Ori.xlsx', sheet_name=\"Table1\", header=0)\n",
    "Table2=pd.read_excel('./note4Ori.xlsx', sheet_name=\"Table2\", header=0)\n",
    "print(Table1.head())\n",
    "print(Table2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe695677",
   "metadata": {},
   "source": [
    "## Let's join two tables\n",
    "\n",
    "### left join:\n",
    "\n",
    "For each row in the left table (i.e., the one in the \"from\" statement), the columns of the observations that satisfy the conditions specified in the \"on\" statement will be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4bf8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to join Table1 and Table2 on 'id' and 'GENDER' columns\n",
    "# The query selects all columns from Table1 and the 'Eye' column from Table2\n",
    "# Note that the use of 'left join' ensures that all records from Table1 are included\n",
    "# even if there are no matching records in Table2 \n",
    "\n",
    "# The records in Table2, which are not in Table1, will not be included in the result\n",
    "# This is because we are using a left join, which includes all records from the left table\n",
    "\n",
    "Query='''select a.*,b.Eye \n",
    "            from Table1 as a \n",
    "            left join Table2 as b \n",
    "            on a.id=b.id and a.GENDER=b.GENDER'''\n",
    "\n",
    "Newtable=pysqldf(Query)\n",
    "\n",
    "\n",
    "print(Newtable.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c263ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Pandas has a much simpler way to merge two dataframes\n",
    "## Please check Appendix 1 for more details on pd.merge at the end of this file\n",
    "# pd.merge is used to merge two DataFrames based on a common column or index\n",
    "\n",
    "NewtablePD=pd.merge(Table1,Table2,how='left',on=('ID','GENDER'))\n",
    "print(NewtablePD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb89ebaf",
   "metadata": {},
   "source": [
    "#### Another example\n",
    "\n",
    "Join two dataframes covering two different time periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0269d8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read an excel file, Fama-French three-factor values plus S&P 500 index returns: Monthly\n",
    "Factordat=pd.read_excel('./note4data.xlsx', sheet_name=\"factors\", header=0)\n",
    "print(Factordat.head())\n",
    "print(Factordat.tail())\n",
    "print(Factordat.Date.min())\n",
    "print(Factordat.Date.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41d6f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude observations before 2005\n",
    "factordat=Factordat[Factordat['Date' ]>= 20050101].copy()\n",
    "print(factordat.head())\n",
    "print(factordat.Date.min())\n",
    "print(factordat.Date.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930f653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read sas data (note42 and exclude thsoe after 2008\n",
    "\n",
    "df=pd.read_sas('./note4w.sas7bdat')\n",
    "print(df.head())\n",
    "print(df.Date.min())\n",
    "print(df.Date.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0cd9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock=df[df['Date']<=20081231].copy()\n",
    "print(stock.head())\n",
    "print(stock.Date.min())\n",
    "print(stock.Date.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6a5099",
   "metadata": {},
   "source": [
    "##### Convert an object columns in bytes to a string before proceeding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56078099",
   "metadata": {},
   "outputs": [],
   "source": [
    "###change bytes to string (For example, the first id is b'IBM'.\n",
    "### b' indicates that it is in bytes.  To change it to a string value,\n",
    "### you need to use decode('utf8')))\n",
    "stock.loc[:,'id']=stock.loc[:,'id'].str.decode('utf8')\n",
    "stock.loc[:,'isign']=stock.loc[:,'isign'].str.decode('utf8')\n",
    "\n",
    "print(stock.head())\n",
    "print(stock.Date.min())\n",
    "print(stock.Date.max())\n",
    "print(factordat.Date.min())\n",
    "print(factordat.Date.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857a3d36",
   "metadata": {},
   "source": [
    "### Let's join two tables using \"left join\".\n",
    "\n",
    "Here, the columns of the observations satisfying the condition specified in the \"on\" statement in the \"factordat\" table are added to the \"stock\" table\n",
    "\n",
    "Note that stock covers the data during 2001 - 2008 while factors cover the data during 2005 - 2013, which indicates that the columns from factors will have missing values during 2001-2004 in the final output.\n",
    "\n",
    "The final output covers the period from 2001 till 2008."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1274635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below, we rename the Date column in the factordat DataFrame as factordate to avoid confusion\n",
    "# with the Date column in the stock DataFrame\n",
    "\n",
    "L_query='''select a.*,b.Date as factordate,b.SP500,b.RmRf,b.SMB,b.HML,b.RF\n",
    "           from stock as a\n",
    "           left join factordat as b\n",
    "           on a.Date = b.Date\n",
    "           order by a.Date, a.id'''\n",
    "Leftjoin=sqldf(L_query,locals())# or Leftjoin=pysqldf(L_query)\n",
    "\n",
    "print(Leftjoin.head())\n",
    "print(Leftjoin.tail())\n",
    "print(stock.Date.min(),factordat.Date.min(),Leftjoin.Date.min()) #Print the minimum date in the joined DataFrame\n",
    "print(stock.Date.max(),factordat.Date.max(),Leftjoin.Date.max()) #Print the maximum date in the joined DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2174c176",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the minimum and maximum dates in the joined DataFrame\n",
    "print('Join Min Date:\\n', Leftjoin[['Date','factordate']].min())\n",
    "print('Join Max Date:\\n  ', Leftjoin[['Date','factordate']].max())\n",
    "\n",
    "## Check the minimum and maximum dates in the joined DataFrame where SP500 is not null\n",
    "## Note that you can specify the condition using the .loc method\n",
    "## Only those rows that satisfy the condition will be included in the result\n",
    "\n",
    "# Note that the SP500 column is in the factordat DataFram only, so we check the 'factordate' column\n",
    "print('Join Min Date, Not missing SP500:\\n', Leftjoin.loc[Leftjoin.SP500.notnull(),['Date','factordate']].min())\n",
    "print('Join Max Date, Not missing SP500:\\n', Leftjoin.loc[Leftjoin.SP500.notnull(),['Date','factordate']].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca55575",
   "metadata": {},
   "source": [
    "### Right join\n",
    "\n",
    "Since right join is not supported, use a trick: Switch tables in \"left join\"\n",
    "\n",
    "Different from the previous one, here, the columns of stock's observations satisfying the conditions will be added to \"factordat\".\n",
    "\n",
    "Here, the returns will be missing for the period from 2009 in the final data.\n",
    "\n",
    "The output covers the period from 2005 till 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54f1ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query statement to perofrm a right join\n",
    "# Since pandasql does not support right join directly, we can use a left join\n",
    "# and then reverse the order of the tables in the query\n",
    "\n",
    "# Note that the Date column in the factordat DataFrame is renamed to factordate\n",
    "# to avoid confusion with the Date column in the stock DataFrame\n",
    "\n",
    "R_query='''select a.*,b.Date as factordate,b.SP500,b.RmRf,b.SMB,b.HML,b.RF\n",
    "            from factordat as b\n",
    "            left join stock as a\n",
    "            on a.Date = b.Date\n",
    "            order by b.Date, a.id'''\n",
    "\n",
    "# All obs in factors are selected in rightjoin*/\n",
    "Rightjoin=pysqldf(R_query)\n",
    "print(Rightjoin.head())\n",
    "print(Rightjoin.tail())\n",
    "\n",
    "#Check how the data coverage changed after the right join\n",
    "print(stock.Date.min(),factordat.Date.min(),Rightjoin.factordate.min()) #Print the minimum date in the joined DataFrame\n",
    "print(stock.Date.max(),factordat.Date.max(),Rightjoin.factordate.max()) #Print the maximum date in the joined DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bee8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the minimum and maximum dates in the joined DataFrame\n",
    "print('Join Min Date:\\n', Rightjoin[['Date','factordate']].min())\n",
    "print('Join Max Date:\\n', Rightjoin[['Date','factordate']].max())\n",
    "\n",
    "## Check the minimum and maximum dates in the joined DataFrame where SP500 is not null\n",
    "## Note that you can specify the condition using the .loc method\n",
    "## Only those rows that satisfy the condition will be included in the result\n",
    "\n",
    "## Returns are in the stock DataFrame only, so we check the 'return' column\n",
    "print('Join Min Date, Not missing returns:\\n', Rightjoin.loc[Rightjoin['return'].notnull(),['Date','factordate']].min())\n",
    "print('Join Max Date, Not missing returns:\\n', Rightjoin.loc[Rightjoin['return'].notnull(),['Date','factordate']].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5b5391",
   "metadata": {},
   "source": [
    "## Inner join\n",
    "\n",
    "Inner join merges two tables's observations that are present in both tables\n",
    "\n",
    "The output will cover the period during 2005 and 2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e21928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only those that are in both stocks and factors are selected*/\n",
    "\n",
    "I_query='''select a.*,b.Date as factordate,b.SP500,b.RmRf,b.SMB,b.HML,b.RF\n",
    "            from stock as a\n",
    "            inner join factordat as b\n",
    "            on a.Date = b.Date\n",
    "            order by a.Date, a.id'''\n",
    "# All obs in factors are selected in rightjoin*/\n",
    "Innerjoin=sqldf(I_query,locals()) #or pysqldf(I_query)\n",
    "print(Innerjoin.head())\n",
    "print(Innerjoin.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ebc160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check how the data coverage changed after the inner join\n",
    "print('Join Min Date:\\n', Innerjoin[['Date','factordate']].min())\n",
    "print('Join Max Date:\\n', Innerjoin[['Date','factordate']].max())\n",
    "\n",
    "#Check the minimum and maximum dates in the joined DataFrame where SP500 is not null\n",
    "print('Join Min Date, Not missing returns and sp500:\\n',\\\n",
    "      Innerjoin.loc[Innerjoin['return'].notnull() & Innerjoin.SP500.notnull(),\\\n",
    "                    ['Date','factordate']].min())\n",
    "print('Join Max Date, Not missing returns and sp500:\\n',\\\n",
    "      Innerjoin.loc[Innerjoin['return'].notnull() & Innerjoin.SP500.notnull(),\\\n",
    "                    ['Date','factordate']].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79382e6",
   "metadata": {},
   "source": [
    "## Cartesian join\n",
    "\n",
    "For each in the \"stock\" (in from) table, all observations in the \"factordat\" table (left join) are added (no conditions are specified)\n",
    "\n",
    "The output covers the period during 2001 and 2013\n",
    "\n",
    "The total number of observations is # in stock times # in factordat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decedfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each observation in stock, all obs in factors are matched*/\n",
    "# Note that there are no conditions specified in the left join\n",
    "# This means that all observations in the stock and factodat dataframes will be included\n",
    "\n",
    "\n",
    "C_query='''select a.*,b.Date as factordate,b.SP500,b.RmRf,b.SMB,b.HML,b.RF\n",
    "            from stock as a\n",
    "            left join factordat as b\n",
    "            order by a.Date, a.id'''\n",
    "Cartesian =sqldf(C_query,locals()) #or #or pysqldf(C_query)\n",
    "\n",
    "print(Cartesian.head())\n",
    "print(Cartesian.tail())#notice that the time period covers only for the one in from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e165c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The number of rows:\\n', Cartesian.Date.count())\n",
    "A=stock.Date.count()\n",
    "B=factordat.Date.count()\n",
    "print(A,B)\n",
    "print('# of rows in stock * # of rows in factordat =  ', A*B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66272137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the minimum and maximum dates in the Cartesian product DataFrame\n",
    "# Note that the Date column in the factordat DataFrame is renamed to factordate\n",
    "# to avoid confusion with the Date column in the stock DataFrame\n",
    "print(Cartesian[['Date','factordate']].min())\n",
    "print(Cartesian[['Date','factordate']].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0eae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To output certain observations that satisfy some conditions after \n",
    "# performing a cartesian product (left join), we can use the following query\n",
    "\n",
    "# After the Cartesian product, we can select all observations that satisfy the condition\n",
    "# specified in the WHERE clause``\n",
    "CW_query='''select a.*,b.Date as factordate,b.SP500,b.RmRf,b.SMB,b.HML,b.RF\n",
    "            from stock as a\n",
    "            left join factordat as b\n",
    "            where a.Date = b.Date\n",
    "            order by a.Date, a.id'''\n",
    "\n",
    "Cartesianwhere =sqldf(CW_query,locals()) #or pysqldf(CW_query)\n",
    "print(Cartesianwhere.head())\n",
    "print(Cartesianwhere.tail())\n",
    "\n",
    "print(Cartesianwhere[['Date','factordate']].min())\n",
    "print(Cartesianwhere[['Date','factordate']].max())\n",
    "print(stock[['Date']].max())\n",
    "print(factordat[['Date']].max())\n",
    "print(stock[['Date']].min())\n",
    "print(factordat[['Date']].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2769b2b-555c-46fe-91b1-1ac066cfdf9d",
   "metadata": {},
   "source": [
    "# Calculate buy-and-hold returns\n",
    "\n",
    "List of input files used here\n",
    "\n",
    "    - return_data.ft\n",
    "    - note4data.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beddb53a-83d3-469e-a275-b93ae2f8c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_feather('./return_data.ft')\n",
    "print(df.head())\n",
    "df=df.rename(columns={'return':'ret'})\n",
    "print(df.head())\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a75301-4076-44c5-af14-0aa5efc7956f",
   "metadata": {},
   "source": [
    "# Calculate holding period returns using SQL\n",
    "\n",
    "Buy-and-hold returns (BHR) over the n periods can be calculated as follows (e.g., using monthly returns, over 3 years (n=36))\n",
    "\n",
    "$$\n",
    "(1+r_{1})\\times (1+r_{2})\\times ... \\times(1+r_{n}) = 1+BHR_{n}\n",
    "$$\n",
    "\n",
    "Since SQL does not have multiplications, we have to use a trick to calculate cumulative returns using SQL.\n",
    "\n",
    "We can convert returns to log returns so that we can use 'sum' instead of 'product'.\n",
    "\n",
    "Take $log=ln=log_{exp}$ in both sides of the above equation, then \n",
    "\n",
    "$$\n",
    "log((1+r_{1})\\times (1+r_{2})\\times ... \\times(1+r_{n})) = log(1+BHR_{n})\n",
    "$$\n",
    "\n",
    "Since $log(x \\times y)=log(x)+log(y)$, \n",
    "\n",
    "$$\n",
    "log(1+BHR_n)=log(1+r_1)+log(1+r_2) ... +log(1+r_n)\n",
    "$$\n",
    "\n",
    "$log(a)=b$ implies that $exp^b=a$. If we let $log(1+BHR_n)=b$, and  $1+BHR_n = a$, then \n",
    "\n",
    "$$\n",
    "exp^{b}=exp^{log(1+BHR_{n})} = a=1+BHR_{n}\n",
    "$$  \n",
    "\n",
    "Therefore, to calculate BHRs, you can sum log returns (=$log(1+r)$) over a range of dates, and then convert it to 1 + BHR by using the exponential function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42dddc4-c791-4db6-af5c-0940440bf013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  calculate the log returns\n",
    "df.loc[:,'logret']=np.log(1.0+df.loc[:,'ret']) #np.log is for element by element log function\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad84009b-227c-4b93-b4f8-00c89baeb280",
   "metadata": {},
   "source": [
    "### Find the month end date\n",
    "\n",
    "The last transaction date of a month may not be the last day of the month.\n",
    "\n",
    "We can find the last day of a month using MonthEnd\n",
    "\n",
    "We can also find the dates a certain number of months before or after the current month using MonthEnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fed6479-5dfb-4b21-979d-b1eab08bcc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "## One can find the the month end any # of months before/after a particular date that is a datetime object,\n",
    "## Using MonthEnd from pandas.tseries.offsets\n",
    "\n",
    "##To use MonthEnd, we need to import it from pandas.tseries.offsets\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "\n",
    "\n",
    "## If your date column is not a datetime object, you can convert it to a datetime object using pd.to_datetime\n",
    "### you have to convert the floating 64 format Date to integer and then to string format\n",
    "### to use pd_to_datetime\n",
    "\n",
    "#The following code converts the 'Date' column to a datetime object\n",
    "df.loc[:,'date1']=pd.to_datetime(round(df.loc[:,'Date']).astype(int).astype(str),format='%Y%m%d')+MonthEnd(0)\n",
    "\n",
    "###find the date 12 months after (+12) and before (-12) the current date\n",
    "df.loc[:,'fmonth12']=df.loc[:,'date1']+MonthEnd(+12)\n",
    "df.loc[:,'bmonth12']=df.loc[:,'date1']+MonthEnd(-12)\n",
    "\n",
    "#Check whether the new columns are created correctly\n",
    "print(df[['date1','fmonth12','bmonth12']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb957c6",
   "metadata": {},
   "source": [
    "##### Find an indicator for year and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe51d466-f7f0-47d0-820e-08fd452b2f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "###find yyyymm to be used as an time indicator (year+month)\n",
    "### since date1 is a datetime variable, dt.year and dt.month\n",
    "### can be used to get year and month from a date.\n",
    "\n",
    "df.loc[:,'yyyymm']=df.loc[:,'date1'].dt.year*100+df.date1.dt.month\n",
    "\n",
    "#Check whether the yyyymm column is created correctly\n",
    "print(df[['date1','yyyymm']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dd9477-1c8d-4d29-a57f-c832c68f272a",
   "metadata": {},
   "source": [
    "### The following query will calculate the sum of logret over the period satisfying the conditions specified in \"on\" statement\n",
    "\n",
    "##### This is an example of a complicated join, which can be performed in a sql statement\n",
    "\n",
    "Here, bhrs are calculated for each id and yyyymm specified in \"group by\" but in addition, they are calculated using all observations in each id and yyyymm category, which satisfy the conditions specified inside the \"on\" statement.\n",
    "\n",
    "left join: Notice that we use the same file (df) in both \"from\" and \"left join\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff02c48-acc7-421b-b9c5-607607bb707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As done before, we can add the logrets for a certain number\n",
    "# of months using SQL.\n",
    "\n",
    "# Calculate the sum of logrets over the one-year period starting from the current month\n",
    "# i.e., over the 12 months using SQL\n",
    "query='''\n",
    "        select a.id,a.yyyymm,sum(b.logret) as logsum,\n",
    "            count(b.logret) as nummonths\n",
    "        from df as a\n",
    "        left join df as b\n",
    "        on a.id = b.id and \n",
    "            (b.date1 >= a.date1) and \n",
    "            (b.date1 < a.fmonth12)\n",
    "        group by a.id, a.yyyymm\n",
    "        order by a.id, a.yyyymm\n",
    "    '''\n",
    "fbhrs=pysqldf(query)\n",
    "###convert sum(b.logret) to bhr by taking exp and subtract 1.\n",
    "fbhrs.loc[:,'bhr1y']=np.exp(fbhrs.logsum)-1.0\n",
    "\n",
    "# Check the columns and the first 20 rows of the resulting DataFrame\n",
    "print(fbhrs.columns)\n",
    "print(fbhrs.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac1ab57",
   "metadata": {},
   "source": [
    "##### Choose only thoes buy-and-hold returns calculated over 12 months.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49d986a-7ef2-4884-b666-ddc98a83868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To treat those with nummonths not being equal to 12 as missing,\n",
    "## one can use np.where.\n",
    "\n",
    "## np.where replaces the values of rows of a specified column, \n",
    "## which satisfy the condition with the values\n",
    "## specified (condition, column to replace, value).\n",
    "\n",
    "#First, check the summary statistics of the bhr1y column\n",
    "print(fbhrs.bhr1y.describe())\n",
    "\n",
    "#Exclude those with nummonths not being equal to 12\n",
    "fbhrs.loc[:,'bhr1y']=np.where(fbhrs.nummonths==12,fbhrs.bhr1y,np.nan)\n",
    "#Check again the summary statistics of the bhr1y column after excluding those with nummonths not being equal to 12\n",
    "print(fbhrs.bhr1y.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218a1908-958e-4906-b69e-fe6316100d23",
   "metadata": {},
   "source": [
    "## Compare the results calculated in the EXCEL, \"bhrs\" worksheet in note4data.xlsx\n",
    "\n",
    "As you can see, the first bhr1y calcualted in EXCEL using the function\n",
    "=PRODUCT(1+C2:C13)-1 is same as the result in bhrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e3ee07-f658-456b-8fbf-52dc5286eb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check whether the bhr1y column is created correctly\n",
    "## by comparing the results with the ones in the note4data.xlsx file\n",
    "## that include a 'BHRs' worksheet in which three different ways of calculating\n",
    "## buy-and-hold returns are shown\n",
    "\n",
    "print(fbhrs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64399119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the file so that we can use it in the next week's note\n",
    "fbhrs.to_feather('./fbhrs.ft')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e594de8d",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "- Join dataframes using pd.merge.\n",
    "- Working with dates in Python\n",
    "- Read SAS data, convert bytes into string and save the file as a feather file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a4fef8",
   "metadata": {},
   "source": [
    "## Appendix 1: pd.merge\n",
    "\n",
    "#### You can join dataframes using pd.merge.\n",
    " \n",
    "The following show how \"join\" can be done using pd.merge\n",
    "\n",
    "Please check the class note for the comparison between join in padasql and pd.merge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87b560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the columns of the stock and factordat DataFrames\n",
    "print(stock.columns)\n",
    "print(factordat.columns)\n",
    "\n",
    "#To avoid the confusion with the Date column in the stock DataFrame,\n",
    "# we rename the Date column in the factordat DataFrame as factordate\n",
    "\n",
    "factordat.rename(columns={'Date': 'factordate'}, inplace=True)\n",
    "\n",
    "#Check the columns after renaming\n",
    "print(factordat.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9931aaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Alternatively, use pd.merge\n",
    "LeftjoinPD=pd.merge(stock,factordat,how='left',left_on=('Date'),right_on=('factordate'))\n",
    "\n",
    "#sort the DataFrame by Date and id\n",
    "# This will sort the DataFrame first by 'Date' and then by 'id'\n",
    "LeftjoinPD.sort_values(by=['Date','id'], ascending=True,inplace=True )\n",
    "print(LeftjoinPD.head())# pay attention to the first columns (index)\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "# This will reset the index of the DataFrame and drop the old index\n",
    "LeftjoinPD.reset_index(drop=True,inplace=True)#If you do not use drop=True, old index will be a column\n",
    "\n",
    "print(LeftjoinPD.head())# pay attention to the first columns (index)\n",
    "print(LeftjoinPD.tail())# pay attention to the first columns (index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097326e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can use one step, rather than doing it in three steps\n",
    "#merge, sort, and reset index\n",
    "# This will merge the stock and factordat DataFrames, sort the resulting DataFrame by '\n",
    "LeftjoinPD=pd.merge(stock,factordat,how='left',\\\n",
    "                    left_on=('Date'),right_on=('factordate')).sort_values(by=['Date','id'],\\\n",
    "                       ascending=True).\\\n",
    "                    reset_index(drop=True)\n",
    "##note that I did not use inplace=True since I am assigning the reuslt into a new dataframe\n",
    "                    \n",
    "print(LeftjoinPD.head())\n",
    "print(LeftjoinPD.tail())\n",
    "##check columns\n",
    "print(Leftjoin.columns)\n",
    "print(LeftjoinPD.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d37769",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Right join\n",
    "## pd.merge has an option to use right join\n",
    "##Here, there is right join, so do not change the order and use how='right'\n",
    "\n",
    "print(factordat.columns)\n",
    "RightjoinPD=pd.merge(stock,factordat,how='right',\\\n",
    "                    left_on=('Date'),right_on='factordate').\\\n",
    "                     sort_values(by=['Date','id'],\\\n",
    "                       ascending=True).\\\n",
    "                    reset_index(drop=True)\n",
    "##note that I did not use inplace=True in sort_values since you need to use it in the last one only\n",
    "                    \n",
    "print(RightjoinPD.head())\n",
    "print(RightjoinPD.tail())\n",
    "##Alternatively, use pd.merge\n",
    "##Here, there is right join, so do not change the order and use how='right'\n",
    "\n",
    "print(factordat.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1f8986",
   "metadata": {},
   "outputs": [],
   "source": [
    "##For inner join, you can use how='inner'\n",
    "\n",
    "InnerjoinPD=pd.merge(stock,factordat,how='inner',\\\n",
    "                    left_on=('Date'),right_on='factordate').\\\n",
    "                     sort_values(by=['Date','id'],\\\n",
    "                       ascending=True).\\\n",
    "                    reset_index(drop=True)\n",
    "##note that I did not use inplace=True in sort_values since you need to use it in the last one only\n",
    "                    \n",
    "print(InnerjoinPD.head())\n",
    "print(InnerjoinPD.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c16deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cartesian product\n",
    "## for each observation in stock, all obs in factors are matched*/\n",
    "\n",
    "\n",
    "##using pd.merge for catesian join is more tricky\n",
    "##You need a common key\n",
    "##Since two do not have one, create one and then remove it later\n",
    "stock['key']=0\n",
    "factordat['key']=0\n",
    "\n",
    "CartesianPD=pd.merge(stock,factordat,how='outer').\\\n",
    "                     sort_values(by=['Date','id'],\\\n",
    "                       ascending=True).\\\n",
    "                    reset_index(drop=True).drop(columns='key')\n",
    "print(CartesianPD.head())\n",
    "print(CartesianPD.tail())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
