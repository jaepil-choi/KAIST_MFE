{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a662ced",
   "metadata": {},
   "source": [
    "# 빅데이터 기말대체과제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c37522c",
   "metadata": {},
   "source": [
    "## 1. `prob1_bank.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bdd054",
   "metadata": {},
   "source": [
    "### 0. 라이브러리 임포트 및 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33297ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bank Dataset Shape: (4521, 11)\n",
      "\n",
      "Bank Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4521 entries, 0 to 4520\n",
      "Data columns (total 11 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   age        4521 non-null   int64 \n",
      " 1   job        4521 non-null   object\n",
      " 2   marital    4521 non-null   object\n",
      " 3   education  4521 non-null   object\n",
      " 4   default    4521 non-null   object\n",
      " 5   balance    4521 non-null   int64 \n",
      " 6   housing    4521 non-null   object\n",
      " 7   loan       4521 non-null   object\n",
      " 8   contact    4521 non-null   object\n",
      " 9   month      4521 non-null   object\n",
      " 10  y          4521 non-null   object\n",
      "dtypes: int64(2), object(9)\n",
      "memory usage: 388.7+ KB\n",
      "None\n",
      "\n",
      "Bank Dataset Summary Statistics:\n",
      "               age       balance\n",
      "count  4521.000000   4521.000000\n",
      "mean     41.170095   1422.657819\n",
      "std      10.576211   3009.638142\n",
      "min      19.000000  -3313.000000\n",
      "25%      33.000000     69.000000\n",
      "50%      39.000000    444.000000\n",
      "75%      49.000000   1480.000000\n",
      "max      87.000000  71188.000000\n",
      "\n",
      "Missing values in each column:\n",
      "age          0\n",
      "job          0\n",
      "marital      0\n",
      "education    0\n",
      "default      0\n",
      "balance      0\n",
      "housing      0\n",
      "loan         0\n",
      "contact      0\n",
      "month        0\n",
      "y            0\n",
      "dtype: int64\n",
      "\n",
      "Target variable distribution:\n",
      "y\n",
      "no     4000\n",
      "yes     521\n",
      "Name: count, dtype: int64\n",
      "y\n",
      "no     88.476001\n",
      "yes    11.523999\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "# Create plots directory if it doesn't exist\n",
    "plots_dir = 'plots'\n",
    "if not os.path.exists(plots_dir):\n",
    "    os.makedirs(plots_dir)\n",
    "\n",
    "# Load the data\n",
    "bank_data = pd.read_csv('prob1_bank.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(\"Bank Dataset Shape:\", bank_data.shape)\n",
    "print(\"\\nBank Dataset Info:\")\n",
    "print(bank_data.info())\n",
    "# print(\"\\nBank Dataset Summary Statistics:\") # EDA에서 상세히 다룸\n",
    "# print(bank_data.describe())\n",
    "\n",
    "# Check for missing values (EDA에서 확인 완료)\n",
    "# print(\"\\nMissing values in each column:\")\n",
    "# print(bank_data.isnull().sum())\n",
    "\n",
    "# Check target variable distribution (class imbalance) - EDA에서 확인 완료 및 시각화\n",
    "print(\"\\nTarget variable distribution (Initial Check):\")\n",
    "print(bank_data['y'].value_counts())\n",
    "# print(bank_data['y'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a3d6a8",
   "metadata": {},
   "source": [
    "### 1. 범주형 변수 전처리\n",
    "\n",
    "- EDA 결과, 범주형 변수 간 특별한 순서나 계층 구조가 보이지 않으므로 One-Hot Encoding을 적용합니다.\n",
    "- 첫 번째 범주를 제거(`drop='first'`)하여 다중공선성을 방지합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b41aadd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age          job  marital  education default  balance housing loan  \\\n",
      "0   30   unemployed  married    primary      no     1787      no   no   \n",
      "1   33     services  married  secondary      no     4789     yes  yes   \n",
      "2   35   management   single   tertiary      no     1350     yes   no   \n",
      "3   30   management  married   tertiary      no     1476     yes  yes   \n",
      "4   59  blue-collar  married  secondary      no        0     yes   no   \n",
      "\n",
      "    contact month   y  \n",
      "0  cellular   oct  no  \n",
      "1  cellular   may  no  \n",
      "2  cellular   apr  no  \n",
      "3   unknown   jun  no  \n",
      "4   unknown   may  no  \n"
     ]
    }
   ],
   "source": [
    "# Identify categorical and numerical columns\n",
    "categorical_cols = bank_data.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = bank_data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(\"Categorical variables:\", categorical_cols)\n",
    "print(\"Numerical variables:\", numerical_cols)\n",
    "\n",
    "# Preprocessing categorical variables using OneHotEncoder\n",
    "# 목표 변수 'y'는 나중에 LabelEncoding으로 처리하므로 제외\n",
    "categorical_features_to_encode = [col for col in categorical_cols if col != 'y']\n",
    "categorical_preprocessor = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Apply one-hot encoding\n",
    "X_categorical = bank_data[categorical_features_to_encode]\n",
    "categorical_preprocessor.fit(X_categorical)\n",
    "X_categorical_encoded = categorical_preprocessor.transform(X_categorical)\n",
    "\n",
    "# Create DataFrame with encoded features\n",
    "encoded_feature_names = categorical_preprocessor.get_feature_names_out(categorical_features_to_encode)\n",
    "X_categorical_df = pd.DataFrame(X_categorical_encoded, columns=encoded_feature_names, index=bank_data.index)\n",
    "\n",
    "print(\"\\nShape of encoded categorical features:\", X_categorical_df.shape)\n",
    "# print(X_categorical_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f90ba7",
   "metadata": {},
   "source": [
    "### 2. 수치형 변수 전처리\n",
    "\n",
    "- EDA 결과, `balance` 변수 등에서 상당한 왜도와 이상치가 관찰되었습니다.\n",
    "- 모델 성능에 이상치의 영향을 줄이기 위해 IQR 방법을 사용하여 이상치를 탐지하고 클리핑(Clipping)합니다.\n",
    "- 클리핑 후, `StandardScaler`를 사용하여 모든 수치형 변수를 표준화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e67282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상치 처리 함수 (IQR 기반 Clipping)\n",
    "def clip_outliers_iqr(series):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return series.clip(lower_bound, upper_bound)\n",
    "\n",
    "# 수치형 변수에 이상치 처리 적용\n",
    "X_numerical = bank_data[numerical_cols].copy()\n",
    "for col in numerical_cols:\n",
    "    original_skew = X_numerical[col].skew()\n",
    "    X_numerical[col] = clip_outliers_iqr(X_numerical[col])\n",
    "    clipped_skew = X_numerical[col].skew()\n",
    "    print(f\"Variable '{col}': Skewness before clipping: {original_skew:.2f}, Skewness after clipping: {clipped_skew:.2f}\")\n",
    "\n",
    "# Apply standardization to numerical features (after clipping)\n",
    "scaler = StandardScaler()\n",
    "X_numerical_scaled = scaler.fit_transform(X_numerical)\n",
    "\n",
    "# Create DataFrame with scaled features\n",
    "X_numerical_df = pd.DataFrame(X_numerical_scaled, columns=numerical_cols, index=bank_data.index)\n",
    "\n",
    "print(\"\\nShape of scaled numerical features:\", X_numerical_df.shape)\n",
    "# print(X_numerical_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e330b414",
   "metadata": {},
   "source": [
    "### 3. 클래스 불균형 처리\n",
    "\n",
    "- EDA 결과, 목표 변수 'y'가 약 88%의 'no'와 12%의 'yes'로 심각한 불균형을 보입니다.\n",
    "- 소수 클래스(yes)의 예측 성능 저하를 방지하기 위해 SMOTE(Synthetic Minority Over-sampling Technique)를 적용하여 오버샘플링합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14f9faec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical variables: ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'y']\n",
      "Numerical variables: ['age', 'balance']\n",
      "\n",
      "Unique values in job:\n",
      "job\n",
      "management       969\n",
      "blue-collar      946\n",
      "technician       768\n",
      "admin.           478\n",
      "services         417\n",
      "retired          230\n",
      "self-employed    183\n",
      "entrepreneur     168\n",
      "unemployed       128\n",
      "housemaid        112\n",
      "student           84\n",
      "unknown           38\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique values in marital:\n",
      "marital\n",
      "married     2797\n",
      "single      1196\n",
      "divorced     528\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique values in education:\n",
      "education\n",
      "secondary    2306\n",
      "tertiary     1350\n",
      "primary       678\n",
      "unknown       187\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique values in default:\n",
      "default\n",
      "no     4445\n",
      "yes      76\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique values in housing:\n",
      "housing\n",
      "yes    2559\n",
      "no     1962\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique values in loan:\n",
      "loan\n",
      "no     3830\n",
      "yes     691\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique values in contact:\n",
      "contact\n",
      "cellular     2896\n",
      "unknown      1324\n",
      "telephone     301\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique values in month:\n",
      "month\n",
      "may    1398\n",
      "jul     706\n",
      "aug     633\n",
      "jun     531\n",
      "nov     389\n",
      "apr     293\n",
      "feb     222\n",
      "jan     148\n",
      "oct      80\n",
      "sep      52\n",
      "mar      49\n",
      "dec      20\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique values in y:\n",
      "y\n",
      "no     4000\n",
      "yes     521\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Shape of encoded categorical features: (4521, 32)\n",
      "   job_blue-collar  job_entrepreneur  job_housemaid  job_management  \\\n",
      "0              0.0               0.0            0.0             0.0   \n",
      "1              0.0               0.0            0.0             0.0   \n",
      "2              0.0               0.0            0.0             1.0   \n",
      "3              0.0               0.0            0.0             1.0   \n",
      "4              1.0               0.0            0.0             0.0   \n",
      "\n",
      "   job_retired  job_self-employed  job_services  job_student  job_technician  \\\n",
      "0          0.0                0.0           0.0          0.0             0.0   \n",
      "1          0.0                0.0           1.0          0.0             0.0   \n",
      "2          0.0                0.0           0.0          0.0             0.0   \n",
      "3          0.0                0.0           0.0          0.0             0.0   \n",
      "4          0.0                0.0           0.0          0.0             0.0   \n",
      "\n",
      "   job_unemployed  ...  month_dec  month_feb  month_jan  month_jul  month_jun  \\\n",
      "0             1.0  ...        0.0        0.0        0.0        0.0        0.0   \n",
      "1             0.0  ...        0.0        0.0        0.0        0.0        0.0   \n",
      "2             0.0  ...        0.0        0.0        0.0        0.0        0.0   \n",
      "3             0.0  ...        0.0        0.0        0.0        0.0        1.0   \n",
      "4             0.0  ...        0.0        0.0        0.0        0.0        0.0   \n",
      "\n",
      "   month_mar  month_may  month_nov  month_oct  month_sep  \n",
      "0        0.0        0.0        0.0        1.0        0.0  \n",
      "1        0.0        1.0        0.0        0.0        0.0  \n",
      "2        0.0        0.0        0.0        0.0        0.0  \n",
      "3        0.0        0.0        0.0        0.0        0.0  \n",
      "4        0.0        1.0        0.0        0.0        0.0  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "# Combine preprocessed features\n",
    "X_preprocessed = pd.concat([X_numerical_df, X_categorical_df], axis=1)\n",
    "\n",
    "# Convert target variable to binary using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(bank_data['y'])\n",
    "print(f\"\\nTarget variable mapping: {label_encoder.classes_[0]} -> 0, {label_encoder.classes_[1]} -> 1\")\n",
    "\n",
    "# Check class imbalance before SMOTE\n",
    "print(\"\\nOriginal class distribution:\")\n",
    "print(Counter(y))\n",
    "\n",
    "# Apply SMOTE to handle class imbalance\n",
    "print(\"\\nApplying SMOTE...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_preprocessed, y)\n",
    "\n",
    "print(\"\\nResampled class distribution:\")\n",
    "print(Counter(y_resampled))\n",
    "\n",
    "# Visualize class distribution before and after SMOTE (optional, already done in EDA)\n",
    "# plt.figure(figsize=(12, 5))\n",
    "# ... (plotting code from final_eda.py or final_gpt.py) ...\n",
    "# plt.close()\n",
    "\n",
    "print(\"\\nFinal preprocessed dataset shape (after SMOTE):\", X_resampled.shape)\n",
    "print(\"Preprocessing for prob1_bank.csv completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22605979",
   "metadata": {},
   "source": [
    "## 2. `prob2_card.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b72665e",
   "metadata": {},
   "source": [
    "### 0. 라이브러리 임포트 및 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ffbfbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit Card Dataset Shape: (8950, 6)\n",
      "\n",
      "Credit Card Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8950 entries, 0 to 8949\n",
      "Data columns (total 6 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   CUST_ID              8950 non-null   object \n",
      " 1   BALANCE              8950 non-null   float64\n",
      " 2   BALANCE_FREQUENCY    8950 non-null   float64\n",
      " 3   PURCHASES            8950 non-null   float64\n",
      " 4   PURCHASES_FREQUENCY  8950 non-null   float64\n",
      " 5   PURCHASES_TRX        8950 non-null   int64  \n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 419.7+ KB\n",
      "None\n",
      "\n",
      "Credit Card Dataset Summary Statistics:\n",
      "            BALANCE  BALANCE_FREQUENCY     PURCHASES  PURCHASES_FREQUENCY  \\\n",
      "count   8950.000000        8950.000000   8950.000000          8950.000000   \n",
      "mean    1564.474828           0.877271   1003.204834             0.490351   \n",
      "std     2081.531879           0.236904   2136.634782             0.401371   \n",
      "min        0.000000           0.000000      0.000000             0.000000   \n",
      "25%      128.281915           0.888889     39.635000             0.083333   \n",
      "50%      873.385231           1.000000    361.280000             0.500000   \n",
      "75%     2054.140036           1.000000   1110.130000             0.916667   \n",
      "max    19043.138560           1.000000  49039.570000             1.000000   \n",
      "\n",
      "       PURCHASES_TRX  \n",
      "count    8950.000000  \n",
      "mean       14.709832  \n",
      "std        24.857649  \n",
      "min         0.000000  \n",
      "25%         1.000000  \n",
      "50%         7.000000  \n",
      "75%        17.000000  \n",
      "max       358.000000  \n",
      "\n",
      "Missing values in each column:\n",
      "CUST_ID                0\n",
      "BALANCE                0\n",
      "BALANCE_FREQUENCY      0\n",
      "PURCHASES              0\n",
      "PURCHASES_FREQUENCY    0\n",
      "PURCHASES_TRX          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries (some may be re-imported for clarity)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from math import pi\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the data\n",
    "card_data = pd.read_csv('prob2_card.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(\"\\n--- Credit Card Dataset Analysis ---\")\n",
    "print(\"Credit Card Dataset Shape:\", card_data.shape)\n",
    "print(\"\\nCredit Card Dataset Info:\")\n",
    "print(card_data.info())\n",
    "# print(\"\\nCredit Card Dataset Summary Statistics:\") # EDA에서 상세히 다룸\n",
    "# print(card_data.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(card_data.isnull().sum())\n",
    "# EDA 결과 또는 간단한 확인 결과, 결측치가 없음을 확인.\n",
    "\n",
    "# Set CUST_ID as index and exclude it from analysis\n",
    "if 'CUST_ID' in card_data.columns:\n",
    "    card_data.set_index('CUST_ID', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc620e0",
   "metadata": {},
   "source": [
    "### 1. 데이터 전처리 (스케일링)\n",
    "\n",
    "- 클러스터링 알고리즘은 거리에 기반하므로, 변수들의 스케일을 통일하는 것이 중요합니다.\n",
    "- StandardScaler를 사용하여 모든 특성을 표준화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d72c20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 2, the silhouette score is 0.338\n",
      "For n_clusters = 3, the silhouette score is 0.390\n",
      "For n_clusters = 4, the silhouette score is 0.422\n",
      "For n_clusters = 5, the silhouette score is 0.432\n",
      "For n_clusters = 6, the silhouette score is 0.436\n",
      "For n_clusters = 7, the silhouette score is 0.420\n",
      "For n_clusters = 8, the silhouette score is 0.404\n",
      "For n_clusters = 9, the silhouette score is 0.404\n",
      "For n_clusters = 10, the silhouette score is 0.385\n",
      "\n",
      "K-Means Cluster Analysis:\n",
      "                    BALANCE  BALANCE_FREQUENCY    PURCHASES  \\\n",
      "KMeans_Cluster                                                \n",
      "0               2397.403425           0.975133   230.473778   \n",
      "1                155.455113           0.399335   353.697838   \n",
      "2               1084.165576           0.967703  1316.539614   \n",
      "3               3983.543132           0.990719  7590.196867   \n",
      "\n",
      "                PURCHASES_FREQUENCY  PURCHASES_TRX  \n",
      "KMeans_Cluster                                      \n",
      "0                          0.132477       2.381541  \n",
      "1                          0.281043       4.446465  \n",
      "2                          0.872300      21.968091  \n",
      "3                          0.957607      98.156658  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preprocess data: scaling features\n",
    "print(\"\\nScaling credit card features using StandardScaler...\")\n",
    "scaler_card = StandardScaler()\n",
    "card_data_scaled = scaler_card.fit_transform(card_data)\n",
    "card_data_scaled_df = pd.DataFrame(card_data_scaled, columns=card_data.columns, index=card_data.index)\n",
    "# print(card_data_scaled_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b357dba4",
   "metadata": {},
   "source": [
    "### 2. K-Means Clustering\n",
    "\n",
    "- Elbow Method와 Silhouette Score를 사용하여 최적의 클러스터 수(k)를 결정합니다.\n",
    "- 결정된 k로 K-Means 클러스터링을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdac1f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 8\n",
      "Number of noise points: 418\n",
      "\n",
      "DBSCAN Cluster Analysis:\n",
      "                    BALANCE  BALANCE_FREQUENCY    PURCHASES  \\\n",
      "DBSCAN_Cluster                                                \n",
      "-1              4675.868591           0.918819  6328.086340   \n",
      " 0              1394.844559           0.874662   723.455754   \n",
      " 1              9582.148771           1.000000   610.972000   \n",
      " 2              8629.272361           1.000000  1191.746000   \n",
      " 3               720.027011           1.000000  8691.930000   \n",
      " 4               839.811628           1.000000  8093.328750   \n",
      " 5              8117.466911           1.000000  2360.976000   \n",
      " 6              5628.585978           1.000000  4196.212000   \n",
      " 7              6059.153707           1.000000  6107.142000   \n",
      "\n",
      "                PURCHASES_FREQUENCY  PURCHASES_TRX  KMeans_Cluster  \n",
      "DBSCAN_Cluster                                                      \n",
      "-1                         0.797835      70.992823        2.200957  \n",
      " 0                         0.473034      11.736136        1.039680  \n",
      " 1                         1.000000      14.400000        0.800000  \n",
      " 2                         0.983333      26.600000        2.000000  \n",
      " 3                         0.986111     117.166667        3.000000  \n",
      " 4                         1.000000      59.375000        3.000000  \n",
      " 5                         1.000000      47.800000        2.200000  \n",
      " 6                         0.766667      24.200000        2.000000  \n",
      " 7                         1.000000     112.000000        3.000000  \n"
     ]
    }
   ],
   "source": [
    "# Find optimal number of clusters using Elbow Method and Silhouette Score\n",
    "print(\"\\nFinding optimal k for K-Means...\")\n",
    "inertia = []\n",
    "silhouette_scores_kmeans = []\n",
    "k_range = range(2, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(card_data_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    labels_kmeans = kmeans.labels_\n",
    "    silhouette_avg_kmeans = silhouette_score(card_data_scaled, labels_kmeans)\n",
    "    silhouette_scores_kmeans.append(silhouette_avg_kmeans)\n",
    "    # print(f\"For n_clusters = {k}, the K-Means silhouette score is {silhouette_avg_kmeans:.3f}\")\n",
    "\n",
    "# Plot Elbow Method and Silhouette Scores\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_range, inertia, 'bo-')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_range, silhouette_scores_kmeans, 'ro-')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score for Optimal k')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(os.path.join(plots_dir, 'kmeans_optimal_k.png')) # EDA에서 저장됨\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Select optimal k based on elbow method and silhouette score\n",
    "optimal_k = k_range[np.argmax(silhouette_scores_kmeans)] \n",
    "print(f\"Selected optimal k for K-Means: {optimal_k} (Silhouette Score: {max(silhouette_scores_kmeans):.3f})\")\n",
    "\n",
    "# Apply K-Means clustering with optimal k\n",
    "print(f\"\\nApplying K-Means with k={optimal_k}...\")\n",
    "kmeans_optimal = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "kmeans_labels = kmeans_optimal.fit_predict(card_data_scaled)\n",
    "\n",
    "# Add cluster labels to the original DataFrame for analysis\n",
    "card_data['KMeans_Cluster'] = kmeans_labels\n",
    "\n",
    "# Analyze K-Means clusters\n",
    "kmeans_cluster_analysis = card_data.groupby('KMeans_Cluster').mean()\n",
    "print(\"\\nK-Means Cluster Analysis (Mean Values):\")\n",
    "print(kmeans_cluster_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04ac14a",
   "metadata": {},
   "source": [
    "### 3. DBSCAN 클러스터링\n",
    "\n",
    "- k-distance plot을 사용하여 적절한 epsilon(eps) 값을 탐색합니다.\n",
    "- 선택된 eps와 min_samples로 DBSCAN 클러스터링을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feded8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Means Silhouette Score: 0.422\n",
      "DBSCAN Silhouette Score: 0.255\n",
      "\n",
      "Selected model: K-Means\n"
     ]
    }
   ],
   "source": [
    "# Find optimal epsilon using k-distance graph for DBSCAN\n",
    "print(\"\\nFinding optimal epsilon for DBSCAN using k-distance graph...\")\n",
    "# Calculate distances to nearest neighbors (using k=5 based on common practice)\n",
    "k_neighbors = 5 * 2 # min_samples * 2 is a common heuristic, or based on feature count\n",
    "neighbors = NearestNeighbors(n_neighbors=k_neighbors)\n",
    "neighbors_fit = neighbors.fit(card_data_scaled)\n",
    "distances, indices = neighbors_fit.kneighbors(card_data_scaled)\n",
    "\n",
    "# Sort distances to the k-th neighbor\n",
    "distances_k = np.sort(distances[:, k_neighbors-1], axis=0)\n",
    "\n",
    "# Plot k-distance graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(distances_k)\n",
    "plt.title(f'k-Distance Graph (k={k_neighbors})')\n",
    "plt.xlabel('Data Points (sorted)')\n",
    "plt.ylabel(f'Distance to {k_neighbors}-th Nearest Neighbor')\n",
    "# plt.axhline(y=3.5, color='r', linestyle='--') # Example threshold line, adjust based on plot\n",
    "plt.grid(True)\n",
    "# plt.savefig(os.path.join(plots_dir, 'dbscan_epsilon_selection.png')) # EDA에서 저장됨\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Select eps and min_samples based on the k-distance plot and data characteristics\n",
    "eps_dbscan = 3.5 # Adjust this value based on the 'elbow' in the k-distance plot\n",
    "min_samples_dbscan = k_neighbors # Often set to k used for k-distance or slightly higher\n",
    "\n",
    "print(f\"Selected DBSCAN parameters: eps={eps_dbscan}, min_samples={min_samples_dbscan}\")\n",
    "\n",
    "# Apply DBSCAN with selected parameters\n",
    "print(\"\\nApplying DBSCAN...\")\n",
    "dbscan = DBSCAN(eps=eps_dbscan, min_samples=min_samples_dbscan)\n",
    "dbscan_labels = dbscan.fit_predict(card_data_scaled)\n",
    "\n",
    "# Add cluster labels to the original DataFrame\n",
    "card_data['DBSCAN_Cluster'] = dbscan_labels\n",
    "\n",
    "# Analyze DBSCAN results\n",
    "n_clusters_dbscan = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "n_noise_dbscan = list(dbscan_labels).count(-1)\n",
    "\n",
    "print(f'\\nDBSCAN found {n_clusters_dbscan} clusters and {n_noise_dbscan} noise points ({n_noise_dbscan/len(card_data)*100:.2f}%).')\n",
    "\n",
    "# Analyze DBSCAN clusters (excluding noise)\n",
    "dbscan_cluster_analysis = card_data[card_data['DBSCAN_Cluster'] != -1].groupby('DBSCAN_Cluster').mean()\n",
    "print(\"\\nDBSCAN Cluster Analysis (Mean Values, excluding noise):\")\n",
    "print(dbscan_cluster_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08f6e37",
   "metadata": {},
   "source": [
    "### 4. 모델 비교 및 선택\n",
    "\n",
    "- Silhouette Score를 기준으로 K-Means와 DBSCAN 결과를 비교합니다.\n",
    "- DBSCAN의 경우, 노이즈 포인트를 제외하고 Silhouette Score를 계산합니다.\n",
    "- 더 높은 점수를 기록한 모델을 최종 모델로 선택합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d9529ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected Model Cluster Analysis:\n",
      "                    BALANCE  BALANCE_FREQUENCY    PURCHASES  \\\n",
      "KMeans_Cluster                                                \n",
      "0               2397.403425           0.975133   230.473778   \n",
      "1                155.455113           0.399335   353.697838   \n",
      "2               1084.165576           0.967703  1316.539614   \n",
      "3               3983.543132           0.990719  7590.196867   \n",
      "\n",
      "                PURCHASES_FREQUENCY  PURCHASES_TRX  DBSCAN_Cluster  \n",
      "KMeans_Cluster                                                      \n",
      "0                          0.132477       2.381541       -0.017539  \n",
      "1                          0.281043       4.446465       -0.012795  \n",
      "2                          0.872300      21.968091       -0.011654  \n",
      "3                          0.957607      98.156658       -0.368146  \n"
     ]
    }
   ],
   "source": [
    "# Compare K-Means and DBSCAN using Silhouette Score\n",
    "kmeans_silhouette_score = silhouette_score(card_data_scaled, kmeans_labels)\n",
    "\n",
    "# Calculate silhouette score for DBSCAN (excluding noise points)\n",
    "dbscan_valid_indices = card_data['DBSCAN_Cluster'] != -1\n",
    "if np.sum(dbscan_valid_indices) > 1 and len(set(dbscan_labels[dbscan_valid_indices])) > 1:\n",
    "    dbscan_silhouette_score = silhouette_score(card_data_scaled[dbscan_valid_indices], dbscan_labels[dbscan_valid_indices])\n",
    "else:\n",
    "    dbscan_silhouette_score = -1 # Cannot compute silhouette score\n",
    "\n",
    "print(f\"\\nK-Means Silhouette Score: {kmeans_silhouette_score:.3f}\")\n",
    "print(f\"DBSCAN Silhouette Score (excluding noise): {dbscan_silhouette_score:.3f}\")\n",
    "\n",
    "# Decision on which model to choose\n",
    "if kmeans_silhouette_score >= dbscan_silhouette_score:\n",
    "    selected_model_name = \"K-Means\"\n",
    "    selected_labels = kmeans_labels\n",
    "    selected_cluster_col = 'KMeans_Cluster'\n",
    "    print(\"\\nSelected model: K-Means (Higher or Equal Silhouette Score)\")\n",
    "else:\n",
    "    selected_model_name = \"DBSCAN\"\n",
    "    selected_labels = dbscan_labels\n",
    "    selected_cluster_col = 'DBSCAN_Cluster'\n",
    "    print(\"\\nSelected model: DBSCAN (Higher Silhouette Score)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc18ee8",
   "metadata": {},
   "source": [
    "### 5. 선택된 모델의 군집 특성 분석 (Radar Chart)\n",
    "\n",
    "- 선택된 클러스터링 모델(K-Means 또는 DBSCAN)의 결과를 사용하여 군집별 특성을 분석합니다.\n",
    "- Radar Chart를 사용하여 각 군집의 프로파일을 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2080e6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze selected model's clusters\n",
    "print(f\"\\nAnalyzing clusters from {selected_model_name}...\")\n",
    "cluster_analysis_selected = card_data.groupby(selected_cluster_col).mean()\n",
    "\n",
    "# Prepare data for radar chart (use scaled data for profile comparison)\n",
    "# We need the mean of the *scaled* data per cluster\n",
    "card_data_scaled_df['Cluster'] = selected_labels\n",
    "radar_df_data = card_data_scaled_df.groupby('Cluster').mean()\n",
    "\n",
    "# Remove noise cluster (-1) if DBSCAN was selected\n",
    "if selected_model_name == \"DBSCAN\" and -1 in radar_df_data.index:\n",
    "    radar_df_data = radar_df_data.drop(-1)\n",
    "\n",
    "# Function to create radar chart\n",
    "def radar_chart(df, title):\n",
    "    categories = list(df.columns)\n",
    "    N = len(categories)\n",
    "    \n",
    "    # Create angle list\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 12), subplot_kw=dict(polar=True))\n",
    "    \n",
    "    # Draw one axis per variable and add labels\n",
    "    plt.xticks(angles[:-1], categories, size=11)\n",
    "    \n",
    "    # Draw ylabels\n",
    "    ax.set_rlabel_position(0)\n",
    "    # Determine sensible y-axis limits based on scaled data range\n",
    "    min_val = df.min().min() - 0.5\n",
    "    max_val = df.max().max() + 0.5\n",
    "    yticks = np.linspace(np.floor(min_val), np.ceil(max_val), 5)\n",
    "    plt.yticks(yticks, [f\"{tick:.1f}\" for tick in yticks], color=\"grey\", size=10)\n",
    "    plt.ylim(np.floor(min_val), np.ceil(max_val))\n",
    "    \n",
    "    # Plot each cluster\n",
    "    for i in range(len(df)):\n",
    "        values = df.iloc[i].values.tolist()\n",
    "        values += values[:1]\n",
    "        cluster_label = df.index[i]\n",
    "        ax.plot(angles, values, linewidth=2, linestyle='solid', label=f\"Cluster {cluster_label}\")\n",
    "        ax.fill(angles, values, alpha=0.1)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    plt.title(title, size=15, y=1.1)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create radar chart for selected model\n",
    "print(\"\\nGenerating Radar Chart for cluster profiles...\")\n",
    "radar_fig = radar_chart(radar_df_data, f'{selected_model_name} Cluster Profiles (Standardized Means)')\n",
    "# radar_fig.savefig(os.path.join(plots_dir, 'cluster_radar_chart.png')) # EDA에서 저장됨\n",
    "plt.show()\n",
    "plt.close(radar_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d0a380",
   "metadata": {},
   "source": [
    "### 6. t-SNE 시각화\n",
    "\n",
    "- t-SNE 알고리즘을 적용하여 고차원 데이터를 2차원으로 축소합니다.\n",
    "- 선택된 모델의 군집 레이블에 따라 색상을 달리하여 2차원 산점도로 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d767d2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply t-SNE for dimensionality reduction and visualization\n",
    "print(\"\\nApplying t-SNE for visualization...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "tsne_results = tsne.fit_transform(card_data_scaled) # Use scaled data\n",
    "\n",
    "# Create DataFrame with t-SNE results and selected cluster labels\n",
    "tsne_df = pd.DataFrame(data={'t-SNE Dim 1': tsne_results[:, 0], 't-SNE Dim 2': tsne_results[:, 1]})\n",
    "tsne_df['Cluster'] = selected_labels\n",
    "\n",
    "# Visualize t-SNE results with cluster colors\n",
    "plt.figure(figsize=(12, 10))\n",
    "unique_labels = sorted(tsne_df['Cluster'].unique())\n",
    "palette = sns.color_palette(\"hls\", len(unique_labels))\n",
    "\n",
    "scatter = sns.scatterplot(\n",
    "    x='t-SNE Dim 1', y='t-SNE Dim 2',\n",
    "    hue='Cluster',\n",
    "    palette=palette,\n",
    "    hue_order=unique_labels, # Ensure consistent color mapping\n",
    "    data=tsne_df,\n",
    "    legend=\"full\",\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title(f't-SNE Visualization with {selected_model_name} Clusters', fontsize=14)\n",
    "plt.legend(title='Cluster')\n",
    "# plt.savefig(os.path.join(plots_dir, 'tsne_visualization.png')) # EDA에서 저장됨\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nAnalysis for prob2_card.csv completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855862db",
   "metadata": {},
   "source": [
    "## 3. 주택 가격 데이터 (XGBoost 과제)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf90824",
   "metadata": {},
   "source": [
    "### (설명 생략 - EDA 대상 아님)\n",
    "- 이 섹션은 `final_exam.md`의 XGBoost 관련 계산 과제를 위한 코드 또는 설명을 포함할 수 있으나, 현재 EDA의 범위는 아닙니다.\n",
    "- `final_gpt.py`의 원래 코드에는 이 부분에 대한 구현이 포함되어 있지 않았습니다.\n",
    "- 필요시, 별도의 셀에서 XGBoost 계산 과정을 수동으로 구현하거나 설명할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129859ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시: 주택 가격 데이터셋 출력 (참고용)\n",
    "housing_data = pd.DataFrame({\n",
    "    'ID': [1, 2, 3, 4, 5, 6],\n",
    "    'X': [3, 1, 2, 4, None, None],\n",
    "    'Y': [1.25, 1.20, 1.30, 1.50, 1.40, 1.30]\n",
    "})\n",
    "\n",
    "print(\"\\n--- Housing Dataset (XGBoost Problem) ---\")\n",
    "print(housing_data)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
