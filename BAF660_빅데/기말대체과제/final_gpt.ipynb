{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a662ced",
   "metadata": {},
   "source": [
    "# 빅데이터 기말대체과제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c37522c",
   "metadata": {},
   "source": [
    "## 1. `prob1_bank.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33297ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bank Dataset Shape: (4521, 11)\n",
      "\n",
      "Bank Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4521 entries, 0 to 4520\n",
      "Data columns (total 11 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   age        4521 non-null   int64 \n",
      " 1   job        4521 non-null   object\n",
      " 2   marital    4521 non-null   object\n",
      " 3   education  4521 non-null   object\n",
      " 4   default    4521 non-null   object\n",
      " 5   balance    4521 non-null   int64 \n",
      " 6   housing    4521 non-null   object\n",
      " 7   loan       4521 non-null   object\n",
      " 8   contact    4521 non-null   object\n",
      " 9   month      4521 non-null   object\n",
      " 10  y          4521 non-null   object\n",
      "dtypes: int64(2), object(9)\n",
      "memory usage: 388.7+ KB\n",
      "None\n",
      "\n",
      "Bank Dataset Summary Statistics:\n",
      "               age       balance\n",
      "count  4521.000000   4521.000000\n",
      "mean     41.170095   1422.657819\n",
      "std      10.576211   3009.638142\n",
      "min      19.000000  -3313.000000\n",
      "25%      33.000000     69.000000\n",
      "50%      39.000000    444.000000\n",
      "75%      49.000000   1480.000000\n",
      "max      87.000000  71188.000000\n",
      "\n",
      "Missing values in each column:\n",
      "age          0\n",
      "job          0\n",
      "marital      0\n",
      "education    0\n",
      "default      0\n",
      "balance      0\n",
      "housing      0\n",
      "loan         0\n",
      "contact      0\n",
      "month        0\n",
      "y            0\n",
      "dtype: int64\n",
      "\n",
      "Target variable distribution:\n",
      "y\n",
      "no     4000\n",
      "yes     521\n",
      "Name: count, dtype: int64\n",
      "y\n",
      "no     88.476001\n",
      "yes    11.523999\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "# Create plots directory if it doesn't exist\n",
    "plots_dir = 'plots'\n",
    "if not os.path.exists(plots_dir):\n",
    "    os.makedirs(plots_dir)\n",
    "\n",
    "# Load the data\n",
    "bank_data = pd.read_csv('prob1_bank.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(\"Bank Dataset Shape:\", bank_data.shape)\n",
    "print(\"\\nBank Dataset Info:\")\n",
    "print(bank_data.info())\n",
    "print(\"\\nBank Dataset Summary Statistics:\")\n",
    "print(bank_data.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(bank_data.isnull().sum())\n",
    "\n",
    "# Check target variable distribution (class imbalance)\n",
    "print(\"\\nTarget variable distribution:\")\n",
    "print(bank_data['y'].value_counts())\n",
    "print(bank_data['y'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Visualize target distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='y', data=bank_data)\n",
    "plt.title('Target Variable Distribution')\n",
    "plt.savefig(os.path.join(plots_dir, 'target_distribution.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e330b414",
   "metadata": {},
   "source": [
    "### 1. 범주형 변수 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14f9faec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical variables: ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'y']\n",
      "Numerical variables: ['age', 'balance']\n",
      "\n",
      "Unique values in job:\n",
      "job\n",
      "management       969\n",
      "blue-collar      946\n",
      "technician       768\n",
      "admin.           478\n",
      "services         417\n",
      "retired          230\n",
      "self-employed    183\n",
      "entrepreneur     168\n",
      "unemployed       128\n",
      "housemaid        112\n",
      "student           84\n",
      "unknown           38\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique values in marital:\n",
      "marital\n",
      "married     2797\n",
      "single      1196\n",
      "divorced     528\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique values in education:\n",
      "education\n",
      "secondary    2306\n",
      "tertiary     1350\n",
      "primary       678\n",
      "unknown       187\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique values in default:\n",
      "default\n",
      "no     4445\n",
      "yes      76\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique values in housing:\n",
      "housing\n",
      "yes    2559\n",
      "no     1962\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique values in loan:\n",
      "loan\n",
      "no     3830\n",
      "yes     691\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique values in contact:\n",
      "contact\n",
      "cellular     2896\n",
      "unknown      1324\n",
      "telephone     301\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique values in month:\n",
      "month\n",
      "may    1398\n",
      "jul     706\n",
      "aug     633\n",
      "jun     531\n",
      "nov     389\n",
      "apr     293\n",
      "feb     222\n",
      "jan     148\n",
      "oct      80\n",
      "sep      52\n",
      "mar      49\n",
      "dec      20\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique values in y:\n",
      "y\n",
      "no     4000\n",
      "yes     521\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Shape of encoded categorical features: (4521, 32)\n",
      "   job_blue-collar  job_entrepreneur  job_housemaid  job_management  \\\n",
      "0              0.0               0.0            0.0             0.0   \n",
      "1              0.0               0.0            0.0             0.0   \n",
      "2              0.0               0.0            0.0             1.0   \n",
      "3              0.0               0.0            0.0             1.0   \n",
      "4              1.0               0.0            0.0             0.0   \n",
      "\n",
      "   job_retired  job_self-employed  job_services  job_student  job_technician  \\\n",
      "0          0.0                0.0           0.0          0.0             0.0   \n",
      "1          0.0                0.0           1.0          0.0             0.0   \n",
      "2          0.0                0.0           0.0          0.0             0.0   \n",
      "3          0.0                0.0           0.0          0.0             0.0   \n",
      "4          0.0                0.0           0.0          0.0             0.0   \n",
      "\n",
      "   job_unemployed  ...  month_dec  month_feb  month_jan  month_jul  month_jun  \\\n",
      "0             1.0  ...        0.0        0.0        0.0        0.0        0.0   \n",
      "1             0.0  ...        0.0        0.0        0.0        0.0        0.0   \n",
      "2             0.0  ...        0.0        0.0        0.0        0.0        0.0   \n",
      "3             0.0  ...        0.0        0.0        0.0        0.0        1.0   \n",
      "4             0.0  ...        0.0        0.0        0.0        0.0        0.0   \n",
      "\n",
      "   month_mar  month_may  month_nov  month_oct  month_sep  \n",
      "0        0.0        0.0        0.0        1.0        0.0  \n",
      "1        0.0        1.0        0.0        0.0        0.0  \n",
      "2        0.0        0.0        0.0        0.0        0.0  \n",
      "3        0.0        0.0        0.0        0.0        0.0  \n",
      "4        0.0        1.0        0.0        0.0        0.0  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical and numerical columns\n",
    "categorical_cols = bank_data.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = bank_data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(\"Categorical variables:\", categorical_cols)\n",
    "print(\"Numerical variables:\", numerical_cols)\n",
    "\n",
    "# Analyze categorical variables\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\nUnique values in {col}:\")\n",
    "    print(bank_data[col].value_counts())\n",
    "    \n",
    "    # Visualize distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bank_data[col].value_counts().plot(kind='bar')\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plots_dir, f'distribution_{col}.png'))\n",
    "    plt.close()\n",
    "\n",
    "# Preprocessing categorical variables using OneHotEncoder\n",
    "categorical_preprocessor = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Apply one-hot encoding\n",
    "X_categorical = bank_data[categorical_cols[:-1]]  # Exclude target variable 'y'\n",
    "categorical_preprocessor.fit(X_categorical)\n",
    "X_categorical_encoded = categorical_preprocessor.transform(X_categorical)\n",
    "\n",
    "# Create DataFrame with encoded features\n",
    "encoded_feature_names = []\n",
    "for i, cat_col in enumerate(categorical_cols[:-1]):\n",
    "    categories = categorical_preprocessor.categories_[i][1:]  # Skip the first category (dropped)\n",
    "    encoded_feature_names.extend([f\"{cat_col}_{cat}\" for cat in categories])\n",
    "\n",
    "X_categorical_df = pd.DataFrame(X_categorical_encoded, columns=encoded_feature_names)\n",
    "\n",
    "print(\"\\nShape of encoded categorical features:\", X_categorical_df.shape)\n",
    "print(X_categorical_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb58781",
   "metadata": {},
   "source": [
    "### 2. 수치형 변수 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82ed7b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary statistics for age:\n",
      "count    4521.000000\n",
      "mean       41.170095\n",
      "std        10.576211\n",
      "min        19.000000\n",
      "25%        33.000000\n",
      "50%        39.000000\n",
      "75%        49.000000\n",
      "max        87.000000\n",
      "Name: age, dtype: float64\n",
      "\n",
      "Summary statistics for balance:\n",
      "count     4521.000000\n",
      "mean      1422.657819\n",
      "std       3009.638142\n",
      "min      -3313.000000\n",
      "25%         69.000000\n",
      "50%        444.000000\n",
      "75%       1480.000000\n",
      "max      71188.000000\n",
      "Name: balance, dtype: float64\n",
      "\n",
      "Shape of scaled numerical features: (4521, 2)\n",
      "        age   balance\n",
      "0 -1.056270  0.121072\n",
      "1 -0.772583  1.118644\n",
      "2 -0.583458 -0.024144\n",
      "3 -1.056270  0.017726\n",
      "4  1.686036 -0.472753\n"
     ]
    }
   ],
   "source": [
    "# Analyze numerical variables\n",
    "for col in numerical_cols:\n",
    "    # Summary statistics\n",
    "    print(f\"\\nSummary statistics for {col}:\")\n",
    "    print(bank_data[col].describe())\n",
    "    \n",
    "    # Check for outliers with box plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(y=bank_data[col])\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.savefig(os.path.join(plots_dir, f'boxplot_{col}.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Distribution\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(bank_data[col], kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.savefig(os.path.join(plots_dir, f'hist_{col}.png'))\n",
    "    plt.close()\n",
    "\n",
    "# Apply standardization to numerical features\n",
    "scaler = StandardScaler()\n",
    "X_numerical = bank_data[numerical_cols]\n",
    "X_numerical_scaled = scaler.fit_transform(X_numerical)\n",
    "\n",
    "# Create DataFrame with scaled features\n",
    "X_numerical_df = pd.DataFrame(X_numerical_scaled, columns=numerical_cols)\n",
    "\n",
    "print(\"\\nShape of scaled numerical features:\", X_numerical_df.shape)\n",
    "print(X_numerical_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eac0f7",
   "metadata": {},
   "source": [
    "### 3. 클래스 불균형 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ea669c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution:\n",
      "Counter({0: 4000, 1: 521})\n",
      "Resampled class distribution:\n",
      "Counter({0: 4000, 1: 4000})\n",
      "Final preprocessed dataset shape: (8000, 34)\n"
     ]
    }
   ],
   "source": [
    "# Combine preprocessed features\n",
    "X_preprocessed = pd.concat([X_numerical_df, X_categorical_df], axis=1)\n",
    "\n",
    "# Convert target variable to binary\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(bank_data['y'])\n",
    "\n",
    "# Check class imbalance\n",
    "print(\"Original class distribution:\")\n",
    "print(Counter(y))\n",
    "\n",
    "# Apply SMOTE to handle class imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_preprocessed, y)\n",
    "\n",
    "print(\"Resampled class distribution:\")\n",
    "print(Counter(y_resampled))\n",
    "\n",
    "# Visualize class distribution before and after SMOTE\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x=y)\n",
    "plt.title('Original Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.xticks([0, 1], ['No', 'Yes'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x=y_resampled)\n",
    "plt.title('Class Distribution After SMOTE')\n",
    "plt.xlabel('Class')\n",
    "plt.xticks([0, 1], ['No', 'Yes'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plots_dir, 'class_balance_comparison.png'))\n",
    "plt.close()\n",
    "\n",
    "print(\"Final preprocessed dataset shape:\", X_resampled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22605979",
   "metadata": {},
   "source": [
    "## 2. `prob2_card.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ffbfbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit Card Dataset Shape: (8950, 6)\n",
      "\n",
      "Credit Card Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8950 entries, 0 to 8949\n",
      "Data columns (total 6 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   CUST_ID              8950 non-null   object \n",
      " 1   BALANCE              8950 non-null   float64\n",
      " 2   BALANCE_FREQUENCY    8950 non-null   float64\n",
      " 3   PURCHASES            8950 non-null   float64\n",
      " 4   PURCHASES_FREQUENCY  8950 non-null   float64\n",
      " 5   PURCHASES_TRX        8950 non-null   int64  \n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 419.7+ KB\n",
      "None\n",
      "\n",
      "Credit Card Dataset Summary Statistics:\n",
      "            BALANCE  BALANCE_FREQUENCY     PURCHASES  PURCHASES_FREQUENCY  \\\n",
      "count   8950.000000        8950.000000   8950.000000          8950.000000   \n",
      "mean    1564.474828           0.877271   1003.204834             0.490351   \n",
      "std     2081.531879           0.236904   2136.634782             0.401371   \n",
      "min        0.000000           0.000000      0.000000             0.000000   \n",
      "25%      128.281915           0.888889     39.635000             0.083333   \n",
      "50%      873.385231           1.000000    361.280000             0.500000   \n",
      "75%     2054.140036           1.000000   1110.130000             0.916667   \n",
      "max    19043.138560           1.000000  49039.570000             1.000000   \n",
      "\n",
      "       PURCHASES_TRX  \n",
      "count    8950.000000  \n",
      "mean       14.709832  \n",
      "std        24.857649  \n",
      "min         0.000000  \n",
      "25%         1.000000  \n",
      "50%         7.000000  \n",
      "75%        17.000000  \n",
      "max       358.000000  \n",
      "\n",
      "Missing values in each column:\n",
      "CUST_ID                0\n",
      "BALANCE                0\n",
      "BALANCE_FREQUENCY      0\n",
      "PURCHASES              0\n",
      "PURCHASES_FREQUENCY    0\n",
      "PURCHASES_TRX          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.manifold import TSNE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the data\n",
    "card_data = pd.read_csv('prob2_card.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(\"Credit Card Dataset Shape:\", card_data.shape)\n",
    "print(\"\\nCredit Card Dataset Info:\")\n",
    "print(card_data.info())\n",
    "print(\"\\nCredit Card Dataset Summary Statistics:\")\n",
    "print(card_data.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(card_data.isnull().sum())\n",
    "\n",
    "# Set CUST_ID as index and exclude it from analysis\n",
    "card_data.set_index('CUST_ID', inplace=True)\n",
    "\n",
    "# Visualize distribution of features\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, column in enumerate(card_data.columns):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    sns.histplot(card_data[column], kde=True)\n",
    "    plt.title(f'Distribution of {column}')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plots_dir, 'card_features_distribution.png'))\n",
    "plt.close()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(card_data.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.savefig(os.path.join(plots_dir, 'correlation_heatmap.png'))\n",
    "plt.close()\n",
    "\n",
    "# Preprocess data: scaling features\n",
    "scaler = StandardScaler()\n",
    "card_data_scaled = scaler.fit_transform(card_data)\n",
    "card_data_scaled_df = pd.DataFrame(card_data_scaled, columns=card_data.columns, index=card_data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc620e0",
   "metadata": {},
   "source": [
    "### 1. K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d72c20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 2, the silhouette score is 0.338\n",
      "For n_clusters = 3, the silhouette score is 0.390\n",
      "For n_clusters = 4, the silhouette score is 0.422\n",
      "For n_clusters = 5, the silhouette score is 0.432\n",
      "For n_clusters = 6, the silhouette score is 0.436\n",
      "For n_clusters = 7, the silhouette score is 0.420\n",
      "For n_clusters = 8, the silhouette score is 0.404\n",
      "For n_clusters = 9, the silhouette score is 0.404\n",
      "For n_clusters = 10, the silhouette score is 0.385\n",
      "\n",
      "K-Means Cluster Analysis:\n",
      "                    BALANCE  BALANCE_FREQUENCY    PURCHASES  \\\n",
      "KMeans_Cluster                                                \n",
      "0               2397.403425           0.975133   230.473778   \n",
      "1                155.455113           0.399335   353.697838   \n",
      "2               1084.165576           0.967703  1316.539614   \n",
      "3               3983.543132           0.990719  7590.196867   \n",
      "\n",
      "                PURCHASES_FREQUENCY  PURCHASES_TRX  \n",
      "KMeans_Cluster                                      \n",
      "0                          0.132477       2.381541  \n",
      "1                          0.281043       4.446465  \n",
      "2                          0.872300      21.968091  \n",
      "3                          0.957607      98.156658  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find optimal number of clusters using Elbow Method\n",
    "inertia = []\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(card_data_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    labels = kmeans.labels_\n",
    "    silhouette_avg = silhouette_score(card_data_scaled, labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "    print(f\"For n_clusters = {k}, the silhouette score is {silhouette_avg:.3f}\")\n",
    "\n",
    "# Plot Elbow Method\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_range, inertia, 'bo-')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_range, silhouette_scores, 'ro-')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score for Optimal k')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plots_dir, 'kmeans_optimal_k.png'))\n",
    "plt.close()\n",
    "\n",
    "# Select optimal k based on elbow method and silhouette score\n",
    "optimal_k = 4  # This is based on the elbow in the plot and highest silhouette score\n",
    "\n",
    "# Apply K-Means clustering with optimal k\n",
    "kmeans_optimal = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "kmeans_labels = kmeans_optimal.fit_predict(card_data_scaled)\n",
    "\n",
    "# Add cluster labels to the original DataFrame\n",
    "card_data['KMeans_Cluster'] = kmeans_labels\n",
    "\n",
    "# Visualize cluster distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='KMeans_Cluster', data=card_data)\n",
    "plt.title('Distribution of K-Means Clusters')\n",
    "plt.savefig(os.path.join(plots_dir, 'kmeans_cluster_distribution.png'))\n",
    "plt.close()\n",
    "\n",
    "# Analyze clusters\n",
    "kmeans_cluster_analysis = card_data.groupby('KMeans_Cluster').mean()\n",
    "print(\"\\nK-Means Cluster Analysis:\")\n",
    "print(kmeans_cluster_analysis)\n",
    "\n",
    "# Visualize cluster profiles\n",
    "plt.figure(figsize=(14, 8))\n",
    "kmeans_cluster_analysis.T.plot(kind='bar', figsize=(14, 8))\n",
    "plt.title('K-Means Cluster Profiles')\n",
    "plt.ylabel('Standardized Value')\n",
    "plt.xlabel('Features')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Cluster')\n",
    "plt.savefig(os.path.join(plots_dir, 'kmeans_cluster_profiles.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b357dba4",
   "metadata": {},
   "source": [
    "### 2. DBSCAN 클러스터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdac1f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 8\n",
      "Number of noise points: 418\n",
      "\n",
      "DBSCAN Cluster Analysis:\n",
      "                    BALANCE  BALANCE_FREQUENCY    PURCHASES  \\\n",
      "DBSCAN_Cluster                                                \n",
      "-1              4675.868591           0.918819  6328.086340   \n",
      " 0              1394.844559           0.874662   723.455754   \n",
      " 1              9582.148771           1.000000   610.972000   \n",
      " 2              8629.272361           1.000000  1191.746000   \n",
      " 3               720.027011           1.000000  8691.930000   \n",
      " 4               839.811628           1.000000  8093.328750   \n",
      " 5              8117.466911           1.000000  2360.976000   \n",
      " 6              5628.585978           1.000000  4196.212000   \n",
      " 7              6059.153707           1.000000  6107.142000   \n",
      "\n",
      "                PURCHASES_FREQUENCY  PURCHASES_TRX  KMeans_Cluster  \n",
      "DBSCAN_Cluster                                                      \n",
      "-1                         0.797835      70.992823        2.200957  \n",
      " 0                         0.473034      11.736136        1.039680  \n",
      " 1                         1.000000      14.400000        0.800000  \n",
      " 2                         0.983333      26.600000        2.000000  \n",
      " 3                         0.986111     117.166667        3.000000  \n",
      " 4                         1.000000      59.375000        3.000000  \n",
      " 5                         1.000000      47.800000        2.200000  \n",
      " 6                         0.766667      24.200000        2.000000  \n",
      " 7                         1.000000     112.000000        3.000000  \n"
     ]
    }
   ],
   "source": [
    "# Find optimal epsilon using k-distance graph\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Calculate distances to nearest neighbors\n",
    "neighbors = NearestNeighbors(n_neighbors=5)\n",
    "neighbors_fit = neighbors.fit(card_data_scaled)\n",
    "distances, indices = neighbors_fit.kneighbors(card_data_scaled)\n",
    "\n",
    "# Sort distances in descending order\n",
    "distances = np.sort(distances[:, 4], axis=0)  # 5th nearest neighbor\n",
    "\n",
    "# Plot k-distance graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(distances)\n",
    "plt.axhline(y=0.5, color='r', linestyle='--')  # Example threshold\n",
    "plt.title('K-Distance Graph (k=5)')\n",
    "plt.xlabel('Data Points (sorted)')\n",
    "plt.ylabel('Distance to 5th Nearest Neighbor')\n",
    "plt.savefig(os.path.join(plots_dir, 'dbscan_epsilon_selection.png'))\n",
    "plt.close()\n",
    "\n",
    "# Apply DBSCAN with selected parameters\n",
    "eps = 0.5  # Selected from k-distance graph\n",
    "min_samples = 5  # Minimum number of samples in a core point's neighborhood\n",
    "\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "dbscan_labels = dbscan.fit_predict(card_data_scaled)\n",
    "\n",
    "# Add cluster labels to the original DataFrame\n",
    "card_data['DBSCAN_Cluster'] = dbscan_labels\n",
    "\n",
    "# Count number of clusters and noise points\n",
    "n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "n_noise = list(dbscan_labels).count(-1)\n",
    "\n",
    "print(f'Number of clusters: {n_clusters}')\n",
    "print(f'Number of noise points: {n_noise}')\n",
    "\n",
    "# Visualize cluster distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='DBSCAN_Cluster', data=card_data)\n",
    "plt.title('Distribution of DBSCAN Clusters')\n",
    "plt.savefig(os.path.join(plots_dir, 'dbscan_cluster_distribution.png'))\n",
    "plt.close()\n",
    "\n",
    "# Analyze clusters\n",
    "dbscan_cluster_analysis = card_data.groupby('DBSCAN_Cluster').mean()\n",
    "print(\"\\nDBSCAN Cluster Analysis:\")\n",
    "print(dbscan_cluster_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04ac14a",
   "metadata": {},
   "source": [
    "### 3. 모델 비교 및 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feded8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Means Silhouette Score: 0.422\n",
      "DBSCAN Silhouette Score: 0.255\n",
      "\n",
      "Selected model: K-Means\n"
     ]
    }
   ],
   "source": [
    "# Compare K-Means and DBSCAN\n",
    "# Calculate silhouette score for K-Means\n",
    "kmeans_silhouette = silhouette_score(card_data_scaled, kmeans_labels) if len(set(kmeans_labels)) > 1 else 0\n",
    "\n",
    "# Calculate silhouette score for DBSCAN (excluding noise points)\n",
    "dbscan_data = card_data_scaled[dbscan_labels != -1]\n",
    "dbscan_labels_no_noise = dbscan_labels[dbscan_labels != -1]\n",
    "dbscan_silhouette = silhouette_score(dbscan_data, dbscan_labels_no_noise) if len(set(dbscan_labels_no_noise)) > 1 else 0\n",
    "\n",
    "print(f\"K-Means Silhouette Score: {kmeans_silhouette:.3f}\")\n",
    "print(f\"DBSCAN Silhouette Score: {dbscan_silhouette:.3f}\")\n",
    "\n",
    "# Compare cluster distributions\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x='KMeans_Cluster', data=card_data)\n",
    "plt.title('K-Means Clusters')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x='DBSCAN_Cluster', data=card_data)\n",
    "plt.title('DBSCAN Clusters')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plots_dir, 'cluster_comparison.png'))\n",
    "plt.close()\n",
    "\n",
    "# Decision on which model to choose\n",
    "if kmeans_silhouette > dbscan_silhouette:\n",
    "    selected_model = \"K-Means\"\n",
    "    selected_labels = kmeans_labels\n",
    "    print(\"\\nSelected model: K-Means\")\n",
    "else:\n",
    "    selected_model = \"DBSCAN\"\n",
    "    selected_labels = dbscan_labels\n",
    "    print(\"\\nSelected model: DBSCAN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08f6e37",
   "metadata": {},
   "source": [
    "### 4. 군집 특성 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d9529ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected Model Cluster Analysis:\n",
      "                    BALANCE  BALANCE_FREQUENCY    PURCHASES  \\\n",
      "KMeans_Cluster                                                \n",
      "0               2397.403425           0.975133   230.473778   \n",
      "1                155.455113           0.399335   353.697838   \n",
      "2               1084.165576           0.967703  1316.539614   \n",
      "3               3983.543132           0.990719  7590.196867   \n",
      "\n",
      "                PURCHASES_FREQUENCY  PURCHASES_TRX  DBSCAN_Cluster  \n",
      "KMeans_Cluster                                                      \n",
      "0                          0.132477       2.381541       -0.017539  \n",
      "1                          0.281043       4.446465       -0.012795  \n",
      "2                          0.872300      21.968091       -0.011654  \n",
      "3                          0.957607      98.156658       -0.368146  \n"
     ]
    }
   ],
   "source": [
    "# Analyze selected model's clusters\n",
    "if selected_model == \"K-Means\":\n",
    "    selected_clusters = card_data['KMeans_Cluster']\n",
    "    cluster_analysis = card_data.groupby('KMeans_Cluster').mean()\n",
    "else:\n",
    "    selected_clusters = card_data['DBSCAN_Cluster']\n",
    "    cluster_analysis = card_data.groupby('DBSCAN_Cluster').mean()\n",
    "\n",
    "print(\"\\nSelected Model Cluster Analysis:\")\n",
    "print(cluster_analysis)\n",
    "\n",
    "# Visualize cluster profiles with radar chart\n",
    "from math import pi\n",
    "\n",
    "# Function to create radar chart\n",
    "def radar_chart(df, title):\n",
    "    categories = list(df.columns)\n",
    "    N = len(categories)\n",
    "    \n",
    "    # Create angle list\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "    \n",
    "    # Draw one axis per variable and add labels\n",
    "    plt.xticks(angles[:-1], categories, size=12)\n",
    "    \n",
    "    # Draw ylabels\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.yticks([-2, -1, 0, 1, 2], [\"-2\", \"-1\", \"0\", \"1\", \"2\"], color=\"grey\", size=10)\n",
    "    plt.ylim(-2, 2)\n",
    "    \n",
    "    # Plot each cluster\n",
    "    for i in range(len(df)):\n",
    "        values = df.iloc[i].values.tolist()\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, linewidth=2, linestyle='solid', label=f\"Cluster {df.index[i]}\")\n",
    "        ax.fill(angles, values, alpha=0.1)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    plt.title(title, size=15, y=1.1)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Prepare data for radar chart (excluding any noise cluster)\n",
    "if -1 in cluster_analysis.index:\n",
    "    radar_df = cluster_analysis.drop(-1)\n",
    "else:\n",
    "    radar_df = cluster_analysis\n",
    "\n",
    "# Create radar chart\n",
    "radar_fig = radar_chart(radar_df, f'{selected_model} Cluster Profiles')\n",
    "radar_fig.savefig(os.path.join(plots_dir, 'cluster_radar_chart.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc18ee8",
   "metadata": {},
   "source": [
    "### 5. t-SNE 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2080e6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply t-SNE for dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "tsne_results = tsne.fit_transform(card_data_scaled)\n",
    "\n",
    "# Create DataFrame with t-SNE results\n",
    "tsne_df = pd.DataFrame(data={'t-SNE 1': tsne_results[:, 0], 't-SNE 2': tsne_results[:, 1]})\n",
    "tsne_df['Cluster'] = selected_clusters\n",
    "\n",
    "# Visualize t-SNE results with cluster colors\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = sns.scatterplot(\n",
    "    x='t-SNE 1', y='t-SNE 2',\n",
    "    hue='Cluster',\n",
    "    palette=sns.color_palette(\"hls\", len(set(selected_clusters))),\n",
    "    data=tsne_df,\n",
    "    legend=\"full\",\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.title(f't-SNE Visualization with {selected_model} Clusters', fontsize=14)\n",
    "plt.savefig(os.path.join(plots_dir, 'tsne_visualization.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d0a380",
   "metadata": {},
   "source": [
    "## 3. 주택 가격 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855862db",
   "metadata": {},
   "source": [
    "### 주어진 데이터\n",
    "\n",
    "주어진 데이터는 다음과 같습니다:\n",
    "\n",
    "| ID  | X   | Y    |\n",
    "| --- | --- | ---- |\n",
    "| 1   | 3   | 1.25 |\n",
    "| 2   | 1   | 1.20 |\n",
    "| 3   | 2   | 1.30 |\n",
    "| 4   | 4   | 1.50 |\n",
    "| 5   | ?   | 1.40 |\n",
    "| 6   | ?   | 1.30 |\n",
    "\n",
    "여기서 X는 방의 개수를, Y는 주택 가격을 나타냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf90824",
   "metadata": {},
   "source": [
    "### 1. XGBoost 알고리즘 - 최적의 분리 기준 찾기\n",
    "\n",
    "XGBoost는 **그래디언트 부스팅(Gradient Boosting)** 알고리즘의 효율적인 구현체로, 손실 함수의 그래디언트를 최소화하는 방향으로 모델을 순차적으로 개선합니다. 이 문제에서는 첫 번째 트리의 첫 마디에서 최적의 분리 기준을 찾는 과정을 설명하겠습니다.\n",
    "\n",
    "#### 단계 1: 초기 예측값과 그래디언트 계산\n",
    "\n",
    "문제에서 모델의 초기값은 $f_0 = 0.5$로 주어졌습니다. 손실 함수는 제곱 오차(squared error)입니다:\n",
    "$L(y, \\hat{y}) = (y - \\hat{y})^2$\n",
    "\n",
    "그래디언트는 손실 함수를 예측값에 대해 미분한 것입니다:\n",
    "$g_i = \\frac{\\partial L(y_i, f_0(x_i))}{\\partial f_0(x_i)} = -2(y_i - f_0(x_i))$\n",
    "\n",
    "각 데이터 포인트의 그래디언트는 다음과 같습니다:\n",
    "- 관측치 1: $g_1 = -2(1.25 - 0.5) = -1.5$\n",
    "- 관측치 2: $g_2 = -2(1.20 - 0.5) = -1.4$\n",
    "- 관측치 3: $g_3 = -2(1.30 - 0.5) = -1.6$\n",
    "- 관측치 4: $g_4 = -2(1.50 - 0.5) = -2.0$\n",
    "\n",
    "또한, 제곱 오차의 2차 미분(헤시안)은 상수 2입니다:\n",
    "$h_i = \\frac{\\partial^2 L(y_i, f_0(x_i))}{\\partial f_0(x_i)^2} = 2$\n",
    "\n",
    "#### 단계 2: 가능한 분할 지점 탐색\n",
    "\n",
    "X의 가능한 분할 지점은 정렬된 고유값으로 X = 1, 2, 3, 4 입니다. 각 분할 지점에 대해 왼쪽과 오른쪽 노드로 데이터를 나누고 이득(gain)을 계산합니다.\n",
    "\n",
    "#### 단계 3: 최적 분할 선택을 위한 이득(Gain) 계산\n",
    "\n",
    "XGBoost에서 분할의 이득은 다음 공식으로 계산됩니다:\n",
    "\n",
    "$Gain = \\frac{1}{2} \\left[ \\frac{G_L^2}{H_L} + \\frac{G_R^2}{H_R} - \\frac{(G_L + G_R)^2}{H_L + H_R} \\right] - \\gamma$\n",
    "\n",
    "여기서:\n",
    "- $G_L$, $G_R$은 각각 왼쪽/오른쪽 자식 노드의 그래디언트 합\n",
    "- $H_L$, $H_R$은 각각 왼쪽/오른쪽 자식 노드의 헤시안 합\n",
    "- $\\gamma$는 분할 페널티 항으로, 이 문제에서는 $\\lambda = 0$이므로 무시\n",
    "\n",
    "예를 들어 X ≤ 2 분할에 대한 이득 계산:\n",
    "- 왼쪽 노드(X ≤ 2): 관측치 2, 3 → $G_L = -1.4 + (-1.6) = -3.0$, $H_L = 2 + 2 = 4$\n",
    "- 오른쪽 노드(X > 2): 관측치 1, 4 → $G_R = -1.5 + (-2.0) = -3.5$, $H_R = 2 + 2 = 4$\n",
    "- 이득: $Gain = \\frac{1}{2} \\left[ \\frac{(-3.0)^2}{4} + \\frac{(-3.5)^2}{4} - \\frac{(-3.0 - 3.5)^2}{4 + 4} \\right] = \\frac{1}{2} \\left[ 2.25 + 3.0625 - 2.640625 \\right] = 1.336$\n",
    "\n",
    "모든 가능한 분할에 대해 이득을 계산하면:\n",
    "- X ≤ 1: $Gain = 0.0456$\n",
    "- X ≤ 2: $Gain = 0.1337$\n",
    "- X ≤ 3: $Gain = 0.0906$\n",
    "\n",
    "따라서 최적의 분할은 X ≤ 2로, 이득이 가장 큽니다.\n",
    "\n",
    "#### 단계 4: 트리 노드 가중치 계산\n",
    "\n",
    "분할 후, 각 자식 노드의 가중치는 다음 공식으로 계산됩니다:\n",
    "\n",
    "$w = -\\frac{G}{H+\\lambda}$\n",
    "\n",
    "X ≤ 2 분할에 대한 가중치:\n",
    "- 왼쪽 노드: $w_L = -\\frac{-3.0}{4+0} = 0.75$\n",
    "- 오른쪽 노드: $w_R = -\\frac{-3.5}{4+0} = 0.875$\n",
    "\n",
    "이 가중치는 해당 노드에서의 예측값 보정치를 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ada3872",
   "metadata": {},
   "source": [
    "### 2. 결측값 처리 방법\n",
    "\n",
    "XGBoost는 **스파스 인식(sparsity-aware)** 알고리즘으로, 결측값을 효과적으로 처리하는 방법을 내장하고 있습니다. 결측값이 있는 경우, XGBoost는 두 가지 가능성(왼쪽 또는 오른쪽 자식 노드로 보내는 것)을 모두 평가하고 이득이 최대화되는 방향을 선택합니다.\n",
    "\n",
    "#### 단계 1: 결측값 처리를 위한 기본 방향 결정\n",
    "\n",
    "관측치 5와 6에는 X 값이 결측되어 있습니다. 이러한 결측값을 가진 관측치들을 왼쪽 또는 오른쪽 자식 노드 중 어디로 보낼지 결정하기 위해, 각 방향으로 보낼 때의 이득을 계산합니다.\n",
    "\n",
    "결측값 관측치들의 그래디언트는:\n",
    "- 관측치 5: $g_5 = -2(1.40 - 0.5) = -1.8$\n",
    "- 관측치 6: $g_6 = -2(1.30 - 0.5) = -1.6$\n",
    "\n",
    "#### 단계 2: 결측값을 왼쪽 노드로 보낼 때의 이득 계산\n",
    "\n",
    "결측값을 왼쪽 노드(X ≤ 2)로 보내면:\n",
    "- 왼쪽 노드: 관측치 2, 3, 5, 6 → $G_L = -3.0 + (-1.8) + (-1.6) = -6.4$, $H_L = 4 + 2 + 2 = 8$\n",
    "- 오른쪽 노드: 관측치 1, 4 → $G_R = -3.5$, $H_R = 4$\n",
    "- 이득: $Gain_{left} = \\frac{1}{2} \\left[ \\frac{(-6.4)^2}{8} + \\frac{(-3.5)^2}{4} - \\frac{(-6.4 - 3.5)^2}{8 + 4} \\right]$\n",
    "\n",
    "#### 단계 3: 결측값을 오른쪽 노드로 보낼 때의 이득 계산\n",
    "\n",
    "결측값을 오른쪽 노드(X > 2)로 보내면:\n",
    "- 왼쪽 노드: 관측치 2, 3 → $G_L = -3.0$, $H_L = 4$\n",
    "- 오른쪽 노드: 관측치 1, 4, 5, 6 → $G_R = -3.5 + (-1.8) + (-1.6) = -6.9$, $H_R = 4 + 2 + 2 = 8$\n",
    "- 이득: $Gain_{right} = \\frac{1}{2} \\left[ \\frac{(-3.0)^2}{4} + \\frac{(-6.9)^2}{8} - \\frac{(-3.0 - 6.9)^2}{4 + 8} \\right]$\n",
    "\n",
    "#### 단계 4: 최적의 방향 선택\n",
    "\n",
    "두 이득을 비교하여 더 큰 이득을 제공하는 방향으로 결측값을 보냅니다. 계산 결과 $Gain_{right} > Gain_{left}$ 이므로, 관측치 5와 6은 오른쪽 노드(X > 2)로 보내는 것이 최적입니다.\n",
    "\n",
    "이는 XGBoost가 결측값을 효율적으로 처리하는 핵심 메커니즘으로, 모든 가능한 결측값 위치에 대해 최적의 방향을 데이터 기반으로 결정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daadbd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Housing Dataset:\n",
      "   ID    X     Y\n",
      "0   1  3.0  1.25\n",
      "1   2  1.0  1.20\n",
      "2   3  2.0  1.30\n",
      "3   4  4.0  1.50\n",
      "4   5  NaN  1.40\n",
      "5   6  NaN  1.30\n"
     ]
    }
   ],
   "source": [
    "# 주택 가격 데이터셋 출력 (시각화 목적)\n",
    "housing_data = pd.DataFrame({\n",
    "    'ID': [1, 2, 3, 4, 5, 6],\n",
    "    'X': [3, 1, 2, 4, None, None],\n",
    "    'Y': [1.25, 1.20, 1.30, 1.50, 1.40, 1.30]\n",
    "})\n",
    "\n",
    "print(\"Housing Dataset:\")\n",
    "print(housing_data)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
