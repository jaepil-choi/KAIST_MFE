{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dd793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1]\n",
    "\n",
    "########## # 1. Feature Transformation ##########\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "datadict = {\n",
    "    'F1': np.random.rand(100),\n",
    "    'F2': np.random.randint(1, 100, size=100),\n",
    "    'F3': np.random.randn(100),\n",
    "    'F4': np.random.uniform(0, 10, size=100),\n",
    "    'F5': np.random.normal(50, 10, size=100),\n",
    "    'F6': np.random.exponential(5, size=100),\n",
    "}\n",
    "data = pd.DataFrame(datadict)\n",
    "X_train = data[:75]\n",
    "X_test = data[75:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90da9d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2]\n",
    "X_train.info()\n",
    "\n",
    "# <class 'pandas.core.frame.DataFrame'>\n",
    "# RangeIndex: 75 entries, 0 to 74\n",
    "# Data columns (total 6 columns):\n",
    "#  #   Column  Non-Null Count  Dtype  \n",
    "# ---  ------  --------------  -----  \n",
    "#  0   F1      75 non-null     float64\n",
    "#  1   F2      75 non-null     int32  \n",
    "#  2   F3      75 non-null     float64\n",
    "#  3   F4      75 non-null     float64\n",
    "#  4   F5      75 non-null     float64\n",
    "#  5   F6      75 non-null     float64\n",
    "# dtypes: float64(5), int32(1)\n",
    "# memory usage: 3.4 KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6975ccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [3]\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler1 = StandardScaler() # 표준화 변환\n",
    "scaler1.fit(X_train) # scaler fitting은 train 데이터만 수행\n",
    "\n",
    "X_train1 = scaler1.transform(X_train)\n",
    "X_test1 = scaler1.transform(X_test) # test set transform도 train에서 학습한 scaler 사용\n",
    "# information leackage 방지\n",
    "\n",
    "print(X_train1.mean(axis=0))\n",
    "print(X_test1.mean(axis=0))\n",
    "print(X_train1.std(axis=0))\n",
    "print(X_test1.std(axis=0))\n",
    "\n",
    "# [ 1.03620816e-17  1.06951485e-16 -5.92118946e-18 -5.47710025e-17\n",
    "#   2.87177689e-16  1.22864681e-16]\n",
    "# [-0.0394831  -0.21916239 -0.05676997  0.08733307 -0.23170978  0.6437487 ]\n",
    "# => test set, train set 에서 학습된 scaler로 전처리했지만 마찬가지로 거의 0\n",
    "\n",
    "# [1. 1. 1. 1. 1. 1.]\n",
    "# [0.9513569  0.88798781 1.19877918 0.87206489 1.13146366 1.44152395]\n",
    "# => 같은 이유로 거의 1. train과 test의 population이 아주 다르지는 않다는 것. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1cf604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler2 = MinMaxScaler()\n",
    "scaler2.fit(X_train)\n",
    "\n",
    "X_train2 = scaler2.transform(X_train)\n",
    "X_test2 = scaler2.transform(X_test)\n",
    "\n",
    "print(X_train2.max(axis=0))\n",
    "print(X_test2.max(axis=0))\n",
    "print(X_train2.min(axis=0))\n",
    "print(X_test2.min(axis=0))\n",
    "\n",
    "# [1. 1. 1. 1. 1. 1.]\n",
    "# [0.94294425 0.98958333 0.96394344 0.96091117 0.98759588 1.06503   ]\n",
    "# [0. 0. 0. 0. 0. 0.]\n",
    "# [ 0.02885686  0.         -0.00666648  0.03262773 -0.04797744 -0.00045541]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d66dc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Power transformation을 위한 데이터 생성\n",
    "\n",
    "# [5]\n",
    "datadict2 = {\n",
    "    'F1': np.random.gamma(2, 2, 1000),\n",
    "    'F2': np.random.normal(0, 1, 1000),\n",
    "    'F3': np.random.uniform(0, 1, 1000)\n",
    "}\n",
    "data2 = pd.DataFrame(datadict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ebab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [6]\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "pt = PowerTransformer(method='yeo-johnson') # box-cox는 negative에서 불가해서 이걸 씀. \n",
    "pt.fit(data2)\n",
    "data2tr = pt.transform(data2)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.hist(data2['F1'], bins=30, color='yellow', alpha=0.5) # 이게 오리지날날\n",
    "plt.hist(data2tr[:, 0], bins=30, color='blue', alpha=0.5) # 이게 변환한거\n",
    "# alpha=0.5 줘서 겹치는 부분 보이게하는 센스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e83188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [7]\n",
    "data2_01 = data2.quantile(0.01)\n",
    "data2_99 = data2.quantile(0.99)\n",
    "data2tr2 = data2.clip(lower=data2_01, upper=data2_99, axis=1) # clip으로 outlier 날리고\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.hist(data2['F1'], bins=30, color='yellow', alpha=0.5) \n",
    "plt.hist(data2tr2['F1'], bins=30, color='blue', alpha=0.5)\n",
    "# clip하면 그 부분 벽 막힌 것 처럼 팍 튀는거 보임 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3da3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [8]\n",
    "bin_bdr = [0, 2.5, 5.0, 7.5, float('inf')] # 직접 range specify 한 것. \n",
    "F1_bin = pd.cut(data2['F1'], bins=bin_bdr, labels=False) # number --> category\n",
    "# bins에 정수 넣으면 등간격으로 만들어 줌\n",
    "F1_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ef2bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [9]\n",
    "F2_rank = data2['F2'].rank()\n",
    "F2_normalized_rank = F2_rank / data2.shape[0]\n",
    "F2_normalized_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4474539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [10]\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "# 데이터를 cdf에서 각 누적 %에 매핑되는 값으로 변환해줌. \n",
    "# 이론상 이런 분포를 따라야 할 때 이런식으로 접근 가능. \n",
    "\n",
    "qt = QuantileTransformer(n_quantiles=100, output_distribution='normal')\n",
    "qt.fit(data2)\n",
    "data2tr3 = qt.transform(data2)\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.hist(data2['F1'], bins=30, color='yellow', alpha=0.5)\n",
    "plt.hist(data2tr3[:, 0], bins=30, color='blue', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0bd94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [11]\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "# label encoder는 정수값으로 변환해줌. \n",
    "city = {\n",
    "    'city': ['Seoul', 'Tokyo', 'Paris', 'Paris', 'Tokyo', 'Seoul',\n",
    "             'London', 'Madrid', 'Seoul', 'Beijing', 'London', 'Paris']\n",
    "}\n",
    "citydf = pd.DataFrame(city)\n",
    "label_encoder = LabelEncoder()\n",
    "citydf['city_encoded'] = label_encoder.fit_transform(citydf['city']) # 이게 표준 .\n",
    "citydf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becdddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [12]\n",
    "\n",
    "# onehot encoder는 각각의 값에 대해 0, 1로 변환해줌.\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False) # False 일 때 dense matrix로 반환.\n",
    "one_hot_encoded = one_hot_encoder.fit_transform(citydf[['city']])\n",
    "pd.DataFrame(one_hot_encoded, columns=label_encoder.classes_)\n",
    "\n",
    "# dense로 하면 column에 label을 넣어 0 0 0 0 1 0 이런 식으로 row를 가지게 해 더 efficient 함\n",
    "# 목적이 기준 테이블에 column으로 붙여야하는게 아니면 이렇게 만들어 쓰는 것도 가능. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86f67d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [13]\n",
    "freq = citydf['city'].value_counts()\n",
    "freq\n",
    "\n",
    "# city\n",
    "# Seoul      3\n",
    "# Paris      3\n",
    "# Tokyo      2\n",
    "# London     2\n",
    "# Madrid     1\n",
    "# Beijing    1\n",
    "# Name: count, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b8910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [14]\n",
    "citydf['city'].map(freq) # series object를 mapper로 써서 dict처럼 mapping 해줌\n",
    "\n",
    "# 0     3\n",
    "# 1     2\n",
    "# 2     3\n",
    "# 3     3\n",
    "# 4     2\n",
    "# 5     3\n",
    "# 6     2\n",
    "# 7     1\n",
    "# 8     3\n",
    "# 9     1\n",
    "# 10    2\n",
    "# 11    3\n",
    "# Name: city, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052788b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [15]\n",
    "tgdict = {\n",
    "    'F1': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank'],\n",
    "    'F2': ['Female', 'Male', 'Male', 'Male', 'Female', 'Male'],\n",
    "    'Y': [20, 50, 60, 80, 30, 50]\n",
    "}\n",
    "tgdf = pd.DataFrame(tgdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441c6ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [16]\n",
    "tg_mean = tgdf.groupby('F2')['Y'].mean()\n",
    "tg_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07619ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [17]\n",
    "tgdf['F3'] = tgdf['F2'].map(tg_mean) # 마찬가지로 series를 mapper로 쓰는 것. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6755e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [18]\n",
    "# Alternative: tgdf['F3'] = tgdf.groupby('F2')['Y'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48debbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [19]\n",
    "tgdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b783207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [20]\n",
    "\n",
    "########## # 2. Feature Selection ##########\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20,\n",
    "                           n_informative=8, n_redundant=12, random_state=1)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b948a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [21]\n",
    "\n",
    "########## ## 2.1 Filter 방식 ##########\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "# ============ SelectKBest parameter =================\n",
    "# score_func\n",
    "#   : f_regression, mutual_info_regression # 여기의 f는 F-stats\n",
    "#   : chi2, f_classif, mutual_info_classif \n",
    "# # chi2 는 독립성 검정통계량\n",
    "# # 여기의 f는 anova F-stats\n",
    "# # mutual_info는 correlation이랑 비스무리한 것. \n",
    "# # corr은 linear 한 관계를 봄.\n",
    "# # mutual_info는 독립인지 연관되어있는지 joint distribution과 marginal distribution을 비교해봄. \n",
    "# # marginal joint dist의 곱이 joint dist와 같으면 두 변수는 독립이라고 볼 수 있음.\n",
    "# # 이게 얼마나 다른지를 본다. 클수록 연관성 크고 작을수록 연관성 작고(즉 negative corr)\n",
    "# k, percentile :\n",
    "\n",
    "from sklearn.feature_selection import f_classif\n",
    "skb = SelectKBest(f_classif)\n",
    "skbfit = skb.fit(X, y)\n",
    "dfscores = pd.DataFrame(skbfit.scores_, columns=['score'])\n",
    "dfscores.sort_values('score', ascending=False)\n",
    "# score을 볼 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f75747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [22]\n",
    "skb = SelectKBest(f_classif, k=5)\n",
    "skbfit = skb.fit(X, y)\n",
    "skb.get_support() # feature selection 하는 애들이 다 쓸 수 있는 method \n",
    "# 변수가 선택되었는지 여부를 True False로 list로 반환. \n",
    "\n",
    "# array([False, False, False, False,  True, False, False, False, False,\n",
    "#        False,  True, False, False,  True, False,  True, False,  True,\n",
    "#        False, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9918957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [23]\n",
    "skb.transform(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f906a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [24]\n",
    "\n",
    "########## ## 2.2 Wrapper 방식 ##########\n",
    "\n",
    "## Wrapper 방식. \n",
    "# regression에서 walk forward 같이 하나씩 넣어보며 하는 방식\n",
    "\n",
    "# 참고: wrapper vs embedded\n",
    "# wrapper: feature selection을 search problem으로 본다. 하나씩 늘려가고 줄여가며 eval metric은 따로 두고 고른다\n",
    "# embedded: feature selection을 model에 포함시킨다. feature selection을 모델의 결과로써 학습시킨다. \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE # Recursive Feature Elimination\n",
    "\n",
    "# ============ RFE  parameter =================\n",
    "# estimator : coef_ 와  feature_importances_ 가 산출가능한 sklearn 지도학습 알고리즘 쓰면 됨. (트리같은거)\n",
    "# 절차: coef 절대값 작은 것 == 도움 안되는 것 제거 --> 또 모델 돌리고 --> 반복\n",
    "# n_features_to_select : 선택할 특성수. default는 전체 특성변수의 절반. (10개면 5개)\n",
    "# step : 매 단계 제거되는 특성 수를 정할 수 있다. default  1. 엄청 많으면 더 늘릴수도. \n",
    "\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, n_features_to_select=8, verbose=1)\n",
    "rfefit = rfe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d7f8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [25]\n",
    "rfefit.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a855bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [26]\n",
    "rfefit.transform(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fccc15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [27]\n",
    "\n",
    "# 하지만, 대체 어디까지 select 해야하지? 몇 개의 특성을 남겨야 하지? \n",
    "# 그 방법이 이것. \n",
    "# k-fold로 cross validation을 하면서 evaluation metric으로 비교. \n",
    "# evaluation은 k-fold에서의 validation set으로 하는 것. \n",
    "\n",
    "# feature 를 20개에서 출발하면 여기서 k-fold, 19개면 또 여기서 k-fold... \n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "# 당연히 coef_, feature_importances_ 가 있는 estimator(지도학습 모델)을 써야 함.\n",
    "rfecv = RFECV(model, cv=5)\n",
    "rfecvfit = rfecv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aeff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [28]\n",
    "rfecvfit.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905432b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [29]\n",
    "rfecvfit.transform(X).shape\n",
    "\n",
    "## 하지만 자동으로 하게 두면 그렇게 잘 generalizae 하진 또 않음... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb108332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [30]\n",
    "\n",
    "# RFECV의 제일 큰 단점은 coef_, feature_importances_ 가 없는 알고리즘에는 적용이 불가능하다는 것.\n",
    "# 최근에 이런게 만들어 짐. SequentialFeatureSelector\n",
    "# coef_, feature_imporatnce_ 없는 sklearn의 임의의 지도학습 모델을 모두 사용 가능함. \n",
    "\n",
    "# 어떻게 선택하는가? \n",
    "\n",
    "# 내부 원리를 간단히 설명하면, \n",
    "\n",
    "# 1. forward면 상수항에서 출발해서\n",
    "# 2. 기존 방식처럼 cross validation (CV) 쓰며 iterate 하는 것은 같은데\n",
    "# 3. k-fold 에서 하나 추가하는 것이 제일 성능을 많이 올렸다면 그것을 선택\n",
    "# 4. 이러면 진짜 걍 cv만 하면서 성능만 보는거니까 모델 dependent 하지 않음. \n",
    "\n",
    "# 단점으로, n_features_to_select를 무조건 지정해야 함. \n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "# ============ SequentialFeatureSelector  parameter =================\n",
    "# n_features_to_select :\n",
    "# direction : 'forward'    'backward'\n",
    "# scoring :. None  estimator score     \n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "sfs = SequentialFeatureSelector(knn, n_features_to_select=3)\n",
    "sfsfit = sfs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8631546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [31]\n",
    "sfsfit.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0313a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [32]\n",
    "sfsfit.transform(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd511669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [33]\n",
    "# Note: Shell commands like ! pip install Boruta should be run externally; thus, it is commented out.\n",
    "! pip install Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5bcd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [34]\n",
    "from boruta import BorutaPy\n",
    "# random forest를 여러 차례 반복해서 선택하는 것. --> wrapper 방식\n",
    "\n",
    "# n_estimators : 각 iteration 마다 포함되는 estimator의 수 (RF의 tree 갯수)\n",
    "# max_iter : 최대 iteration 회수 (강의안에서의 =m) \n",
    "# alpha : 최종 가설검정에서 H0: \"random하게 hit 했다.\" 를 기각할 수 있어야 함. 이에 대한 threshold\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=123, max_depth=5)\n",
    "brtfs = BorutaPy(rf, n_estimators=7, max_iter=15, verbose=1, random_state=123, alpha=0.01)\n",
    "\n",
    "# As of 2025/03/12, boruta는 예전 numpy에 맞게 코딩이 되어있어서 이것을 이렇게 새로 할당을 해줘야 boruta가 돌아감.\n",
    "np.int = np.int64\n",
    "np.float = np.float64\n",
    "np.bool = np.bool_\n",
    "\n",
    "brtfs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52be93c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [35]\n",
    "brtfs.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94605443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [36]\n",
    "brtfs.transform(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f1fdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [37]\n",
    "\n",
    "########## ## 2.3 Embedded 방식 ##########\n",
    "# parametric 방식에선 lasso 같은게 fitting 과정에서 feature를 drop 하니까 이것이 embedded\n",
    "# tree 기반 모델은 feature importace를 알아서 model fitting 과정에서 사용하니까 이것도 embedded\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "selector = SelectFromModel(estimator=RandomForestClassifier())\n",
    "selector.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1492e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [38]\n",
    "selector.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014bcc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [39]\n",
    "selector.transform(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f37e3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [40]\n",
    "\n",
    "# 3. Under/Over Sampling\n",
    "\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
    "                           n_clusters_per_class=1, weights=[0.99], random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3322b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [41]\n",
    "pd.Series(y).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f775df89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [42]\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(X[y==0, 0], X[y==0, 1], 'b+', label=\"class 0\")\n",
    "plt.plot(X[y==1, 0], X[y==1, 1], 'r*', label=\"class 1\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1795282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [43]\n",
    "\n",
    "## CNN (Condensed Nearest Neighbour)\n",
    "# \n",
    "# 1NN을 사용해서 majority class를 undersample하는 방식.\n",
    "# 시간 많이 걸림. \n",
    "\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "undersample1 = CondensedNearestNeighbour(n_neighbors=1)\n",
    "X1, y1 = undersample1.fit_resample(X, y)\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(X1[y1==0, 0], X1[y1==0, 1], 'b+', label=\"class 0\")\n",
    "plt.plot(X1[y1==1, 0], X1[y1==1, 1], 'r*', label=\"class 1\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b126b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [44]\n",
    "\n",
    "## Tomek Links\n",
    "\n",
    "# major이 거의 줄지 않음. 경계선만 깔끔하게 해주는거라. \n",
    "\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "undersample2 = TomekLinks()\n",
    "X2, y2 = undersample2.fit_resample(X, y)\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(X2[y2==0, 0], X2[y2==0, 1], 'b+', label=\"class 0\")\n",
    "plt.plot(X2[y2==1, 0], X2[y2==1, 1], 'r*', label=\"class 1\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920a993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [45]\n",
    "\n",
    "# SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "# option으로 k_neighbors 있음.\n",
    "# 돌리면 major, minior 갯수 같게 맞춰줌. \n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "oversample1 = SMOTE()\n",
    "OX1, Oy1 = oversample1.fit_resample(X, y)\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(OX1[Oy1==0, 0], OX1[Oy1==0, 1], 'b+', label=\"class 0\")\n",
    "plt.plot(OX1[Oy1==1, 0], OX1[Oy1==1, 1], 'r*', label=\"class 1\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979760b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [46]\n",
    "\n",
    "# ADASYN (Adaptive Synthetic Sampling)\n",
    "# SMOTE의 변형.\n",
    "# major과 가까운 곳에 minor를 많이 생성. \n",
    "\n",
    "from imblearn.over_sampling import ADASYN\n",
    "oversample2 = ADASYN()\n",
    "\n",
    "OX2, Oy2 = oversample2.fit_resample(X, y)\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(OX2[Oy2==0, 0], OX2[Oy2==0, 1], 'b+', label=\"class 0\")\n",
    "plt.plot(OX2[Oy2==1, 0], OX2[Oy2==1, 1], 'r*', label=\"class 1\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
