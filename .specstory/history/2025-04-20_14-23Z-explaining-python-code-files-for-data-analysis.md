<!-- Generated by SpecStory -->

# Explaining Python code files for data analysis (2025-04-20 14:23Z)

_**User**_

Use instructions written in #file:py-files-teacher-o4mh-web.mdc . 

As you can read from the file, you should explain code files that I mention one by one, as if you're a teacher. 

You should explain #file:Ch1_(1).py , #file:Ch1_(2).py , #file:Ch2.py , #file:Ch3_notebook.py one by one. 

---

_**Assistant**_

# Review of Python Files

## 1. `Ch1_(1).py`
**Timestamp:** 2025-03-11

### 1. What the code does
This file demonstrates various techniques for handling missing values and detecting outliers in data, which are fundamental data preprocessing steps. The code uses a mix of pandas and scikit-learn methods to showcase different approaches.

### 2. Keywords / Topics
- Missing value handling
- Data imputation
- Outlier detection
- IQR method
- Z-score standardization
- Linear regression imputation
- KNN imputation
- Local Outlier Factor
- Isolation Forest

### 3. Table of Contents
- Data Creation and Exploration (lines 1-14)
- Missing Value Handling with Deletion (lines 16-27)
- Missing Value Handling with Statistics (lines 29-39)
- Missing Value Handling with Prediction (lines 41-70)
- Outlier Detection with Statistical Methods (lines 72-97)
- Outlier Detection with Machine Learning (lines 99-139)

### 4. Detailed Section-by-Section Explanation

#### Data Creation and Exploration (lines 1-14)
- **a) Key concepts:** Creating a sample dataset with missing values, inspecting data structure
- **b) Implementation details:** Creates a dictionary with three features where `f2` has missing values, converts it to DataFrame, and inspects missing value proportion with `isna().mean()`
- **c) Insights from comments:** Each code block is labeled numerically [1], [2], etc., suggestive of a teaching example with progressive steps

#### Missing Value Handling with Deletion (lines 16-27)
- **a) Key concepts:** Row removal strategies for missing values
- **b) Implementation details:** Demonstrates two approaches:
  - `dropna()` to remove any row with missing values
  - `dropna(subset=['f3'])` to remove only rows where specific columns have missing values
- **c) Insights from comments:** Simple approaches that work well when missing data proportion is small

#### Missing Value Handling with Statistics (lines 29-39)
- **a) Key concepts:** Imputation using statistical measures
- **b) Implementation details:** 
  - First shows filling with a constant value (-999)
  - Then demonstrates filling with column means
  - Finally shows a more sophisticated group-based mean imputation using `groupby` and `transform`
- **c) Insights from comments:** Group-based imputation preserves more information about the relationships in data

#### Missing Value Handling with Prediction (lines 41-70)
- **a) Key concepts:** Using predictive models for imputation
- **b) Implementation details:** 
  - Linear regression model trained on complete data to predict missing values
  - KNN imputer that uses nearby observations to fill in missing values
- **c) Insights from comments:** Numerical encoding is necessary for categorical values (`f3`) before using KNN imputation

#### Outlier Detection with Statistical Methods (lines 72-97)
- **a) Key concepts:** Traditional statistical approaches to identify outliers
- **b) Implementation details:** 
  - IQR method using quartiles to define a valid range (Q1-1.5*IQR to Q3+1.5*IQR)
  - Z-score standardization to identify values deviating more than 3 standard deviations
- **c) Insights from comments:** Both methods create boolean masks for outlier identification

#### Outlier Detection with Machine Learning (lines 99-139)
- **a) Key concepts:** Modern ML approaches for outlier detection
- **b) Implementation details:** 
  - Creates synthetic dataset with clear outliers for visualization
  - Implements Local Outlier Factor (LOF) which finds anomalies based on local density deviations
  - Implements Isolation Forest which isolates outliers using random splits
- **c) Insights from comments:** Comments explain key parameters for Isolation Forest such as contamination (expected outlier proportion), n_estimators (number of trees), and sampling parameters

## 2. `Ch1_(2).py`
**Timestamp:** 2025-03-11

### 1. What the code does
This file explores feature transformation and feature selection methods, which are essential for improving model performance. The code includes techniques for scaling, transformation, encoding categorical variables, and handling imbalanced classes.

### 2. Keywords / Topics
- Feature transformation
- Standardization
- Min-Max scaling
- Power transformation
- Binning
- Categorical encoding
- Feature selection
- Filter methods
- Wrapper methods
- Embedded methods
- Imbalanced data handling
- Undersampling
- Oversampling

### 3. Table of Contents
- Feature Transformation Setup (lines 1-32)
- Numerical Feature Scaling (lines 34-62)
- Power Transformation Techniques (lines 66-118)
- Categorical Feature Encoding (lines 120-181)
- Feature Selection Introduction (lines 183-189)
- Filter Methods for Feature Selection (lines 191-212)
- Wrapper Methods for Feature Selection (lines 214-281)
- Embedded Method for Feature Selection (lines 283-292)
- Handling Imbalanced Data (lines 294-353)

### 4. Detailed Section-by-Section Explanation

#### Feature Transformation Setup (lines 1-32)
- **a) Key concepts:** Creating a synthetic dataset for transformation demonstrations
- **b) Implementation details:** Generates random data with various distributions (uniform, normal, etc.) and splits into train/test sets
- **c) Insights from comments:** Marked with a clear header "Feature Transformation" to indicate the beginning of this topic

#### Numerical Feature Scaling (lines 34-62)
- **a) Key concepts:** Standardization and normalization techniques
- **b) Implementation details:** 
  - `StandardScaler` transforms features to have mean=0 and std=1
  - `MinMaxScaler` rescales features to a specific range (typically [0, 1])
- **c) Insights from comments:** Important note about fitting scalers on training data only to prevent information leakage

#### Power Transformation Techniques (lines 66-118)
- **a) Key concepts:** Transformations to make data more normally distributed
- **b) Implementation details:** 
  - Demonstrates `PowerTransformer` with Yeo-Johnson method (works with negative values)
  - Shows clipping outliers at percentiles
  - Illustrates binning continuous variables into categories
  - Explains ranking transformation
  - Shows `QuantileTransformer` for distribution mapping
- **c) Insights from comments:** Visualizations compare original vs. transformed distributions to show the effect

#### Categorical Feature Encoding (lines 120-181)
- **a) Key concepts:** Converting categorical variables to numerical representations
- **b) Implementation details:** 
  - `LabelEncoder` for ordinal encoding
  - `OneHotEncoder` for creating binary columns
  - Frequency encoding maps categories to their occurrence counts
  - Target mean encoding replaces categories with target statistics
- **c) Insights from comments:** Notes that "dense matrix" representation is more efficient when not needing to join with the base table

#### Feature Selection Introduction (lines 183-189)
- **a) Key concepts:** Creating synthetic data for feature selection demonstrations
- **b) Implementation details:** Uses `make_classification` to create a dataset with known informative and redundant features
- **c) Insights from comments:** Clear header indicating the start of Feature Selection section

#### Filter Methods for Feature Selection (lines 191-212)
- **a) Key concepts:** Statistical tests to select features
- **b) Implementation details:** Uses `SelectKBest` with `f_classif` (ANOVA F-test) to rank and select features
- **c) Insights from comments:** Detailed explanation of score functions including F-tests, chi-squared, and mutual information

#### Wrapper Methods for Feature Selection (lines 214-281)
- **a) Key concepts:** Search algorithms that use model performance as selection criteria
- **b) Implementation details:** 
  - `RFE` (Recursive Feature Elimination) removes features with lowest importance
  - `RFECV` combines RFE with cross-validation for optimal feature count
  - `SequentialFeatureSelector` adds features incrementally
  - `BorutaPy` uses random forest and statistical testing
- **c) Insights from comments:** Explains that wrapper methods treat feature selection as a search problem while embedded methods incorporate it into model training

#### Embedded Method for Feature Selection (lines 283-292)
- **a) Key concepts:** Model-based feature importance
- **b) Implementation details:** Uses `SelectFromModel` with RandomForest to select important features
- **c) Insights from comments:** Mentions both parametric (Lasso) and tree-based models as common embedded approaches

#### Handling Imbalanced Data (lines 294-353)
- **a) Key concepts:** Techniques for dealing with class imbalance
- **b) Implementation details:** 
  - `CondensedNearestNeighbour` for undersampling
  - `TomekLinks` for cleaning decision boundaries
  - `SMOTE` for generating synthetic minority examples
  - `ADASYN` for adaptive synthetic sampling
- **c) Insights from comments:** Notes that Tomek Links mainly clean the decision boundary rather than substantially reducing majority class

## 3. `Ch2.py`
**Timestamp:** 2025-03-13

### 1. What the code does
This file demonstrates clustering techniques, particularly DBSCAN and KMeans algorithms, along with methods for evaluating cluster quality. It works with real data about individual investment preferences and visualizes the results.

### 2. Keywords / Topics
- Clustering algorithms
- DBSCAN
- KMeans
- k-nearest neighbors
- Silhouette score
- t-SNE
- Dimensionality reduction
- Cluster evaluation
- Elbow method

### 3. Table of Contents
- Data Import and Preprocessing (lines 1-42)
- DBSCAN Parameter Selection (lines 44-68)
- DBSCAN Clustering Implementation (lines 70-85)
- Cluster Visualization (lines 87-114)
- KMeans Clustering and Evaluation (lines 116-133)
- Silhouette Score Analysis (lines 135-175)

### 4. Detailed Section-by-Section Explanation

#### Data Import and Preprocessing (lines 1-42)
- **a) Key concepts:** Loading real-world data and preparing it for clustering
- **b) Implementation details:** 
  - Loads an Excel file containing investment preference data
  - Removes the ID column as it's not useful for clustering
  - Standardizes features to have zero mean and unit variance
- **c) Insights from comments:** Describes dataset variables (AGE, EDUC, MARRIED, etc.) and mentions that ordinal categorical variables can help clustering while nominal ones need encoding or removal

#### DBSCAN Parameter Selection (lines 44-68)
- **a) Key concepts:** Determining appropriate epsilon and min_samples parameters for DBSCAN
- **b) Implementation details:** 
  - Uses k-nearest neighbors to compute distances
  - Plots sorted distances for the k-th nearest neighbor to identify the "elbow point"
  - Compares k-distance graphs for different k values (4 to 8)
- **c) Insights from comments:** The elbow point helps identify a suitable epsilon value, with points before the elbow becoming core points and those after potentially becoming border/noise points

#### DBSCAN Clustering Implementation (lines 70-85)
- **a) Key concepts:** Applying DBSCAN with selected parameters
- **b) Implementation details:** 
  - Runs DBSCAN with eps=2.0 and min_samples=4
  - Adds cluster labels to the data
  - Calculates means of features for each cluster
- **c) Insights from comments:** Notes that noise points are labeled as -1

#### Cluster Visualization (lines 87-114)
- **a) Key concepts:** Understanding cluster characteristics and visualizing high-dimensional data
- **b) Implementation details:** 
  - Creates bar charts showing mean feature values for each cluster
  - Uses t-SNE to reduce dimensionality to 2D for visualization
  - Creates a scatter plot of clusters in the 2D t-SNE space
- **c) Insights from comments:** Suggests that alternative visualizations like hexagonal radar charts might be better for comparing cluster profiles

#### KMeans Clustering and Evaluation (lines 116-133)
- **a) Key concepts:** Using KMeans as an alternative clustering approach and determining optimal cluster count
- **b) Implementation details:** 
  - Calculates Sum of Squared Errors (SSE) for KMeans with different cluster counts
  - Plots the SSE "elbow curve" to help select optimal cluster count
  - Implements KMeans with 9 clusters
- **c) Insights from comments:** Notes that sometimes the elbow point is ambiguous, and in those cases, t-SNE visualization can help choose cluster count

#### Silhouette Score Analysis (lines 135-175)
- **a) Key concepts:** Using silhouette scores to evaluate cluster quality
- **b) Implementation details:** 
  - Defines a function to plot detailed silhouette scores for all clusters
  - Applies the function to both DBSCAN and KMeans clustering results
- **c) Insights from comments:** Higher silhouette scores indicate better-defined clusters, and visualizing scores can reveal which clusters are well-separated and which might be overlapping

## 4. `Ch3_notebook.py`
**Timestamp:** 2025-04-09

### 1. What the code does
This notebook demonstrates the application of XGBoost for classification using Lending Club loan data. It covers the entire machine learning workflow from data preparation through model training, hyperparameter tuning, and model interpretation using SHAP values.

### 2. Keywords / Topics
- XGBoost
- Classification
- Hyperparameter tuning
- Feature importance
- Model evaluation
- Early stopping
- SHAP values
- Model interpretation
- RandomizedSearchCV
- Confusion matrix

### 3. Table of Contents
- Introduction and Data Loading (lines 1-21)
- Data Preparation (lines 23-34)
- XGBoost Parameter Overview (lines 36-144)
- Model Building and Tuning (lines 146-181)
- Final Model Training (lines 183-212)
- Model Analysis and Evaluation (lines 214-235)
- Model Interpretation with SHAP (lines 237-266)

### 4. Detailed Section-by-Section Explanation

#### Introduction and Data Loading (lines 1-21)
- **a) Key concepts:** Setting up the environment and loading real-world financial data
- **b) Implementation details:** 
  - Imports necessary libraries (pandas, numpy, matplotlib, sklearn)
  - Loads the Lending Club loan data from a CSV file
- **c) Insights from comments:** Explains that the dataset contains loan information from 2007-2017 with the target variable "charged_off" indicating loan default

#### Data Preparation (lines 23-34)
- **a) Key concepts:** Preparing data for machine learning
- **b) Implementation details:** 
  - Separates features (X) and target variable (Y)
  - Creates train/validation/test splits using stratified sampling
  - Checks class balance in training data
- **c) Insights from comments:** Uses a two-stage split approach to create separate validation and test sets

#### XGBoost Parameter Overview (lines 36-144)
- **a) Key concepts:** Understanding the many parameters of XGBoost
- **b) Implementation details:** This section contains extensive comments explaining:
  - General parameters (booster, verbosity, n_jobs)
  - Booster parameters (n_estimators, learning_rate, min_child_weight, etc.)
  - Learning task parameters (objective, eval_metric, early_stopping)
- **c) Insights from comments:** Very detailed explanations of each parameter's purpose, default values, and how they affect the model's behavior

#### Model Building and Tuning (lines 146-181)
- **a) Key concepts:** Creating an initial model and optimizing hyperparameters
- **b) Implementation details:** 
  - Sets up initial XGBoost classifier with class weight balancing
  - Defines parameter grid for tuning
  - Uses RandomizedSearchCV for efficient hyperparameter search
  - Analyzes search results to find best parameters
- **c) Insights from comments:** Uses scale_pos_weight to handle class imbalance by setting it to the ratio of negative to positive samples

#### Final Model Training (lines 183-212)
- **a) Key concepts:** Training the final model with optimal parameters
- **b) Implementation details:** 
  - Creates model with best parameters from search
  - Uses early stopping to prevent overfitting
  - Analyzes evaluation metrics from training
  - Visualizes learning curves for training and validation
- **c) Insights from comments:** Uses a large n_estimators with early stopping to automatically find the optimal number of trees

#### Model Analysis and Evaluation (lines 214-235)
- **a) Key concepts:** Understanding model behavior and performance
- **b) Implementation details:** 
  - Visualizes feature importances
  - Makes predictions on test data
  - Evaluates using confusion matrix and accuracy
- **c) Insights from comments:** Feature importance helps identify which factors most strongly influence loan default prediction

#### Model Interpretation with SHAP (lines 237-266)
- **a) Key concepts:** Explaining individual predictions and global model behavior
- **b) Implementation details:** 
  - Sets up a sample instance for explanation
  - Calculates SHAP values for training data
  - Creates force plot for individual prediction explanation
  - Creates summary plot showing feature impacts across the dataset
- **c) Insights from comments:** SHAP values provide transparent explanations of how each feature contributes to predictions, which is crucial in financial applications

---

_**User**_

Look at the code in #file:final_eda.py . There's a problem with broken Korean characters. You should search the web to see how to make Korean characters not broken when plotting. 

---

_**Assistant**_

Okay, I can help with that. Broken Korean characters in matplotlib plots on Windows usually happen because the default font doesn't support Korean glyphs.

To fix this, I will add code to set the default font for `matplotlib` to 'Malgun Gothic', which is typically available on Windows and supports Korean. I'll also handle a common issue where changing the font breaks the minus sign display.


````
\
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from scipy import stats
import platform # Import platform module

# 시각화를 위한 설정
# 운영체제별 글꼴 설정
if platform.system() == 'Windows':
    plt.rc('font', family='Malgun Gothic')
elif platform.system() == 'Darwin': # macOS
    plt.rc('font', family='AppleGothic')
else: # Linux
    # Linux에서는 나눔고딕과 같은 폰트 설치 필요
    # plt.rc('font', family='NanumGothic')
    pass

# 마이너스 폰트 깨짐 방지
plt.rc('axes', unicode_minus=False)

plt.rcParams['font.size'] = 12
# ...existing code...

````
I've added code to `final_eda.py` to set the font correctly for displaying Korean characters in plots, depending on your operating system. It will use 'Malgun Gothic' on Windows. I also included a fix for the minus sign display.

---

_**User**_

Let's replace all Korean in plots to English for now. Your solution didn't work and I don't have time to troubleshoot this. 

---

_**Assistant**_

```python
# %% [markdown]
# # BAF660 빅데이터와 금융자료분석 - 기말대체과제 EDA
#
# 이 노트북에서는 포르투갈 은행의 정기예금 프로모션 전화 데이터(`prob1_bank.csv`)에 대한 
# 탐색적 데이터 분석(EDA)을 수행합니다.

# %% [markdown]
# ## 1. 데이터 로드 및 기본 정보 확인

# %%
# 필요한 라이브러리 임포트
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from scipy import stats
# import platform # Import platform module # Removed platform import

# 시각화를 위한 설정
# # 운영체제별 글꼴 설정 # Removed font setting block
# if platform.system() == 'Windows':
#     plt.rc('font', family='Malgun Gothic')
# elif platform.system() == 'Darwin': # macOS
#     plt.rc('font', family='AppleGothic')
# else: # Linux
#     # Linux에서는 나눔고딕과 같은 폰트 설치 필요
#     # plt.rc('font', family='NanumGothic')
#     pass

# 마이너스 폰트 깨짐 방지
plt.rc('axes', unicode_minus=False)

plt.rcParams['font.size'] = 12
# ...existing code...
print("데이터셋 형태:", bank_data.shape)
print("\n데이터셋 정보:")
print(bank_data.info())
# ...existing code...
print("\n데이터셋 상위 5개 행:")
print(bank_data.head())

# %% [markdown]
# ## 2. 결측치 분석
#
# Ch1_(1).py에서 배운 결측치 처리 방법을 활용하여 분석합니다.

# %%
# 결측치 확인
missing_values = bank_data.isnull().sum()
missing_percent = bank_data.isnull().mean() * 100

missing_df = pd.DataFrame({
    'Missing Count': missing_values, # Changed from '결측치 수'
    'Missing Ratio (%)': missing_percent # Changed from '결측치 비율(%)'
})

print("Missing Value Analysis:") # Changed from '결측치 분석:'
print(missing_df[missing_df['Missing Count'] > 0]) # Changed from '결측치 수'

if missing_df['Missing Count'].sum() == 0: # Changed from '결측치 수'
    print("No missing values.") # Changed from '결측치가 없습니다.'

# %% [markdown]
# ## 3. 데이터 분포 탐색

# %% [markdown]
# ### 3.1 범주형 변수 분석

# %%
# 범주형 변수와 수치형 변수 분리
categorical_cols = bank_data.select_dtypes(include=['object']).columns.tolist()
numerical_cols = bank_data.select_dtypes(include=['int64', 'float64']).columns.tolist()

print("Categorical Columns:", categorical_cols) # Changed from '범주형 변수:'
print("Numerical Columns:", numerical_cols) # Changed from '수치형 변수:'

# %%
# 범주형 변수 분포 시각화
plt.figure(figsize=(15, len(categorical_cols)*4))

for i, col in enumerate(categorical_cols):
    plt.subplot(len(categorical_cols), 2, 2*i+1)
    sns.countplot(y=col, data=bank_data, order=bank_data[col].value_counts().index)
    plt.title(f'{col} Distribution') # Changed from f'{col} 분포'
    plt.xlabel('Frequency') # Changed from '빈도'
    
    # 각 범주별 타겟 변수 분포
    plt.subplot(len(categorical_cols), 2, 2*i+2)
    cross_tab = pd.crosstab(bank_data[col], bank_data['y'], normalize='index') * 100
    cross_tab.plot(kind='bar', stacked=False)
    plt.title(f'{col} by Target Variable (y) Distribution') # Changed from f'{col}별 목표변수(y) 분포'
    plt.ylabel('Ratio (%)') # Changed from '비율 (%)'
    plt.legend(title='Target Variable') # Changed from '목표변수'
    
plt.tight_layout()
plt.show()

# %% [markdown]
# ### 3.2 수치형 변수 분석

# %%
# 수치형 변수 기술 통계량
numerical_stats = bank_data[numerical_cols].describe().T
numerical_stats['Variance'] = bank_data[numerical_cols].var() # Changed from '분산'
numerical_stats['Skewness'] = bank_data[numerical_cols].skew() # Changed from '왜도'
numerical_stats['Kurtosis'] = bank_data[numerical_cols].kurtosis() # Changed from '첨도'

print("Numerical Variable Descriptive Statistics:") # Changed from '수치형 변수 기술 통계량:'
print(numerical_stats)

# %%
# 수치형 변수 분포 시각화
plt.figure(figsize=(15, len(numerical_cols)*4))

for i, col in enumerate(numerical_cols):
    # 히스토그램 및 KDE 플롯
    plt.subplot(len(numerical_cols), 2, 2*i+1)
    sns.histplot(bank_data[col], kde=True)
    plt.axvline(bank_data[col].mean(), color='r', linestyle='--', label='Mean') # Changed from '평균'
    plt.axvline(bank_data[col].median(), color='g', linestyle='-.', label='Median') # Changed from '중앙값'
    plt.title(f'{col} Distribution') # Changed from f'{col} 분포'
    plt.legend()
    
    # 박스플롯
    plt.subplot(len(numerical_cols), 2, 2*i+2)
    sns.boxplot(x=bank_data[col])
    plt.title(f'{col} Boxplot') # Changed from f'{col} 박스플롯'
    
plt.tight_layout()
plt.show() 

# %% [markdown]
# ## 4. 이상치 탐지
#
# Ch1_(1).py에서 배운 이상치 탐지 방법을 활용하여 수치형 변수의 이상치를 분석합니다.

# %% [markdown]
# ### 4.1 IQR 방법을 이용한 이상치 탐지

# %%
# IQR(사분위 범위) 방법으로 이상치 탐지
def detect_outliers_iqr(df, col):
    # ...existing code...
    
    return outliers, lower_bound, upper_bound

# 각 수치형 변수에 대해 이상치 탐지
plt.figure(figsize=(15, 20))
outlier_stats = {}

for i, col in enumerate(numerical_cols):
    outliers, lower_bound, upper_bound = detect_outliers_iqr(bank_data, col)
    outlier_count = len(outliers)
    outlier_percent = (outlier_count / len(bank_data)) * 100
    
    outlier_stats[col] = {
        'outlier_count': outlier_count,
        'outlier_percent': outlier_percent,
        'lower_bound': lower_bound,
        'upper_bound': upper_bound
    }
    
    plt.subplot(len(numerical_cols), 1, i+1)
    plt.scatter(bank_data.index, bank_data[col], alpha=0.5)
    plt.axhline(y=lower_bound, color='r', linestyle='--', label=f'Lower Bound ({lower_bound:.2f})') # Changed from f'하한선 ({lower_bound:.2f})'
    plt.axhline(y=upper_bound, color='r', linestyle='--', label=f'Upper Bound ({upper_bound:.2f})') # Changed from f'상한선 ({upper_bound:.2f})'
    plt.title(f'{col} Outliers (IQR Method) - {outlier_count} ({outlier_percent:.2f}%)') # Changed from f'{col} 이상치 (IQR 방법) - {outlier_count}개 ({outlier_percent:.2f}%)'
    plt.ylabel(col)
    plt.legend()

plt.tight_layout()
plt.show()

# 이상치 통계 출력
outlier_summary = pd.DataFrame.from_dict(outlier_stats, orient='index')
print("Outlier Summary (IQR Method):") # Changed from 'IQR 방법으로 탐지한 이상치 요약:'
print(outlier_summary)

# %% [markdown]
# ### 4.2 Z-점수 방법을 이용한 이상치 탐지

# %%
# Z-점수 방법으로 이상치 탐지
def detect_outliers_zscore(df, col, threshold=3):
    # ...existing code...
    
    return outliers, z_scores

# 각 수치형 변수에 대해 Z-점수 이상치 탐지
plt.figure(figsize=(15, 20))
zscore_outlier_stats = {}

for i, col in enumerate(numerical_cols):
    outliers, z_scores = detect_outliers_zscore(bank_data, col)
    outlier_count = len(outliers)
    outlier_percent = (outlier_count / len(bank_data)) * 100
    
    zscore_outlier_stats[col] = {
        'outlier_count': outlier_count,
        'outlier_percent': outlier_percent
    }
    
    plt.subplot(len(numerical_cols), 1, i+1)
    plt.scatter(bank_data.index, abs(z_scores), alpha=0.5)
    plt.axhline(y=3, color='r', linestyle='--', label='Threshold (3)') # Changed from '임계값 (3)'
    plt.title(f'{col} Outliers (Z-Score Method) - {outlier_count} ({outlier_percent:.2f}%)') # Changed from f'{col} 이상치 (Z-점수 방법) - {outlier_count}개 ({outlier_percent:.2f}%)'
    plt.ylabel('|Z-Score|') # Changed from '|Z-점수|'
    plt.legend()

plt.tight_layout()
plt.show()

# Z-점수 이상치 통계 출력
zscore_summary = pd.DataFrame.from_dict(zscore_outlier_stats, orient='index')
print("Outlier Summary (Z-Score Method):") # Changed from 'Z-점수 방법으로 탐지한 이상치 요약:'
print(zscore_summary)

# %% [markdown]
# ### 4.3 머신러닝 기반 이상치 탐지 (IsolationForest)

# %%
from sklearn.ensemble import IsolationForest

# IsolationForest를 사용한 이상치 탐지
def detect_outliers_iforest(df, cols, contamination=0.05):
    # ...existing code...
    
    return df_outliers, y_pred == -1

# 모든 수치형 변수에 대해 IsolationForest 적용
isolation_results, is_outlier = detect_outliers_iforest(bank_data, numerical_cols)

# 이상치 결과 출력
outlier_count = isolation_results['outlier'].sum()
outlier_percent = (outlier_count / len(isolation_results)) * 100

print(f"Outliers detected by IsolationForest: {outlier_count} ({outlier_percent:.2f}%)") # Changed from f"IsolationForest로 탐지한 이상치: {outlier_count}개 ({outlier_percent:.2f}%)"

# 시각화를 위해 PCA로 차원 축소
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(bank_data[numerical_cols])

# 이상치 시각화
plt.figure(figsize=(10, 8))
plt.scatter(X_pca[~is_outlier, 0], X_pca[~is_outlier, 1], c='blue', s=5, label='Normal Data') # Changed from '정상 데이터'
plt.scatter(X_pca[is_outlier, 0], X_pca[is_outlier, 1], c='red', s=20, label='Outlier') # Changed from '이상치'
plt.title('Outlier Detection using IsolationForest (PCA Visualization)') # Changed from 'IsolationForest를 이용한 이상치 탐지 (PCA 축소 시각화)'
plt.xlabel('Principal Component 1') # Changed from '주성분 1'
plt.ylabel('Principal Component 2') # Changed from '주성분 2'
plt.legend()
plt.show()

# %% [markdown]
# ### 4.4 LOF(Local Outlier Factor)를 이용한 이상치 탐지

# %%
from sklearn.neighbors import LocalOutlierFactor

# LOF를 사용한 이상치 탐지
def detect_outliers_lof(df, cols, contamination=0.05):
    # ...existing code...
    
    return df_outliers, y_pred == -1

# 모든 수치형 변수에 대해 LOF 적용
lof_results, lof_is_outlier = detect_outliers_lof(bank_data, numerical_cols)

# 이상치 결과 출력
lof_outlier_count = lof_results['outlier'].sum()
lof_outlier_percent = (lof_outlier_count / len(lof_results)) * 100

print(f"Outliers detected by LOF: {lof_outlier_count} ({lof_outlier_percent:.2f}%)") # Changed from f"LOF로 탐지한 이상치: {lof_outlier_count}개 ({lof_outlier_percent:.2f}%)"

# 시각화
plt.figure(figsize=(10, 8))
plt.scatter(X_pca[~lof_is_outlier, 0], X_pca[~lof_is_outlier, 1], c='blue', s=5, label='Normal Data') # Changed from '정상 데이터'
plt.scatter(X_pca[lof_is_outlier, 0], X_pca[lof_is_outlier, 1], c='red', s=20, label='Outlier') # Changed from '이상치'
plt.title('Outlier Detection using LOF (PCA Visualization)') # Changed from 'LOF를 이용한 이상치 탐지 (PCA 축소 시각화)'
plt.xlabel('Principal Component 1') # Changed from '주성분 1'
plt.ylabel('Principal Component 2') # Changed from '주성분 2'
plt.legend()
plt.show()

# %% [markdown]
# ### 4.5 이상치 처리 방법 비교

# %%
# 이상치 탐지 방법 비교
outlier_comparison = pd.DataFrame({
    'IQR Method': [outlier_stats[col]['outlier_count'] for col in numerical_cols], # Changed from 'IQR 방법'
    'Z-Score Method': [zscore_outlier_stats[col]['outlier_count'] for col in numerical_cols], # Changed from 'Z-점수 방법'
    'IsolationForest': [isolation_results['outlier'].sum()] * len(numerical_cols),
    'LOF': [lof_results['outlier'].sum()] * len(numerical_cols)
}, index=numerical_cols)

print("Comparison of Outlier Counts by Detection Method:") # Changed from '이상치 탐지 방법별 이상치 개수 비교:'
print(outlier_comparison)

# 이상치 처리 방법 시연: balance 변수에 대해 다양한 방법 적용
outlier_col = 'balance'
bank_outlier = bank_data.copy()

# 1. 이상치 제거
outliers_iqr, _, _ = detect_outliers_iqr(bank_outlier, outlier_col)
bank_outlier_removed = bank_outlier[~bank_outlier.index.isin(outliers_iqr.index)]

# 2. 이상치 대체: 상하한선으로 클리핑
Q1 = bank_outlier[outlier_col].quantile(0.25)
Q3 = bank_outlier[outlier_col].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
bank_outlier_clipped = bank_outlier.copy()
bank_outlier_clipped[outlier_col] = bank_outlier_clipped[outlier_col].clip(lower_bound, upper_bound)

# 3. 이상치 대체: 평균으로 대체
bank_outlier_mean = bank_outlier.copy()
bank_outlier_mean.loc[bank_outlier_mean.index.isin(outliers_iqr.index), outlier_col] = bank_outlier[outlier_col].mean()

# 시각화: 이상치 처리 전후 비교
plt.figure(figsize=(15, 10))

plt.subplot(2, 2, 1)
sns.histplot(bank_outlier[outlier_col], kde=True)
plt.title('Original Data') # Changed from '원본 데이터'

plt.subplot(2, 2, 2)
sns.histplot(bank_outlier_removed[outlier_col], kde=True)
plt.title('After Outlier Removal') # Changed from '이상치 제거 후'

plt.subplot(2, 2, 3)
sns.histplot(bank_outlier_clipped[outlier_col], kde=True)
plt.title('After Outlier Clipping') # Changed from '이상치 클리핑 후'

plt.subplot(2, 2, 4)
sns.histplot(bank_outlier_mean[outlier_col], kde=True)
plt.title('After Outlier Mean Imputation') # Changed from '이상치 평균 대체 후'

plt.tight_layout()
plt.show()

print(f"Original data size: {len(bank_outlier)}") # Changed from f"원본 데이터 크기: {len(bank_outlier)}"
print(f"Data size after outlier removal: {len(bank_outlier_removed)}") # Changed from f"이상치 제거 후 데이터 크기: {len(bank_outlier_removed)}"
print(f"Removed outlier ratio: {(1 - len(bank_outlier_removed)/len(bank_outlier))*100:.2f}%") # Changed from f"제거된 이상치 비율: {(1 - len(bank_outlier_removed)/len(bank_outlier))*100:.2f}%" 

# %% [markdown]
# ## 5. 특성 변환 (Feature Transformation)
#
# Ch1_(2).py에서 배운 특성 변환 방법을 활용하여 데이터를 변환합니다.

# %% [markdown]
# ### 5.1 수치형 변수 변환

# %%
# 스케일링 방법 비교 (StandardScaler, MinMaxScaler)
from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer

# 변환을 적용할 데이터 준비 (outlier 처리된 데이터 사용)
X_numeric = bank_outlier_clipped[numerical_cols].copy()

# StandardScaler 적용
scaler_standard = StandardScaler()
X_scaled_standard = scaler_standard.fit_transform(X_numeric)
X_scaled_standard_df = pd.DataFrame(X_scaled_standard, columns=numerical_cols)

# MinMaxScaler 적용
scaler_minmax = MinMaxScaler()
X_scaled_minmax = scaler_minmax.fit_transform(X_numeric)
X_scaled_minmax_df = pd.DataFrame(X_scaled_minmax, columns=numerical_cols)

# PowerTransformer 적용 (Yeo-Johnson 변환)
scaler_power = PowerTransformer(method='yeo-johnson')
X_scaled_power = scaler_power.fit_transform(X_numeric)
X_scaled_power_df = pd.DataFrame(X_scaled_power, columns=numerical_cols)

# 변환 결과 시각화
for col in numerical_cols:
    plt.figure(figsize=(15, 10))
    
    plt.subplot(2, 2, 1)
    sns.histplot(X_numeric[col], kde=True)
    plt.title(f'Original Data: {col}') # Changed from f'원본 데이터: {col}'
    
    plt.subplot(2, 2, 2)
    sns.histplot(X_scaled_standard_df[col], kde=True)
    plt.title(f'StandardScaler: {col}')
    
    plt.subplot(2, 2, 3)
    sns.histplot(X_scaled_minmax_df[col], kde=True)
    plt.title(f'MinMaxScaler: {col}')
    
    plt.subplot(2, 2, 4)
    sns.histplot(X_scaled_power_df[col], kde=True)
    plt.title(f'PowerTransformer: {col}')
    
    plt.tight_layout()
    plt.show()

# %% [markdown]
# ### 5.2 범주형 변수 인코딩

# %%
# 범주형 변수 인코딩 방법 비교 (Label Encoding, One-Hot Encoding)
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# 예시로 사용할 범주형 변수 선택
cat_col_example = 'job'

# LabelEncoder 적용
le = LabelEncoder()
label_encoded = le.fit_transform(bank_data[cat_col_example])
print(f"LabelEncoder Result (first 10): {label_encoded[:10]}") # Changed from f"LabelEncoder 적용 결과 (처음 10개): {label_encoded[:10]}"
print(f"Original Categories: {bank_data[cat_col_example].iloc[:10].values}") # Changed from f"원본 범주: {bank_data[cat_col_example].iloc[:10].values}"
print(f"Encoding Mapping: {dict(zip(le.classes_, range(len(le.classes_))))}") # Changed from f"인코딩 매핑: {dict(zip(le.classes_, range(len(le.classes_))))}"

# OneHotEncoder 적용
ohe = OneHotEncoder(sparse_output=False, drop='first')
onehot_encoded = ohe.fit_transform(bank_data[[cat_col_example]])
onehot_df = pd.DataFrame(
    onehot_encoded, 
    columns=[f"{cat_col_example}_{cat}" for cat in le.classes_[1:]]
)
print("\nOneHotEncoder Result (first 5 rows, first 5 columns):") # Changed from "\nOneHotEncoder 적용 결과 (처음 5개 행, 처음 5개 열):"
print(onehot_df.iloc[:5, :5])

# %% [markdown]
# ### 5.3 빈화(Binning) 및 구간화

# %%
# 수치형 변수 binning 예시 - age 변수
bin_col = 'age'
bank_binned = bank_data.copy()

# 동일 너비 구간화
equal_width_bins = pd.cut(bank_data[bin_col], bins=5)
bank_binned['age_equal_width'] = equal_width_bins

# 동일 빈도 구간화
equal_freq_bins = pd.qcut(bank_data[bin_col], q=5)
bank_binned['age_equal_freq'] = equal_freq_bins

# 사용자 정의 구간화
custom_bins = [0, 30, 40, 50, 60, 100]
custom_labels = ['0-30', '31-40', '41-50', '51-60', '60+']
custom_binned = pd.cut(bank_data[bin_col], bins=custom_bins, labels=custom_labels)
bank_binned['age_custom'] = custom_binned

# 결과 비교
binning_comparison = pd.DataFrame({
    'Original': bank_data[bin_col], # Changed from '원본'
    'Equal Width Binning': bank_binned['age_equal_width'], # Changed from '동일 너비 구간화'
    'Equal Frequency Binning': bank_binned['age_equal_freq'], # Changed from '동일 빈도 구간화'
    'Custom Binning': bank_binned['age_custom'] # Changed from '사용자 정의 구간화'
})

print("Binning Comparison (first 10 rows):") # Changed from "Binning 비교 (처음 10개 행):"
print(binning_comparison.head(10))

# 구간화 시각화
plt.figure(figsize=(15, 10))

plt.subplot(2, 2, 1)
sns.histplot(bank_data[bin_col], kde=True, bins=20)
plt.title('Original Data') # Changed from '원본 데이터'

plt.subplot(2, 2, 2)
sns.countplot(y='age_equal_width', data=bank_binned)
plt.title('Equal Width Binning') # Changed from '동일 너비 구간화'

plt.subplot(2, 2, 3)
sns.countplot(y='age_equal_freq', data=bank_binned)
plt.title('Equal Frequency Binning') # Changed from '동일 빈도 구간화'

plt.subplot(2, 2, 4)
sns.countplot(y='age_custom', data=bank_binned)
plt.title('Custom Binning') # Changed from '사용자 정의 구간화'

plt.tight_layout()
plt.show()

# %% [markdown]
# ## 6. 특성 선택 (Feature Selection)

# %% [markdown]
# ### 6.1 필터 방식 특성 선택

# %%
from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif

# 목표 변수 인코딩
target = bank_data['y'].map({'yes': 1, 'no': 0})

# 수치형 특성에 대한 F-통계량 기반 특성 선택
X_num = StandardScaler().fit_transform(bank_outlier_clipped[numerical_cols])
f_selector = SelectKBest(f_classif, k='all')
f_selector.fit(X_num, target)

# 결과 시각화
feature_scores = pd.DataFrame({
    'Feature': numerical_cols, # Changed from '특성'
    'F-Statistic': f_selector.scores_, # Changed from 'F-통계량'
    'p-value': f_selector.pvalues_ # Changed from 'p-값'
})
feature_scores = feature_scores.sort_values('F-Statistic', ascending=False) # Changed from 'F-통계량'

plt.figure(figsize=(12, 6))
sns.barplot(x='F-Statistic', y='Feature', data=feature_scores) # Changed from 'F-통계량', '특성'
plt.title('Numerical Feature Importance based on F-Statistic') # Changed from 'F-통계량 기반 수치형 특성 중요도'
plt.show()

print("Numerical Feature Importance based on F-Statistic:") # Changed from "F-통계량 기반 수치형 특성 중요도:"
print(feature_scores)

# 범주형 특성에 대한 카이제곱 기반 특성 선택
# OneHotEncoder를 사용하여 범주형 변수 인코딩
X_cat_data = bank_data[categorical_cols[:-1]]  # 'y' 제외
ohe = OneHotEncoder(sparse_output=False)
X_cat = ohe.fit_transform(X_cat_data)

# 특성명 생성
feature_names = []
for i, col in enumerate(categorical_cols[:-1]):
    categories = ohe.categories_[i]
    feature_names.extend([f"{col}_{cat}" for cat in categories])

# 카이제곱 통계량 계산
chi2_selector = SelectKBest(chi2, k='all')
chi2_selector.fit(X_cat, target)

# 결과 시각화
chi2_scores = pd.DataFrame({
    'Feature': feature_names, # Changed from '특성'
    'Chi-squared Statistic': chi2_selector.scores_, # Changed from '카이제곱 통계량'
    'p-value': chi2_selector.pvalues_ # Changed from 'p-값'
})
chi2_scores = chi2_scores.sort_values('Chi-squared Statistic', ascending=False) # Changed from '카이제곱 통계량'

plt.figure(figsize=(12, 10))
sns.barplot(x='Chi-squared Statistic', y='Feature', data=chi2_scores.head(20)) # Changed from '카이제곱 통계량', '특성'
plt.title('Categorical Feature Importance based on Chi-squared (Top 20)') # Changed from '카이제곱 통계량 기반 범주형 특성 중요도 (상위 20개)'
plt.show()

print("Categorical Feature Importance based on Chi-squared (Top 10):") # Changed from "카이제곱 통계량 기반 범주형 특성 중요도 (상위 10개):"
print(chi2_scores.head(10))

# 상호정보량 기반 특성 선택
# 모든 특성에 대한 상호정보량 계산
X_combined = np.hstack([X_num, X_cat])
all_feature_names = numerical_cols + feature_names

mi_selector = SelectKBest(mutual_info_classif, k='all')
mi_selector.fit(X_combined, target)

mi_scores = pd.DataFrame({
    'Feature': all_feature_names, # Changed from '특성'
    'Mutual Information': mi_selector.scores_ # Changed from '상호정보량'
})
mi_scores = mi_scores.sort_values('Mutual Information', ascending=False) # Changed from '상호정보량'

plt.figure(figsize=(12, 8))
sns.barplot(x='Mutual Information', y='Feature', data=mi_scores.head(20)) # Changed from '상호정보량', '특성'
plt.title('Feature Importance based on Mutual Information (Top 20)') # Changed from '상호정보량 기반 특성 중요도 (상위 20개)'
plt.show()

print("Feature Importance based on Mutual Information (Top 10):") # Changed from "상호정보량 기반 특성 중요도 (상위 10개):"
print(mi_scores.head(10))

# %% [markdown]
# ### 6.2 임베디드 방식 특성 선택

# %%
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel

# 모든 특성에 대한 랜덤 포레스트 기반 특성 중요도
rf = RandomForestClassifier(n_estimators=100, random_state=42)
selector = SelectFromModel(rf, threshold='median')
selector.fit(X_combined, target)

# 특성 중요도 계산
importances = selector.estimator_.feature_importances_
rf_scores = pd.DataFrame({
    'Feature': all_feature_names, # Changed from '특성'
    'Importance': importances # Changed from '중요도'
})
rf_scores = rf_scores.sort_values('Importance', ascending=False) # Changed from '중요도'

plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=rf_scores.head(20)) # Changed from '중요도', '특성'
plt.title('Feature Importance based on Random Forest (Top 20)') # Changed from '랜덤 포레스트 기반 특성 중요도 (상위 20개)'
plt.show()

print("Feature Importance based on Random Forest (Top 10):") # Changed from "랜덤 포레스트 기반 특성 중요도 (상위 10개):"
print(rf_scores.head(10))

# 선택된 특성
selected_features = np.array(all_feature_names)[selector.get_support()]
print(f"\nSelected Features ({len(selected_features)}):") # Changed from f"\n선택된 특성 ({len(selected_features)}개):"
print(selected_features)

# %% [markdown]
# ## 7. 클래스 불균형 분석

# %%
# 목표 변수의 클래스 분포 확인
target_counts = bank_data['y'].value_counts()
target_percent = bank_data['y'].value_counts(normalize=True) * 100

print("Target Variable Distribution:") # Changed from "목표 변수 분포:"
print(target_counts)
print(f"\nRatio: {target_percent[0]:.2f}% vs {target_percent[1]:.2f}%") # Changed from f"\n비율: {target_percent[0]:.2f}% vs {target_percent[1]:.2f}%"

# 시각화
plt.figure(figsize=(10, 6))
sns.countplot(x='y', data=bank_data)
plt.title('Target Variable Distribution') # Changed from '목표 변수 분포'
plt.xlabel('Term Deposit Subscription') # Changed from '정기예금 가입 여부'
plt.ylabel('Frequency') # Changed from '빈도'
for i, count in enumerate(target_counts):
    plt.text(i, count + 100, f"{count} ({target_percent[i]:.1f}%)", ha='center')
plt.show()

# 클래스 불균형 처리 방법 시연
from imblearn.under_sampling import RandomUnderSampler, TomekLinks
from imblearn.over_sampling import SMOTE, ADASYN

# 특성과 목표 변수 준비
X = X_combined
y = target.values

# 1. 랜덤 언더샘플링
rus = RandomUnderSampler(random_state=42)
X_rus, y_rus = rus.fit_resample(X, y)

# 2. Tomek Links
tl = TomekLinks()
X_tl, y_tl = tl.fit_resample(X, y)

# 3. SMOTE
smote = SMOTE(random_state=42)
X_smote, y_smote = smote.fit_resample(X, y)

# 4. ADASYN
adasyn = ADASYN(random_state=42)
X_adasyn, y_adasyn = adasyn.fit_resample(X, y)

# 결과 비교
sampling_results = pd.DataFrame({
    'Method': ['Original Data', 'Random Undersampling', 'Tomek Links', 'SMOTE', 'ADASYN'], # Changed from '방법', '원본 데이터', '랜덤 언더샘플링'
    'Sample Size': [len(y), len(y_rus), len(y_tl), len(y_smote), len(y_adasyn)], # Changed from '표본 크기'
    'Class 0 (No) Ratio': [ # Changed from '클래스 0 (미가입) 비율'
        (y == 0).sum() / len(y) * 100,
        (y_rus == 0).sum() / len(y_rus) * 100,
        (y_tl == 0).sum() / len(y_tl) * 100,
        (y_smote == 0).sum() / len(y_smote) * 100,
        (y_adasyn == 0).sum() / len(y_adasyn) * 100
    ],
    'Class 1 (Yes) Ratio': [ # Changed from '클래스 1 (가입) 비율'
        (y == 1).sum() / len(y) * 100,
        (y_rus == 1).sum() / len(y_rus) * 100,
        (y_tl == 1).sum() / len(y_tl) * 100,
        (y_smote == 1).sum() / len(y_smote) * 100,
        (y_adasyn == 1).sum() / len(y_adasyn) * 100
    ]
})

print("Comparison of Class Imbalance Handling Methods:") # Changed from "클래스 불균형 처리 방법 비교:"
print(sampling_results)

# 시각화
plt.figure(figsize=(12, 6))
bar_width = 0.35
index = np.arange(len(sampling_results['Method'])) # Changed from '방법'

plt.bar(index, sampling_results['Class 0 (No) Ratio'], bar_width, label='No (Class 0)') # Changed from '클래스 0 (미가입) 비율', '미가입 (클래스 0)'
plt.bar(index, sampling_results['Class 1 (Yes) Ratio'], bar_width, # Changed from '클래스 1 (가입) 비율'
        bottom=sampling_results['Class 0 (No) Ratio'], label='Yes (Class 1)') # Changed from '클래스 0 (미가입) 비율', '가입 (클래스 1)'

plt.xlabel('Sampling Method') # Changed from '샘플링 방법'
plt.ylabel('Ratio (%)') # Changed from '비율 (%)'
plt.title('Class Distribution by Imbalance Handling Method') # Changed from '클래스 불균형 처리 방법별 클래스 분포'
plt.xticks(index, sampling_results['Method'], rotation=45) # Changed from '방법'
plt.legend()
plt.tight_layout()
plt.show() 

# %% [markdown]
# ## 8. 변수 간 상관관계 분석

# %%
# 수치형 변수 간 상관관계 (피어슨 상관계수)
corr_matrix = bank_data[numerical_cols].corr()

plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Pearson Correlation between Numerical Variables') # Changed from '수치형 변수 간 피어슨 상관계수'
plt.show()

# 스피어만 상관계수 (비선형 관계 탐지에 유용)
spearman_corr = bank_data[numerical_cols].corr(method='spearman')

plt.figure(figsize=(12, 10))
sns.heatmap(spearman_corr, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Spearman Correlation between Numerical Variables') # Changed from '수치형 변수 간 스피어만 상관계수'
plt.show()

# 범주형 변수와 목표 변수 간 관계 분석 (크래머 V 계수)
from scipy.stats import chi2_contingency

def cramers_v(x, y):
    # ...existing code...
    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))

# 각 범주형 변수와 목표 변수 간의 크래머 V 계수 계산
cramers_results = {}
for col in categorical_cols[:-1]:  # 목표 변수 'y' 제외
    cramers_results[col] = cramers_v(bank_data[col], bank_data['y'])

# 결과 시각화
cramers_df = pd.DataFrame({
    'Variable': list(cramers_results.keys()), # Changed from '변수'
    'Cramer\'s V Coefficient': list(cramers_results.values()) # Changed from '크래머 V 계수'
}).sort_values('Cramer\'s V Coefficient', ascending=False) # Changed from '크래머 V 계수'

plt.figure(figsize=(12, 6))
sns.barplot(x='Cramer\'s V Coefficient', y='Variable', data=cramers_df) # Changed from '크래머 V 계수', '변수'
plt.title('Cramer\'s V between Categorical Variables and Target') # Changed from '범주형 변수와 목표 변수 간 크래머 V 계수'
plt.show()

print("Cramer's V between Categorical Variables and Target:") # Changed from "범주형 변수와 목표 변수 간 크래머 V 계수:"
print(cramers_df)

# %% [markdown]
# ## 9. 차원 축소 및 군집화 분석
#
# Ch2.py에서 배운 차원 축소 및 군집화 방법을 활용하여 데이터를 분석합니다.

# %% [markdown]
# ### 9.1 PCA (주성분 분석)

# %%
from sklearn.decomposition import PCA

# 수치형 데이터에 대해 PCA 수행
pca = PCA()
pca.fit(X_num)

# 설명된 분산 비율
explained_variance_ratio = pca.explained_variance_ratio_
cumulative_variance_ratio = np.cumsum(explained_variance_ratio)

# 결과 시각화
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.bar(range(1, len(explained_variance_ratio)+1), explained_variance_ratio)
plt.xlabel('Principal Component') # Changed from '주성분'
plt.ylabel('Explained Variance Ratio') # Changed from '설명된 분산 비율'
plt.title('Explained Variance Ratio per Principal Component') # Changed from '각 주성분별 설명된 분산 비율'

plt.subplot(1, 2, 2)
plt.plot(range(1, len(cumulative_variance_ratio)+1), cumulative_variance_ratio, marker='o')
plt.axhline(y=0.95, color='r', linestyle='--', label='95% Threshold') # Changed from '95% 임계값'
plt.xlabel('Number of Principal Components') # Changed from '주성분 수'
plt.ylabel('Cumulative Explained Variance Ratio') # Changed from '누적 설명된 분산 비율'
plt.title('Cumulative Explained Variance Ratio') # Changed from '누적 설명된 분산 비율'
plt.legend()

plt.tight_layout()
plt.show()

# 주성분에 대한 변수 기여도 시각화
plt.figure(figsize=(14, 8))
components = pd.DataFrame(
    pca.components_,
    columns=numerical_cols
)

plt.subplot(1, 2, 1)
sns.heatmap(components.iloc[:2], annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Variable Contribution to First 2 Principal Components') # Changed from '처음 2개 주성분에 대한 변수 기여도'

# PCA 결과 시각화 (처음 2개 주성분)
X_pca_all = pca.transform(X_num)
pca_df = pd.DataFrame(
    X_pca_all[:, :2],
    columns=['PC1', 'PC2']
)
pca_df['y'] = bank_data['y']

plt.subplot(1, 2, 2)
sns.scatterplot(x='PC1', y='PC2', hue='y', data=pca_df, alpha=0.6)
plt.title('PCA Result Visualization (First 2 Principal Components)') # Changed from 'PCA 결과 시각화 (처음 2개 주성분)'

plt.tight_layout()
plt.show()

# %% [markdown]
# ### 9.2 t-SNE (T-distributed Stochastic Neighbor Embedding)

# %%
from sklearn.manifold import TSNE

# t-SNE 수행
tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)
X_tsne = tsne.fit_transform(X_combined)

# 결과 시각화
tsne_df = pd.DataFrame(
    X_tsne,
    columns=['t-SNE1', 't-SNE2']
)
tsne_df['y'] = bank_data['y']

plt.figure(figsize=(12, 10))
sns.scatterplot(x='t-SNE1', y='t-SNE2', hue='y', data=tsne_df, alpha=0.6)
plt.title('t-SNE Visualization') # Changed from 't-SNE 시각화'
plt.legend(title='Term Deposit Subscription') # Changed from '정기예금 가입 여부'
plt.show()

# %% [markdown]
# ### 9.3 K-Means 군집화

# %%
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# 최적의 군집 수 결정 (엘보우 방법)
inertia = []
silhouette_scores = []
k_range = range(2, 11)

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_combined)
    inertia.append(kmeans.inertia_)
    
    # 실루엣 점수 계산
    labels = kmeans.labels_
    silhouette_scores.append(silhouette_score(X_combined, labels))

# 엘보우 곡선 및 실루엣 점수 시각화
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(k_range, inertia, 'bo-')
plt.xlabel('Number of Clusters (k)') # Changed from '군집 수 (k)'
plt.ylabel('Inertia') # Changed from '관성 (Inertia)'
plt.title('Elbow Method') # Changed from '엘보우 방법'

plt.subplot(1, 2, 2)
plt.plot(k_range, silhouette_scores, 'ro-')
plt.xlabel('Number of Clusters (k)') # Changed from '군집 수 (k)'
plt.ylabel('Silhouette Score') # Changed from '실루엣 점수'
plt.title('Silhouette Score Method') # Changed from '실루엣 점수 방법'

plt.tight_layout()
plt.show()

# 최적의 k 선택
optimal_k = k_range[np.argmax(silhouette_scores)]
print(f"Optimal number of clusters (k): {optimal_k} (Silhouette Score: {max(silhouette_scores):.3f})") # Changed from f"최적의 군집 수 (k): {optimal_k} (실루엣 점수: {max(silhouette_scores):.3f})"

# 최적의 k로 K-Means 수행
kmeans_optimal = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
kmeans_labels = kmeans_optimal.fit_predict(X_combined)

# PCA로 차원 축소한 데이터에 군집 레이블 할당
pca_df['cluster'] = kmeans_labels

# 결과 시각화
plt.figure(figsize=(12, 10))
sns.scatterplot(x='PC1', y='PC2', hue='cluster', palette='viridis', 
                data=pca_df, alpha=0.6, s=50)
plt.title(f'K-Means Clustering Result (k={optimal_k})') # Changed from f'K-Means 군집화 결과 (k={optimal_k})'
plt.legend(title='Cluster') # Changed from '군집'
plt.show()

# 군집별 목표 변수 분포 확인
cluster_target_dist = pd.crosstab(
    kmeans_labels,
    bank_data['y'],
    normalize='index'
) * 100

print("Target Variable Distribution per Cluster (%):") # Changed from "군집별 목표 변수 분포 (%):"
print(cluster_target_dist)

# 시각화
plt.figure(figsize=(12, 6))
cluster_target_dist.plot(kind='bar', stacked=True)
plt.xlabel('Cluster') # Changed from '군집'
plt.ylabel('Ratio (%)') # Changed from '비율 (%)'
plt.title('Target Variable Distribution per Cluster') # Changed from '군집별 목표 변수 분포'
plt.legend(title='Term Deposit Subscription') # Changed from '정기예금 가입 여부'
plt.xticks(rotation=0)
plt.show()

# %% [markdown]
# ### 9.4 DBSCAN 군집화

# %%
from sklearn.cluster import DBSCAN
from sklearn.neighbors import NearestNeighbors

# k-거리 그래프를 통한 eps 파라미터 결정
neighbors = NearestNeighbors(n_neighbors=5)
neighbors_fit = neighbors.fit(X_num)
distances, indices = neighbors_fit.kneighbors(X_num)

# 정렬된 k-거리 플롯
distances = np.sort(distances[:, 4])
plt.figure(figsize=(12, 6))
plt.plot(distances)
plt.axhline(y=1.0, color='r', linestyle='--')
plt.title('k-Distance Graph (k=5)') # Changed from 'k-거리 그래프 (k=5)'
plt.xlabel('Data Points (sorted)') # Changed from '데이터 포인트 (정렬됨)'
plt.ylabel('Distance to 5th Neighbor') # Changed from '5번째 이웃까지의 거리'
plt.show()

# DBSCAN 적용
dbscan = DBSCAN(eps=1.0, min_samples=5)
dbscan_labels = dbscan.fit_predict(X_num)

# 결과 분석
n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)
n_noise = list(dbscan_labels).count(-1)

print(f"DBSCAN Result: {n_clusters} clusters, {n_noise} noise points ({n_noise/len(dbscan_labels)*100:.2f}%)") # Changed from f"DBSCAN 결과: {n_clusters}개 군집, {n_noise}개 노이즈 포인트 ({n_noise/len(dbscan_labels)*100:.2f}%)"

# PCA 결과에 DBSCAN 레이블 추가
pca_df['dbscan_cluster'] = dbscan_labels

# 결과 시각화
plt.figure(figsize=(12, 10))
colors = np.array(['gray' if label == -1 else f'C{label+1}' for label in dbscan_labels])
plt.scatter(pca_df['PC1'], pca_df['PC2'], c=colors, alpha=0.6, s=50)
plt.title('DBSCAN Clustering Result') # Changed from 'DBSCAN 군집화 결과'
plt.xlabel('PC1')
plt.ylabel('PC2')

# 노이즈 포인트 레이블 추가를 위한 핸들
noise_handle = plt.scatter([], [], c='gray', label='Noise')
cluster_handles = [plt.scatter([], [], c=f'C{i+1}', label=f'Cluster {i}') for i in range(n_clusters)]
plt.legend(handles=[noise_handle] + cluster_handles)

plt.show()

# DBSCAN 군집별 목표 변수 분포 (노이즈 포인트 제외)
dbscan_df = pd.DataFrame({
    'cluster': dbscan_labels,
    'y': bank_data['y']
})
dbscan_df = dbscan_df[dbscan_df['cluster'] != -1]  # 노이즈 포인트 제외

if len(dbscan_df) > 0:
    dbscan_target_dist = pd.crosstab(
        dbscan_df['cluster'],
        dbscan_df['y'],
        normalize='index'
    ) * 100

    print("\nDBSCAN Target Variable Distribution per Cluster (excluding noise) (%):") # Changed from "\nDBSCAN 군집별 목표 변수 분포 (노이즈 포인트 제외) (%):"
    print(dbscan_target_dist)

    # 시각화
    plt.figure(figsize=(12, 6))
    dbscan_target_dist.plot(kind='bar', stacked=True)
    plt.xlabel('DBSCAN Cluster') # Changed from 'DBSCAN 군집'
    plt.ylabel('Ratio (%)') # Changed from '비율 (%)'
    plt.title('DBSCAN Target Variable Distribution per Cluster (excluding noise)') # Changed from 'DBSCAN 군집별 목표 변수 분포 (노이즈 포인트 제외)'
    plt.legend(title='Term Deposit Subscription') # Changed from '정기예금 가입 여부'
    plt.xticks(rotation=0)
    plt.show()
else:
    print("No non-noise points found by DBSCAN.") # Changed from "DBSCAN에서 노이즈가 아닌 포인트가 없습니다."

# %% [markdown]
# ## 10. 결론 및 인사이트

# %% [markdown]
# ### 10.1 데이터 요약

# %%
# 데이터셋 요약
print(f"Dataset size: {bank_data.shape[0]} rows × {bank_data.shape[1]} columns") # Changed from '데이터셋 크기', '행', '열'
print(f"Categorical columns: {len(categorical_cols)} - {', '.join(categorical_cols)}") # Changed from '범주형 변수', '개'
print(f"Numerical columns: {len(numerical_cols)} - {', '.join(numerical_cols)}") # Changed from '수치형 변수', '개'
print(f"Target variable distribution: No {target_counts['no']} ({target_percent['no']:.2f}%), Yes {target_counts['yes']} ({target_percent['yes']:.2f}%)") # Changed from '목표 변수 분포', '미가입', '개', '가입', '개'

# %% [markdown]
# ### 10.2 주요 발견 사항 및 인사이트
#
# 1. **Data Distribution & Missing Values**
#    - The dataset has no missing values, so no imputation is needed.
#    - Among numerical variables, 'balance' shows a highly right-skewed distribution with outliers.
#
# 2. **Outlier Analysis**
#    - Outliers were detected using various methods (IQR, Z-score, IsolationForest, LOF).
#    - A significant number of outliers were found, especially in the 'balance' variable; clipping seems effective for handling them.
#
# 3. **Feature Transformation & Encoding**
#    - StandardScaler, MinMaxScaler, and PowerTransformer were applied to numerical variables to improve data distribution.
#    - Categorical variables were encoded using LabelEncoder or OneHotEncoder depending on the purpose.
#    - The 'age' variable can be binned into meaningful intervals for analysis.
#
# 4. **Feature Selection**
#    - Important features were selected using various methods (F-statistic, Chi-squared, Mutual Information, Random Forest).
#    - Random Forest-based feature importance analysis indicated 'duration', 'age', and 'balance' as top features.
#
# 5. **Class Imbalance**
#    - The target variable is imbalanced, with about 88% 'No' and 12% 'Yes'.
#    - Oversampling techniques like SMOTE or ADASYN can create a balanced dataset.
#
# 6. **Correlation Analysis**
#    - Correlations between numerical variables are generally low.
#    - Among categorical variables, 'contact', 'month', and 'poutcome' show relatively higher association with the target variable.
#
# 7. **Dimensionality Reduction & Clustering**
#    - PCA results show that the first 4 principal components explain about 95% of the variance.
#    - K-Means clustering suggests an optimal number of 3 clusters, each showing different distributions of the target variable.
#    - DBSCAN captures the density-based structure well but classifies a significant number of points as noise.
#
# 8. **Recommendations for Predictive Modeling**
#    - Due to severe class imbalance, using sampling techniques like SMOTE is recommended.
#    - The 'balance' variable with many outliers should be considered for clipping or log transformation.
#    - Building models focusing on high-importance features is efficient.
#    - OneHotEncoder is suitable for encoding categorical variables.

# %% [markdown]
# ### 10.3 후속 분석 권장사항
#
# 1. **Enhanced Feature Engineering**
#    - Extract seasonality features from date-related variables ('month').
#    - Create interaction features between numerical variables.
#    - Generate categorical variables through more detailed binning.
#
# 2. **Advanced Modeling Techniques**
#    - Apply ensemble models like XGBoost, LightGBM.
#    - Optimize hyperparameters through cross-validation.
#    - Analyze SHAP values for model interpretability.
#
# 3. **Deriving Business Insights**
#    - Identify customer segments with a high probability of subscribing to term deposits.
#    - Develop customer targeting strategies for optimizing marketing campaigns.
#    - Conduct in-depth analysis of key factors influencing term deposit subscriptions.
```

---

_**User**_

Apply the above code to the file. 


---

_**Assistant**_

I'll help you apply those changes to the file. Let's update the code in `final_eda.py` to change the Korean text to English.

---

