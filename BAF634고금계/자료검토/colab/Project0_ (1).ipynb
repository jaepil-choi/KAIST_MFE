{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832a291c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lfUtAoQ0n66T",
    "outputId": "9336748b-ca09-43aa-eeaf-509915e4ea3b"
   },
   "outputs": [],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca53c49",
   "metadata": {
    "id": "YpbF9bbNeJu_",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# _= load_dotenv(find_dotenv())\n",
    "\n",
    "# ap =os.getenv(\"MY_VAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d4c4a0",
   "metadata": {
    "id": "4l9Clt5rqb7A"
   },
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install openai\n",
    "!pip install langchain-community # Install the langchain_community package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13243159",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s-ImCydtuahh",
    "outputId": "06f55c70-efe9-49ba-af4c-665a02a0bb8c"
   },
   "outputs": [],
   "source": [
    "pip install -U langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c3c24d",
   "metadata": {
    "id": "E68TvgAvqi9c"
   },
   "outputs": [],
   "source": [
    "# prompt: 어떻게 api_key를 넣는가?\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load the environment variables from the .env file\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Get the OpenAI API key from the environment variables\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a705e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GBTRomSctof1",
    "outputId": "162f6713-ab25-47f1-9ab2-2fd103381244"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI \n",
    "\n",
    "api_key = \"YOUR API KEY HERE\"\n",
    "\n",
    "# Create an OpenAI object using the API key\n",
    "llm = OpenAI(temperature=0, openai_api_key=api_key)\n",
    "\n",
    "# Use the llm object to interact with the OpenAI API\n",
    "response = llm.generate([\"Hello world!\"])\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0ec0c4",
   "metadata": {
    "id": "fqnH_TF0rF3d"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "chat=ChatOpenAI(temperature=0,\n",
    "                openai_api_key=api_key\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9971987b",
   "metadata": {
    "id": "PXQaXdH2uDrb"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template=\"\"\"Question: {query} \\n\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e183114",
   "metadata": {
    "id": "IlHMFAn8vGLW"
   },
   "outputs": [],
   "source": [
    "question=\"What is 2+2?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc79c58",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rD7fn48_vK-Z",
    "outputId": "8133f8c5-1f32-4a70-f3d6-d527480ac96a"
   },
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt = prompt,\n",
    "    llm=chat,\n",
    ")\n",
    "\n",
    "print(llm_chain.predict(query=question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f69d186",
   "metadata": {
    "id": "teroeCufwzl_"
   },
   "outputs": [],
   "source": [
    "questions = [\n",
    "    {\"query\":\"What is 2+2?\"},\n",
    "    {\"query\":\"What is 3+3?\"},\n",
    "    {\"query\":\"What is 4+4?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39e38ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ubOv9Z_rw1bh",
    "outputId": "ec9f3a6c-1360-4439-f009-74c1a5f5a858"
   },
   "outputs": [],
   "source": [
    "print(llm_chain.run(questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390754d0",
   "metadata": {
    "id": "ihHLD0x7x2xW"
   },
   "source": [
    "### Other Example: Inserting Context to ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5222ab",
   "metadata": {
    "id": "ZvDZljiUw1X5"
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Given the following context: {context} \\n\n",
    "Question: Who is Bob's wife? \\n\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt = prompt,\n",
    "    llm = chat\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ddd127",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "998Ow0c1w1T_",
    "outputId": "3215af28-115c-45a8-f3c6-c39f198ccb31"
   },
   "outputs": [],
   "source": [
    "sentence=\"Bob is married to Mary\"\n",
    "print(llm_chain.predict(context=sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732bc04d",
   "metadata": {
    "id": "K7CU6DT8zqRs"
   },
   "source": [
    "Simple Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea517ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nxlm30tSw1QU",
    "outputId": "4cb2eaf9-62dc-4a08-b534-748ea965bfdf"
   },
   "outputs": [],
   "source": [
    "first_template = \"\"\"Given the following context: {query} \\n\n",
    "Question: How old is John? \\n\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "first_prompt = ChatPromptTemplate.from_template(first_template)\n",
    "\n",
    "\n",
    "first_chain = LLMChain(\n",
    "    prompt = first_prompt,\n",
    "    llm = chat\n",
    ")\n",
    "\n",
    "first_question = \"John's age is half dad's age. Dad is 42 years old.\"\n",
    "\n",
    "first_response = first_chain.run(query = first_question)\n",
    "print(first_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef14cba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JqCNeSqCw1Mv",
    "outputId": "4339fcec-4063-46e1-bcd2-f9387e9623b6"
   },
   "outputs": [],
   "source": [
    "second_template = \"\"\"Given the following context:\n",
    "You are are a gift recommender. Given a person's age, \\\n",
    "it is your job to suggest an approapriate gift for them. \\n\\\n",
    "\n",
    "Person Age: \\n\\\n",
    "{input} \\n\\\n",
    "Suggest gift:\n",
    "\"\"\"\n",
    "\n",
    "second_prompt = ChatPromptTemplate.from_template(second_template)\n",
    "\n",
    "second_chain = LLMChain(\n",
    "    prompt = second_prompt,\n",
    "    llm = chat\n",
    ")\n",
    "\n",
    "final_response = second_chain.run(input = first_response)\n",
    "print(final_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b002f298",
   "metadata": {
    "id": "QydiXsCOw1JV"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "overall_chain = SimpleSequentialChain(\n",
    "    chains = [first_chain, second_chain],\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a890949a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "zH_3KaJYw1Fz",
    "outputId": "c32d4d55-fbac-48d9-8aad-7cb8875fa294"
   },
   "outputs": [],
   "source": [
    "overall_chain.run(\"John's age is half dad's age. Dad is 42 years old.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4ac8b5",
   "metadata": {
    "id": "rHlMPRFw0LOQ"
   },
   "source": [
    "Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc11f7fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tGyBMup4w1Cf",
    "outputId": "2a2c3f3f-38c0-4afe-a551-e7e13bbb6344"
   },
   "outputs": [],
   "source": [
    "pip install -U langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc5f0f1",
   "metadata": {
    "id": "9Fe3V7Zsw0-9"
   },
   "outputs": [],
   "source": [
    "# # prompt: 어떻게 api_key를 넣는가?\n",
    "\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# # Load the environment variables from the .env file\n",
    "# load_dotenv(find_dotenv())\n",
    "\n",
    "# # Get the OpenAI API key from the environment variables\n",
    "# api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c74ec2",
   "metadata": {
    "id": "2T1Bn4Ev0eT1"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "chat=ChatOpenAI(temperature=0,\n",
    "                openai_api_key=api_key\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16375d15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ZdXukOt0ePd",
    "outputId": "e75ac1ea-a465-4448-8d31-6a17836eb4c3"
   },
   "outputs": [],
   "source": [
    "pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d54487",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KbZSLutD0eLb",
    "outputId": "9ddce0b8-fa54-4d17-a98b-ba3f724a1b00"
   },
   "outputs": [],
   "source": [
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "\n",
    "wikipedia = WikipediaAPIWrapper(top_k_results = 1)\n",
    "print(wikipedia.run('Tesla, Inc.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838339fd",
   "metadata": {
    "id": "JNrTqsb806F-"
   },
   "source": [
    "Please generate a refined document of the following document. And please ensure that the refined document meets the following criteria:\n",
    "1. The refined document should be abstract and does not change any original meaning of the document.\n",
    "2. The refined document should retain all the important objects, concepts, and relationships between them.\n",
    "3. The refined document should only contain information that is from the document.\n",
    "4. The refined document should be readable and easy to understand without any abbreviations and misspellings.\n",
    "Here is the content: [x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06653d36",
   "metadata": {
    "id": "wdD6738J0-eN"
   },
   "source": [
    "You are a knowledge graph extractor, and your task is to extract and return a knowledge graph from a given text.Let’s extract it step by step:\n",
    "(1). Identify the entities in the text. An entity can be a noun or a noun phrase that refers to a real-world object or an abstract concept. You can use a named entity recognition (NER) tool or a part-of-speech (POS) tagger to identify the entities.\n",
    "(2). Identify the relationships between the entities. A relationship can be a verb or a prepositional phrase that connects two entities. You can use dependency parsing\n",
    "to identify the relationships.\n",
    "(3). Summarize each entity and relation as short as possible and remove any stop words.\n",
    "(4). Only return the knowledge graph in the triplet format: (’head entity’, ’relation’, ’tail entity’).\n",
    "(5). Most importantly, if you cannot find any knowledge, please just output: \"None\".\n",
    "Here is the content: [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a39e08",
   "metadata": {
    "id": "thWft8vv0eH7"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "first_template = \"\"\"Please generate a refined document of the following document. \\n\\\n",
    "And please ensure that the refined document meets the following criteria: \\n\\\n",
    "1. The refined document should be abstract and does not change any original \\\n",
    "meaning of the document. \\n\\\n",
    "2. The refined document should retain all the important objects, concepts, and \\\n",
    "relationship between them. \\n\\\n",
    "3. The refined document should only contain information that is from \\\n",
    "the document. \\n\\\n",
    "4. The refined document should be readable and easy to understand without any \\\n",
    "abbrevations and misspellings. \\n\\\n",
    "Here is the content: {content}\n",
    "\"\"\"\n",
    "\n",
    "first_prompt = ChatPromptTemplate.from_template(first_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26656f10",
   "metadata": {
    "id": "od2yrMFK1GAU"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "first_chain = LLMChain(\n",
    "    prompt = first_prompt,\n",
    "    llm = chat\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b04d539",
   "metadata": {
    "id": "1F17FoHT1F8c"
   },
   "outputs": [],
   "source": [
    "second_template = \"\"\"You are a knowledge graph extractor, and your task is to extract\\\n",
    "and return a knowledge graph from a given text. Let's extract it step by step:\\n\\\n",
    "(1). Identify the entities in the text. An entity can be a noun or noun phrase \\\n",
    "that refers to a real-world object or an abstract concept. You can use a named entity\\\n",
    "recognition (NER) tool or a part-of-speech (POS) tagger to identify the entities. \\n\\\n",
    "(2). Identify the relationships between the entities. A relationship can be a verb \\\n",
    "or a prepositional phrase that connects two entities. You can use dependency parsing \\\n",
    "to identify the relationships. \\n\\\n",
    "(3). Summarize each entity and relation as short as possible and remove any stop words. \\n\\\n",
    "(4). Only return the knowledge graph in the triplet format: ('head entity', 'relation', 'tail entity'). \\n\\\n",
    "(5). Most importantly, if you cannot find any knowledge, please just output: \"None\". \\n\\\n",
    "Here is the content: {content}\n",
    "\"\"\"\n",
    "second_prompt = ChatPromptTemplate.from_template(second_template)\n",
    "\n",
    "second_chain = LLMChain(\n",
    "    prompt = second_prompt,\n",
    "    llm = chat\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a982a",
   "metadata": {
    "id": "c1qgcyg71F40"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "overall_chain = SimpleSequentialChain(\n",
    "    chains = [first_chain, second_chain],\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28b5a86",
   "metadata": {
    "id": "QrM0GSK01F1F"
   },
   "outputs": [],
   "source": [
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "\n",
    "wikipedia = WikipediaAPIWrapper(top_k_results = 1)\n",
    "wiki_pages = wikipedia.run('Tesla, Inc.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a830114a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "id": "-bjk4XiX1Fxp",
    "outputId": "9ebd91fa-7e77-433c-ce74-eafaa8ba6c6a"
   },
   "outputs": [],
   "source": [
    "overall_chain.run(wiki_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8868f30b",
   "metadata": {
    "id": "K5wsRmtA1fEf"
   },
   "source": [
    "Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cb0737",
   "metadata": {
    "id": "_pDaRPsQ1Ft9"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.agents import load_tools\n",
    "\n",
    "chat = ChatOpenAI(temperature = 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a059d2f8",
   "metadata": {
    "id": "TwQxmHzI1Fqh"
   },
   "outputs": [],
   "source": [
    "tools = load_tools([\"wikipedia\"], llm = chat, top_k_results = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff648be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7F-Jav-u1mLO",
    "outputId": "c01caa29-b96e-4d62-f755-a771060ca7e9"
   },
   "outputs": [],
   "source": [
    "agent = initialize_agent(tools,\n",
    "                         llm = chat,\n",
    "                         agent = AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "                        handle_parsing_errors=True,\n",
    "                         verbose = True\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7e3140",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "id": "L8rpRtfg1mHt",
    "outputId": "8d28e7de-6bb9-47f2-c3db-2735d356fc89"
   },
   "outputs": [],
   "source": [
    "agent.run(\"Tesla, Inc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f949999f",
   "metadata": {
    "id": "cIMl3Dhq5Rig"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d9e5fe",
   "metadata": {
    "id": "FXcKSEyv5Rel"
   },
   "outputs": [],
   "source": [
    "overall_chain = SimpleSequentialChain(\n",
    "    chains = [agent,\n",
    "              first_chain,\n",
    "              second_chain],\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c67a06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "-_OR-dp25RbE",
    "outputId": "2af8e09c-cc84-44ae-974c-662cd92af6d5"
   },
   "outputs": [],
   "source": [
    "overall_chain.run(\"Tesla, Inc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a39d3b5",
   "metadata": {
    "id": "0fY7Wf655q96"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c180d9c",
   "metadata": {
    "id": "0UG7WJWx5q6H"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ff1a22",
   "metadata": {
    "id": "yaHvvdUt5q2u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0653eddd",
   "metadata": {
    "id": "DagiNO4l5qyt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2078ae75",
   "metadata": {
    "id": "SqEx-nYc5qvE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2f7123",
   "metadata": {
    "id": "l4NFw9DU5qrJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e187dff",
   "metadata": {
    "id": "yBLrKEB15qnU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9c8502",
   "metadata": {
    "id": "IaczkNHy5qj6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
